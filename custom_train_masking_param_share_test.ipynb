{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n",
    "from tensorboardX import SummaryWriter\n",
    "import cv2\n",
    "import io\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed, keep_bbox_from_image_range, \\\n",
    "    keep_bbox_from_lidar_range, write_pickle, write_label, \\\n",
    "    iou2d, iou3d_camera, iou_bev\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the scalars and convert the plot to a tensor image\n",
    "def plot_scalars(scalars, step):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(scalars)\n",
    "    ax.set_title(f'Step {step}')\n",
    "    ax.set_xlabel('Scalar Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Convert the plot to a PNG image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert PNG buffer to a tensor image\n",
    "    image = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)  # Decode the image\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1)#.unsqueeze(0)  # Convert to PyTorch tensor and add batch dimension\n",
    "    return image\n",
    "\n",
    "def save_summary(writer, loss_dict, global_step, tag, gating_prob, lr=None, momentum=None, model=None, data=None, flag=False):\n",
    "    for k, v in loss_dict.items():\n",
    "        writer.add_scalar(f'{tag}/{k}', v, global_step)\n",
    "    if lr is not None:\n",
    "        writer.add_scalar('lr', lr, global_step)\n",
    "    if momentum is not None:\n",
    "        writer.add_scalar('momentum', momentum, global_step)\n",
    "    if model is not None and global_step % 1000 == 0:\n",
    "        for tag, value in model.named_parameters():\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "            if value.grad is not None:\n",
    "                writer.add_histogram(tag + \"/grad\", value.grad.cpu(), global_step)\n",
    "    \n",
    "    if global_step % 100 == 0:\n",
    "        image = plot_scalars(gating_prob.detach().cpu().numpy(), global_step)\n",
    "        writer.add_image(\"Scalars Plot\", image, global_step=global_step)\n",
    "    # for i in data:\n",
    "    #     del i['gt_names']\n",
    "    #     del i['image_info']\n",
    "    #     del i['calib_info']\n",
    "    #     del i['difficulty']\n",
    "\n",
    "    # writer.add_graph(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_root = \"dataset/KITTI\"\n",
    "        self.saved_path = \"logs/pillar_sequence_mem_lin_gat_9_param_share_test\"\n",
    "        self.saved_path_exact = self.saved_path + \"/results_exact\"\n",
    "        self.saved_path_estimate = self.saved_path + \"/results_estimate\"\n",
    "        self.batch_size = 2\n",
    "        self.num_workers = 4\n",
    "        self.window_length = 1\n",
    "        self.nclasses = 3\n",
    "        self.init_lr = 0.00025\n",
    "        self.max_epoch = 200\n",
    "        self.log_freq = 1        \n",
    "        self.ckpt_freq_epoch = 1\n",
    "        self.val_freq_epoch = 1\n",
    "        self.no_cuda = not torch.cuda.is_available()\n",
    " \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Kitti(data_root=args.data_root,\n",
    "                        split='train')\n",
    "val_dataset = Kitti(data_root=args.data_root,\n",
    "                    split='val')\n",
    "train_dataloader = get_dataloader(dataset=train_dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    num_workers=args.num_workers,\n",
    "                                    shuffle=True)\n",
    "val_dataloader = get_dataloader(dataset=val_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                num_workers=args.num_workers,\n",
    "                                shuffle=False)\n",
    "\n",
    "# data = train_dataset.__getitem__(9)\n",
    "\n",
    "train_dataset_length = len(train_dataset.sorted_ids)\n",
    "train_dataset_batch_count =  train_dataset_length \n",
    "val_dataset_length = len(val_dataset.sorted_ids)\n",
    "\n",
    "def get_sequence_from_velodyne_path(file_path):\n",
    "    parts = file_path.split('/')\n",
    "    file_name = parts[-1]\n",
    "    extracted_part = file_name.split('_')[0]\n",
    "    return extracted_part\n",
    "\n",
    "# Print the extracted part\n",
    "\n",
    "CLASSES = Kitti.CLASSES\n",
    "LABEL2CLASSES = {v:k for k, v in CLASSES.items()}\n",
    "\n",
    "\n",
    "pcd_limit_range = np.array([0, -40, -3, 70.4, 40, 0.0], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, Optimizer, Scheduler, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.no_cuda:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses).cuda()\n",
    "else:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses)\n",
    "\n",
    "model_flag = False\n",
    "\n",
    "loss_func = Loss()\n",
    "\n",
    "max_iters = 2* train_dataset_batch_count * args.max_epoch\n",
    "init_lr = args.init_lr\n",
    "optimizer = torch.optim.AdamW(params=pointpillars.parameters(), \n",
    "                                lr=init_lr, \n",
    "                                betas=(0.95, 0.99),\n",
    "                                weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,  \n",
    "                                                max_lr=init_lr*10, \n",
    "                                                total_steps=max_iters, \n",
    "                                                pct_start=0.4, \n",
    "                                                anneal_strategy='cos',\n",
    "                                                cycle_momentum=True, \n",
    "                                                base_momentum=0.95*0.895, \n",
    "                                                max_momentum=0.95,\n",
    "                                                div_factor=10)\n",
    "\n",
    "\n",
    "saved_logs_path = os.path.join(args.saved_path, 'summary')\n",
    "import shutil\n",
    "if os.path.exists(saved_logs_path):\n",
    "    shutil.rmtree(saved_logs_path)\n",
    "os.makedirs(saved_logs_path, exist_ok=True)\n",
    "writer = SummaryWriter(saved_logs_path)\n",
    "saved_ckpt_path = os.path.join(args.saved_path, 'checkpoints')\n",
    "os.makedirs(saved_ckpt_path, exist_ok=True)\n",
    "\n",
    "# Directory for exact results\n",
    "saved_path_exact = args.saved_path_exact\n",
    "os.makedirs(saved_path_exact, exist_ok=True)\n",
    "saved_submit_path_exact = os.path.join(saved_path_exact, 'submit')\n",
    "os.makedirs(saved_submit_path_exact, exist_ok=True)\n",
    "\n",
    "# Directory for estimate results\n",
    "saved_path_estimate = args.saved_path_estimate\n",
    "os.makedirs(saved_path_estimate, exist_ok=True)\n",
    "saved_submit_path_estimate = os.path.join(saved_path_estimate, 'submit')\n",
    "os.makedirs(saved_submit_path_estimate, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_thresholds(tp_scores, total_num_valid_gt, num_sample_pts=41):\n",
    "    score_thresholds = []\n",
    "    tp_scores = sorted(tp_scores)[::-1]\n",
    "    cur_recall, pts_ind = 0, 0\n",
    "    for i, score in enumerate(tp_scores):\n",
    "        lrecall = (i + 1) / total_num_valid_gt\n",
    "        rrecall = (i + 2) / total_num_valid_gt\n",
    "\n",
    "        if i == len(tp_scores) - 1:\n",
    "            score_thresholds.append(score)\n",
    "            break\n",
    "\n",
    "        if (lrecall + rrecall) / 2 < cur_recall:\n",
    "            continue\n",
    "\n",
    "        score_thresholds.append(score)\n",
    "        pts_ind += 1\n",
    "        cur_recall = pts_ind / (num_sample_pts - 1)\n",
    "    return score_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(det_results, gt_results, CLASSES, saved_path):\n",
    "    '''\n",
    "    det_results: list,\n",
    "    gt_results: dict(id -> det_results)\n",
    "    CLASSES: dict\n",
    "    '''\n",
    "    assert len(det_results) == len(gt_results)\n",
    "    f = open(os.path.join(saved_path, 'eval_results.txt'), 'w')\n",
    "\n",
    "    # 1. calculate iou\n",
    "    ious = {\n",
    "        'bbox_2d': [],\n",
    "        'bbox_bev': [],\n",
    "        'bbox_3d': []\n",
    "    }\n",
    "    ids = list(sorted(gt_results.keys()))\n",
    "    for id in ids:\n",
    "        gt_result = gt_results[id]['annos']\n",
    "        det_result = det_results[id]\n",
    "\n",
    "        # 1.1, 2d bboxes iou\n",
    "        gt_bboxes2d = gt_result['bbox'].astype(np.float32)\n",
    "        det_bboxes2d = det_result['bbox'].astype(np.float32)\n",
    "        iou2d_v = iou2d(torch.from_numpy(gt_bboxes2d).cuda(), torch.from_numpy(det_bboxes2d).cuda())\n",
    "        ious['bbox_2d'].append(iou2d_v.cpu().numpy())\n",
    "\n",
    "        # 1.2, bev iou\n",
    "        gt_location = gt_result['location'].astype(np.float32)\n",
    "        gt_dimensions = gt_result['dimensions'].astype(np.float32)\n",
    "        gt_rotation_y = gt_result['rotation_y'].astype(np.float32)\n",
    "        det_location = det_result['location'].astype(np.float32)\n",
    "        det_dimensions = det_result['dimensions'].astype(np.float32)\n",
    "        det_rotation_y = det_result['rotation_y'].astype(np.float32)\n",
    "\n",
    "        gt_bev = np.concatenate([gt_location[:, [0, 2]], gt_dimensions[:, [0, 2]], gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bev = np.concatenate([det_location[:, [0, 2]], det_dimensions[:, [0, 2]], det_rotation_y[:, None]], axis=-1)\n",
    "        iou_bev_v = iou_bev(torch.from_numpy(gt_bev).cuda(), torch.from_numpy(det_bev).cuda())\n",
    "        ious['bbox_bev'].append(iou_bev_v.cpu().numpy())\n",
    "\n",
    "        # 1.3, 3dbboxes iou\n",
    "        gt_bboxes3d = np.concatenate([gt_location, gt_dimensions, gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bboxes3d = np.concatenate([det_location, det_dimensions, det_rotation_y[:, None]], axis=-1)\n",
    "        iou3d_v = iou3d_camera(torch.from_numpy(gt_bboxes3d).cuda(), torch.from_numpy(det_bboxes3d).cuda())\n",
    "        ious['bbox_3d'].append(iou3d_v.cpu().numpy())\n",
    "\n",
    "    MIN_IOUS = {\n",
    "        'Pedestrian': [0.5, 0.5, 0.5],\n",
    "        'Cyclist': [0.5, 0.5, 0.5],\n",
    "        'Car': [0.7, 0.7, 0.7]\n",
    "    }\n",
    "    MIN_HEIGHT = [40, 25, 25]\n",
    "\n",
    "    overall_results = {}\n",
    "    for e_ind, eval_type in enumerate(['bbox_2d', 'bbox_bev', 'bbox_3d']):\n",
    "        eval_ious = ious[eval_type]\n",
    "        eval_ap_results, eval_aos_results = {}, {}\n",
    "        for cls in CLASSES:\n",
    "            eval_ap_results[cls] = []\n",
    "            eval_aos_results[cls] = []\n",
    "            CLS_MIN_IOU = MIN_IOUS[cls][e_ind]\n",
    "            for difficulty in [0, 1, 2]:\n",
    "                # 1. bbox property\n",
    "                total_gt_ignores, total_det_ignores, total_dc_bboxes, total_scores = [], [], [], []\n",
    "                total_gt_alpha, total_det_alpha = [], []\n",
    "                for id in ids:\n",
    "                    gt_result = gt_results[id]['annos']\n",
    "                    det_result = det_results[id]\n",
    "\n",
    "                    # 1.1 gt bbox property\n",
    "                    cur_gt_names = gt_result['name']\n",
    "                    cur_difficulty = gt_result['difficulty']\n",
    "                    gt_ignores, dc_bboxes = [], []\n",
    "                    for j, cur_gt_name in enumerate(cur_gt_names):\n",
    "                        ignore = cur_difficulty[j] < 0 or cur_difficulty[j] > difficulty\n",
    "                        if cur_gt_name == cls:\n",
    "                            valid_class = 1\n",
    "                        elif cls == 'Pedestrian' and cur_gt_name == 'Person_sitting':\n",
    "                            valid_class = 0\n",
    "                        elif cls == 'Car' and cur_gt_name == 'Van':\n",
    "                            valid_class = 0\n",
    "                        else:\n",
    "                            valid_class = -1\n",
    "                        \n",
    "                        if valid_class == 1 and not ignore:\n",
    "                            gt_ignores.append(0)\n",
    "                        elif valid_class == 0 or (valid_class == 1 and ignore):\n",
    "                            gt_ignores.append(1)\n",
    "                        else:\n",
    "                            gt_ignores.append(-1)\n",
    "                        \n",
    "                        if cur_gt_name == 'DontCare':\n",
    "                            dc_bboxes.append(gt_result['bbox'][j])\n",
    "                    total_gt_ignores.append(gt_ignores)\n",
    "                    total_dc_bboxes.append(np.array(dc_bboxes))\n",
    "                    total_gt_alpha.append(gt_result['alpha'])\n",
    "\n",
    "                    # 1.2 det bbox property\n",
    "                    cur_det_names = det_result['name']\n",
    "                    cur_det_heights = det_result['bbox'][:, 3] - det_result['bbox'][:, 1]\n",
    "                    det_ignores = []\n",
    "                    for j, cur_det_name in enumerate(cur_det_names):\n",
    "                        if cur_det_heights[j] < MIN_HEIGHT[difficulty]:\n",
    "                            det_ignores.append(1)\n",
    "                        elif cur_det_name == cls:\n",
    "                            det_ignores.append(0)\n",
    "                        else:\n",
    "                            det_ignores.append(-1)\n",
    "                    total_det_ignores.append(det_ignores)\n",
    "                    total_scores.append(det_result['score'])\n",
    "                    total_det_alpha.append(det_result['alpha'])\n",
    "\n",
    "                # 2. calculate scores thresholds for PR curve\n",
    "                tp_scores = []\n",
    "                for i, id in enumerate(ids):\n",
    "                    cur_eval_ious = eval_ious[i]\n",
    "                    gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                    scores = total_scores[i]\n",
    "\n",
    "                    nn, mm = cur_eval_ious.shape\n",
    "                    assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                    for j in range(nn):\n",
    "                        if gt_ignores[j] == -1:\n",
    "                            continue\n",
    "                        match_id, match_score = -1, -1\n",
    "                        for k in range(mm):\n",
    "                            if not assigned[k] and det_ignores[k] >= 0 and cur_eval_ious[j, k] > CLS_MIN_IOU and scores[k] > match_score:\n",
    "                                match_id = k\n",
    "                                match_score = scores[k]\n",
    "                        if match_id != -1:\n",
    "                            assigned[match_id] = True\n",
    "                            if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                tp_scores.append(match_score)\n",
    "                total_num_valid_gt = np.sum([np.sum(np.array(gt_ignores) == 0) for gt_ignores in total_gt_ignores])\n",
    "                score_thresholds = get_score_thresholds(tp_scores, total_num_valid_gt)    \n",
    "            \n",
    "                # 3. draw PR curve and calculate mAP\n",
    "                tps, fns, fps, total_aos = [], [], [], []\n",
    "\n",
    "                for score_threshold in score_thresholds:\n",
    "                    tp, fn, fp = 0, 0, 0\n",
    "                    aos = 0\n",
    "                    for i, id in enumerate(ids):\n",
    "                        cur_eval_ious = eval_ious[i]\n",
    "                        gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                        gt_alpha, det_alpha = total_gt_alpha[i], total_det_alpha[i]\n",
    "                        scores = total_scores[i]\n",
    "\n",
    "                        nn, mm = cur_eval_ious.shape\n",
    "                        assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                        for j in range(nn):\n",
    "                            if gt_ignores[j] == -1:\n",
    "                                continue\n",
    "                            match_id, match_iou = -1, -1\n",
    "                            for k in range(mm):\n",
    "                                if not assigned[k] and det_ignores[k] >= 0 and scores[k] >= score_threshold and cur_eval_ious[j, k] > CLS_MIN_IOU:\n",
    "    \n",
    "                                    if det_ignores[k] == 0 and cur_eval_ious[j, k] > match_iou:\n",
    "                                        match_iou = cur_eval_ious[j, k]\n",
    "                                        match_id = k\n",
    "                                    elif det_ignores[k] == 1 and match_iou == -1:\n",
    "                                        match_id = k\n",
    "\n",
    "                            if match_id != -1:\n",
    "                                assigned[match_id] = True\n",
    "                                if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                    tp += 1\n",
    "                                    if eval_type == 'bbox_2d':\n",
    "                                        aos += (1 + np.cos(gt_alpha[j] - det_alpha[match_id])) / 2\n",
    "                            else:\n",
    "                                if gt_ignores[j] == 0:\n",
    "                                    fn += 1\n",
    "                            \n",
    "                        for k in range(mm):\n",
    "                            if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                fp += 1\n",
    "                        \n",
    "                        # In case 2d bbox evaluation, we should consider dontcare bboxes\n",
    "                        if eval_type == 'bbox_2d':\n",
    "                            dc_bboxes = total_dc_bboxes[i]\n",
    "                            det_bboxes = det_results[id]['bbox']\n",
    "                            if len(dc_bboxes) > 0:\n",
    "                                ious_dc_det = iou2d(torch.from_numpy(det_bboxes), torch.from_numpy(dc_bboxes), metric=1).numpy().T\n",
    "                                for j in range(len(dc_bboxes)):\n",
    "                                    for k in range(len(det_bboxes)):\n",
    "                                        if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                            if ious_dc_det[j, k] > CLS_MIN_IOU:\n",
    "                                                fp -= 1\n",
    "                                                assigned[k] = True\n",
    "                            \n",
    "                    tps.append(tp)\n",
    "                    fns.append(fn)\n",
    "                    fps.append(fp)\n",
    "                    if eval_type == 'bbox_2d':\n",
    "                        total_aos.append(aos)\n",
    "\n",
    "                tps, fns, fps = np.array(tps), np.array(fns), np.array(fps)\n",
    "\n",
    "                precisions = tps / (tps + fns) # actually this is recalls\n",
    "                # precisions = tps / (tps + fps)\n",
    "                for i in range(len(score_thresholds)):\n",
    "                    precisions[i] = np.max(precisions[i:])\n",
    "                \n",
    "                sums_AP = 0\n",
    "                for i in range(0, len(score_thresholds), 4):\n",
    "                    sums_AP += precisions[i]\n",
    "                mAP = sums_AP / 11 * 100\n",
    "                eval_ap_results[cls].append(mAP)\n",
    "\n",
    "                if eval_type == 'bbox_2d':\n",
    "                    total_aos = np.array(total_aos)\n",
    "                    similarity = total_aos / (tps + fps)\n",
    "                    for i in range(len(score_thresholds)):\n",
    "                        similarity[i] = np.max(similarity[i:])\n",
    "                    sums_similarity = 0\n",
    "                    for i in range(0, len(score_thresholds), 4):\n",
    "                        sums_similarity += similarity[i]\n",
    "                    mSimilarity = sums_similarity / 11 * 100\n",
    "                    eval_aos_results[cls].append(mSimilarity)\n",
    "\n",
    "        print(f'=========={eval_type.upper()}==========')\n",
    "        print(f'=========={eval_type.upper()}==========', file=f)\n",
    "        for k, v in eval_ap_results.items():\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            print(f'==========AOS==========')\n",
    "            print(f'==========AOS==========', file=f)\n",
    "            for k, v in eval_aos_results.items():\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        \n",
    "        overall_results[eval_type] = np.mean(list(eval_ap_results.values()), 0)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            overall_results['AOS'] = np.mean(list(eval_aos_results.values()), 0)\n",
    "    \n",
    "    print(f'\\n==========Overall==========')\n",
    "    print(f'\\n==========Overall==========', file=f)\n",
    "    for k, v in overall_results.items():\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_losses(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict):\n",
    "    \n",
    "    ################# Full features #################\n",
    "    bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "    batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "    pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    bbox_pred0 = bbox_pred0[pos_idx]\n",
    "    batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "    batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "    num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "    bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                            bbox_pred=bbox_pred0,\n",
    "                            bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                            batched_labels=batched_bbox_labels, \n",
    "                            num_cls_pos=num_cls_pos, \n",
    "                            batched_bbox_reg=batched_bbox_reg0, \n",
    "                            batched_dir_labels=batched_dir_labels)\n",
    "\n",
    "    return loss_dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(model, lambd = 1.01):\n",
    "\n",
    "    A = model.pillar_encoder.global_masks[0]\n",
    "    # B = torch.tensor([lambd ** i for i in range(len(A))], dtype=torch.float32).unsqueeze(0).to(A.device)\n",
    "    _, indices = torch.sort(A, dim=1, descending=True)\n",
    "    B = torch.pow(lambd, indices)\n",
    "    regularized_loss = torch.sum(A * B)\n",
    "\n",
    "\n",
    "    A = model.neck.global_masks[0]\n",
    "    _, indices = torch.sort(A, dim=1, descending=True)\n",
    "    B = torch.pow(lambd, indices)\n",
    "    regularized_loss += torch.sum(A * B)\n",
    "\n",
    "\n",
    "    for A in model.backbone.global_masks:\n",
    "        _, indices = torch.sort(A, dim=1, descending=True)\n",
    "        B = torch.pow(lambd, indices)\n",
    "        regularized_loss += torch.sum(A * B)\n",
    "\n",
    "\n",
    "    return regularized_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict):\n",
    "    bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "    batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "    pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    bbox_pred0 = bbox_pred0[pos_idx]\n",
    "\n",
    "    batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "\n",
    "\n",
    "    batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "\n",
    "    num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "\n",
    "    bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                            bbox_pred=bbox_pred0,\n",
    "                            bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                            batched_labels=batched_bbox_labels, \n",
    "                            num_cls_pos=num_cls_pos, \n",
    "                            batched_bbox_reg=batched_bbox_reg0, \n",
    "                            batched_dir_labels=batched_dir_labels)\n",
    "    return loss_dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # pointpillars.load_state_dict(torch.load(\"logs/pillar_sequence_memory_gating_binary/checkpoints/epoch_60.pth\"))\n",
    "checkpoint = torch.load(\"logs/pillar_sequence_mem_lin_gat_9_param_share_test/checkpoints/epoch_3.pth.tar\")\n",
    "pointpillars.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch0 = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2633 [00:00<?, ?it/s]/home/sayeed/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      " 19%|█▉        | 501/2633 [05:36<23:52,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5475603.677957535]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and Formatting the results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/433 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 391/2633 [04:28<25:41,  1.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 98\u001b[0m\n\u001b[1;32m     92\u001b[0m bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0 \u001b[38;5;241m=\u001b[39m pointpillars(batched_pts\u001b[38;5;241m=\u001b[39mbatched_pts, \n\u001b[1;32m     93\u001b[0m                             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     94\u001b[0m                             batched_gt_bboxes\u001b[38;5;241m=\u001b[39mbatched_gt_bboxes, \n\u001b[1;32m     95\u001b[0m                             batched_gt_labels\u001b[38;5;241m=\u001b[39mbatched_labels, level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     97\u001b[0m loss_dict0 \u001b[38;5;241m=\u001b[39m bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mloss_dict0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m loss_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m current_loss\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "training_loss0 = []\n",
    "\n",
    "\n",
    "epoch0 = 0\n",
    "global_step=0\n",
    "\n",
    "for epoch in range(epoch0, args.max_epoch):\n",
    "# for epoch in range(1):\n",
    "    epoch_loss0 = 0\n",
    "    epoch_gLoss0 = 0\n",
    "\n",
    "    train_indices = np.random.randint(train_dataset_length, size=train_dataset_batch_count)\n",
    "\n",
    "    print('=' * 20, epoch, '=' * 20)\n",
    "\n",
    "    train_step, val_step = 0, 0\n",
    "\n",
    "    pointpillars.train()\n",
    "\n",
    "    for i, data_dict in enumerate(tqdm(train_dataloader)):\n",
    "        if i > 500:\n",
    "            break\n",
    "        if not args.no_cuda:\n",
    "            # move the tensors to the cuda\n",
    "            for key in data_dict:\n",
    "                for j, item in enumerate(data_dict[key]):\n",
    "                    if torch.is_tensor(item):\n",
    "                        data_dict[key][j] = data_dict[key][j].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batched_pts = data_dict['batched_pts']\n",
    "        batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "        batched_labels = data_dict['batched_labels']\n",
    "        batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "        ################# Full features #################\n",
    "\n",
    "        # bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0 = pointpillars(batched_pts=batched_pts, \n",
    "        #                             mode='train',\n",
    "        #                             batched_gt_bboxes=batched_gt_bboxes, \n",
    "        #                             batched_gt_labels=batched_labels)\n",
    "\n",
    "        # loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n",
    "\n",
    "\n",
    "        # loss = loss_dict0['total_loss']  \n",
    "        # loss.backward()\n",
    "\n",
    "        # bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0, \\\n",
    "        #         bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, anchor_target_dict1, \\\n",
    "        #             bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict2 = pointpillars(batched_pts=batched_pts, \n",
    "        #                             mode='train',\n",
    "        #                             batched_gt_bboxes=batched_gt_bboxes, \n",
    "        #                             batched_gt_labels=batched_labels)\n",
    "\n",
    "        bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0 = pointpillars(batched_pts=batched_pts, \n",
    "                                    mode='train',\n",
    "                                    batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                    batched_gt_labels=batched_labels, level = 0)\n",
    "        \n",
    "        loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n",
    "        # loss_dict1 = bugicugi(bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, anchor_target_dict1)\n",
    "        # loss_dict2 = bugicugi(bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict2)\n",
    "\n",
    "\n",
    "        # loss = loss_dict0['total_loss'] +  loss_dict1['total_loss'] +  loss_dict2['total_loss']  \n",
    "        loss_dict0['total_loss'].backward()\n",
    "        # loss_dict1['total_loss'].backward()\n",
    "        # loss_dict2['total_loss'].backward()\n",
    "        # loss.backward()\n",
    "\n",
    "        current_loss = loss_dict0['total_loss'].item()\n",
    "        if current_loss < 1e5 and current_loss>-1e5:\n",
    "            epoch_loss0 += current_loss\n",
    "\n",
    "        bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0 = pointpillars(batched_pts=batched_pts, \n",
    "                                    mode='train',\n",
    "                                    batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                    batched_gt_labels=batched_labels, level = 1)\n",
    "    \n",
    "        loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n",
    "        loss_dict0['total_loss'].backward()\n",
    "\n",
    "        current_loss = loss_dict0['total_loss'].item()\n",
    "        if current_loss < 1e5 and current_loss>-1e5:\n",
    "            epoch_loss0 += current_loss\n",
    "\n",
    "\n",
    "        bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0 = pointpillars(batched_pts=batched_pts, \n",
    "                                    mode='train',\n",
    "                                    batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                    batched_gt_labels=batched_labels, level = 2)\n",
    "    \n",
    "        loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n",
    "        loss_dict0['total_loss'].backward()\n",
    "\n",
    "        current_loss = loss_dict0['total_loss'].item()\n",
    "        if current_loss < 1e5 and current_loss>-1e5:\n",
    "            epoch_loss0 += current_loss\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(pointpillars.parameters(), max_norm=35)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "        train_step += 1\n",
    "\n",
    "        global_step = 2*train_dataset_batch_count*epoch + train_step\n",
    "\n",
    "        # if global_step % args.log_freq == 0:\n",
    "        #     save_summary(writer, loss_dict, global_step, 'train', p,\n",
    "        #                     lr=optimizer.param_groups[0]['lr'], \n",
    "        #                     momentum=optimizer.param_groups[0]['betas'][0],\n",
    "        #                     model=pointpillars, data=data_cuda, flag=model_flag)\n",
    "\n",
    "    training_loss0.append(epoch_loss0)\n",
    "    print(training_loss0)\n",
    "\n",
    "    if epoch % args.ckpt_freq_epoch == 0:\n",
    "\n",
    "        checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': pointpillars.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }   \n",
    "        torch.save(checkpoint, os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth.tar'))\n",
    "    ###################################### Validation ######################################\n",
    "\n",
    "    if (epoch % args.val_freq_epoch) == 0:\n",
    "    # if (epoch % args.val_freq_epoch) == 0:\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(training_loss0))\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.legend([\"loss\", \"g_loss\"])\n",
    "        plt.show()\n",
    "\n",
    "        ################################### Validation ###################################\n",
    "\n",
    "        pointpillars.eval()\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                format_results = {}\n",
    "                print('Predicting and Formatting the results.')\n",
    "                for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "                    if not args.no_cuda:\n",
    "                        # move the tensors to the cuda\n",
    "                        for key in data_dict:\n",
    "                            for j, item in enumerate(data_dict[key]):\n",
    "                                if torch.is_tensor(item):\n",
    "                                    data_dict[key][j] = data_dict[key][j].cuda()\n",
    "                    \n",
    "                    batched_pts = data_dict['batched_pts']\n",
    "                    batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "                    batched_labels = data_dict['batched_labels']\n",
    "                    batched_difficulty = data_dict['batched_difficulty']\n",
    "                    batch_results0  = pointpillars(batched_pts=batched_pts,\n",
    "                                            mode='val0',\n",
    "                                            batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                            batched_gt_labels=batched_labels)\n",
    "\n",
    "                    for j, result in enumerate(batch_results0):\n",
    "                        format_result = {\n",
    "                            'name': [],\n",
    "                            'truncated': [],\n",
    "                            'occluded': [],\n",
    "                            'alpha': [],\n",
    "                            'bbox': [],\n",
    "                            'dimensions': [],\n",
    "                            'location': [],\n",
    "                            'rotation_y': [],\n",
    "                            'score': []\n",
    "                        }\n",
    "                        \n",
    "                        calib_info = data_dict['batched_calib_info'][j]\n",
    "                        tr_velo_to_cam = calib_info['Tr_velo_to_cam'].astype(np.float32)\n",
    "                        r0_rect = calib_info['R0_rect'].astype(np.float32)\n",
    "                        P2 = calib_info['P2'].astype(np.float32)\n",
    "                        image_shape = data_dict['batched_img_info'][j]['image_shape']\n",
    "                        idx = data_dict['batched_img_info'][j]['image_idx']\n",
    "                        result_filter = keep_bbox_from_image_range(result, tr_velo_to_cam, r0_rect, P2, image_shape)\n",
    "                        result_filter = keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n",
    "\n",
    "                        lidar_bboxes = result_filter['lidar_bboxes']\n",
    "                        labels, scores = result_filter['labels'], result_filter['scores']\n",
    "                        bboxes2d, camera_bboxes = result_filter['bboxes2d'], result_filter['camera_bboxes']\n",
    "                        \n",
    "                        for lidar_bbox, label, score, bbox2d, camera_bbox in \\\n",
    "                            zip(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes):\n",
    "                            format_result['name'].append(LABEL2CLASSES[label])\n",
    "                            format_result['truncated'].append(0.0)\n",
    "                            format_result['occluded'].append(0)\n",
    "                            alpha = camera_bbox[6] - np.arctan2(camera_bbox[0], camera_bbox[2])\n",
    "                            format_result['alpha'].append(alpha)\n",
    "                            format_result['bbox'].append(bbox2d)\n",
    "                            format_result['dimensions'].append(camera_bbox[3:6])\n",
    "                            format_result['location'].append(camera_bbox[:3])\n",
    "                            format_result['rotation_y'].append(camera_bbox[6])\n",
    "                            format_result['score'].append(score)\n",
    "                        \n",
    "                        write_label(format_result, os.path.join(args.saved_path_exact, f'{idx:06d}.txt'))\n",
    "\n",
    "                        format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "                \n",
    "                write_pickle(format_results, os.path.join(args.saved_path, 'results.pkl'))\n",
    "\n",
    "            print('Evaluating.. Please wait several seconds.')\n",
    "            do_eval(format_results, val_dataset.data_infos, CLASSES, args.saved_path)\n",
    "        except:\n",
    "            None\n",
    "        # do_eval(format_results, dict(islice(val_dataset.data_infos.items(), i)), CLASSES, args.saved_path)\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loss0[0] < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and Formatting the results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/433 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/433 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m image_shape \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatched_img_info\u001b[39m\u001b[38;5;124m'\u001b[39m][j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_shape\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     41\u001b[0m idx \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatched_img_info\u001b[39m\u001b[38;5;124m'\u001b[39m][j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 42\u001b[0m result_filter \u001b[38;5;241m=\u001b[39m \u001b[43mkeep_bbox_from_image_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_velo_to_cam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr0_rect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m result_filter \u001b[38;5;241m=\u001b[39m keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n\u001b[1;32m     45\u001b[0m lidar_bboxes \u001b[38;5;241m=\u001b[39m result_filter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlidar_bboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/uncertainty_estimation/point_cloud/PointPillars/utils/process.py:564\u001b[0m, in \u001b[0;36mkeep_bbox_from_image_range\u001b[0;34m(result, tr_velo_to_cam, r0_rect, P2, image_shape)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03mresult: dict(lidar_bboxes, labels, scores)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03mtr_velo_to_cam: shape=(4, 4)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03mreturn: dict(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    562\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image_shape\n\u001b[0;32m--> 564\u001b[0m lidar_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlidar_bboxes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    565\u001b[0m labels \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    566\u001b[0m scores \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "pointpillars.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    format_results = {}\n",
    "    print('Predicting and Formatting the results.')\n",
    "    for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "        if not args.no_cuda:\n",
    "            # move the tensors to the cuda\n",
    "            for key in data_dict:\n",
    "                for j, item in enumerate(data_dict[key]):\n",
    "                    if torch.is_tensor(item):\n",
    "                        data_dict[key][j] = data_dict[key][j].cuda()\n",
    "        \n",
    "        batched_pts = data_dict['batched_pts']\n",
    "        batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "        batched_labels = data_dict['batched_labels']\n",
    "        batched_difficulty = data_dict['batched_difficulty']\n",
    "        batch_results0  = pointpillars(batched_pts=batched_pts,\n",
    "                                mode='val0',\n",
    "                                batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                batched_gt_labels=batched_labels)\n",
    "\n",
    "        for j, result in enumerate(batch_results0):\n",
    "            format_result = {\n",
    "                'name': [],\n",
    "                'truncated': [],\n",
    "                'occluded': [],\n",
    "                'alpha': [],\n",
    "                'bbox': [],\n",
    "                'dimensions': [],\n",
    "                'location': [],\n",
    "                'rotation_y': [],\n",
    "                'score': []\n",
    "            }\n",
    "            \n",
    "            calib_info = data_dict['batched_calib_info'][j]\n",
    "            tr_velo_to_cam = calib_info['Tr_velo_to_cam'].astype(np.float32)\n",
    "            r0_rect = calib_info['R0_rect'].astype(np.float32)\n",
    "            P2 = calib_info['P2'].astype(np.float32)\n",
    "            image_shape = data_dict['batched_img_info'][j]['image_shape']\n",
    "            idx = data_dict['batched_img_info'][j]['image_idx']\n",
    "            result_filter = keep_bbox_from_image_range(result, tr_velo_to_cam, r0_rect, P2, image_shape)\n",
    "            result_filter = keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n",
    "\n",
    "            lidar_bboxes = result_filter['lidar_bboxes']\n",
    "            labels, scores = result_filter['labels'], result_filter['scores']\n",
    "            bboxes2d, camera_bboxes = result_filter['bboxes2d'], result_filter['camera_bboxes']\n",
    "            \n",
    "            for lidar_bbox, label, score, bbox2d, camera_bbox in \\\n",
    "                zip(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes):\n",
    "                format_result['name'].append(LABEL2CLASSES[label])\n",
    "                format_result['truncated'].append(0.0)\n",
    "                format_result['occluded'].append(0)\n",
    "                alpha = camera_bbox[6] - np.arctan2(camera_bbox[0], camera_bbox[2])\n",
    "                format_result['alpha'].append(alpha)\n",
    "                format_result['bbox'].append(bbox2d)\n",
    "                format_result['dimensions'].append(camera_bbox[3:6])\n",
    "                format_result['location'].append(camera_bbox[:3])\n",
    "                format_result['rotation_y'].append(camera_bbox[6])\n",
    "                format_result['score'].append(score)\n",
    "            \n",
    "            write_label(format_result, os.path.join(args.saved_path_exact, f'{idx:06d}.txt'))\n",
    "\n",
    "            format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "    \n",
    "    write_pickle(format_results, os.path.join(args.saved_path, 'results.pkl'))\n",
    "\n",
    "print('Evaluating.. Please wait several seconds.')\n",
    "do_eval(format_results, val_dataset.data_infos, CLASSES, args.saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], [], []), ([], [], [])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch_results0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointpillars.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    format_results = {}\n",
    "    print('Predicting and Formatting the results.')\n",
    "    for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "        if not args.no_cuda:\n",
    "            # move the tensors to the cuda\n",
    "            for key in data_dict:\n",
    "                for j, item in enumerate(data_dict[key]):\n",
    "                    if torch.is_tensor(item):\n",
    "                        data_dict[key][j] = data_dict[key][j].cuda()\n",
    "        \n",
    "        batched_pts = data_dict['batched_pts']\n",
    "        batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "        batched_labels = data_dict['batched_labels']\n",
    "        batched_difficulty = data_dict['batched_difficulty']\n",
    "        batch_results0  = pointpillars(batched_pts=batched_pts,\n",
    "                                mode='val2',\n",
    "                                batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                batched_gt_labels=batched_labels)\n",
    "\n",
    "        for j, result in enumerate(batch_results0):\n",
    "            format_result = {\n",
    "                'name': [],\n",
    "                'truncated': [],\n",
    "                'occluded': [],\n",
    "                'alpha': [],\n",
    "                'bbox': [],\n",
    "                'dimensions': [],\n",
    "                'location': [],\n",
    "                'rotation_y': [],\n",
    "                'score': []\n",
    "            }\n",
    "            \n",
    "            calib_info = data_dict['batched_calib_info'][j]\n",
    "            tr_velo_to_cam = calib_info['Tr_velo_to_cam'].astype(np.float32)\n",
    "            r0_rect = calib_info['R0_rect'].astype(np.float32)\n",
    "            P2 = calib_info['P2'].astype(np.float32)\n",
    "            image_shape = data_dict['batched_img_info'][j]['image_shape']\n",
    "            idx = data_dict['batched_img_info'][j]['image_idx']\n",
    "            result_filter = keep_bbox_from_image_range(result, tr_velo_to_cam, r0_rect, P2, image_shape)\n",
    "            result_filter = keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n",
    "\n",
    "            lidar_bboxes = result_filter['lidar_bboxes']\n",
    "            labels, scores = result_filter['labels'], result_filter['scores']\n",
    "            bboxes2d, camera_bboxes = result_filter['bboxes2d'], result_filter['camera_bboxes']\n",
    "            \n",
    "            for lidar_bbox, label, score, bbox2d, camera_bbox in \\\n",
    "                zip(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes):\n",
    "                format_result['name'].append(LABEL2CLASSES[label])\n",
    "                format_result['truncated'].append(0.0)\n",
    "                format_result['occluded'].append(0)\n",
    "                alpha = camera_bbox[6] - np.arctan2(camera_bbox[0], camera_bbox[2])\n",
    "                format_result['alpha'].append(alpha)\n",
    "                format_result['bbox'].append(bbox2d)\n",
    "                format_result['dimensions'].append(camera_bbox[3:6])\n",
    "                format_result['location'].append(camera_bbox[:3])\n",
    "                format_result['rotation_y'].append(camera_bbox[6])\n",
    "                format_result['score'].append(score)\n",
    "            \n",
    "            write_label(format_result, os.path.join(args.saved_path_exact, f'{idx:06d}.txt'))\n",
    "\n",
    "            format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "    \n",
    "    write_pickle(format_results, os.path.join(args.saved_path, 'results.pkl'))\n",
    "\n",
    "print('Evaluating.. Please wait several seconds.')\n",
    "do_eval(format_results, val_dataset.data_infos, CLASSES, args.saved_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
