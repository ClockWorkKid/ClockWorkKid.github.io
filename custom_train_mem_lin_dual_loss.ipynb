{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n",
    "from tensorboardX import SummaryWriter\n",
    "import cv2\n",
    "import io\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed, keep_bbox_from_image_range, \\\n",
    "    keep_bbox_from_lidar_range, write_pickle, write_label, \\\n",
    "    iou2d, iou3d_camera, iou_bev\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the scalars and convert the plot to a tensor image\n",
    "def plot_scalars(scalars, step):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(scalars)\n",
    "    ax.set_title(f'Step {step}')\n",
    "    ax.set_xlabel('Scalar Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Convert the plot to a PNG image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert PNG buffer to a tensor image\n",
    "    image = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)  # Decode the image\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1)#.unsqueeze(0)  # Convert to PyTorch tensor and add batch dimension\n",
    "    return image\n",
    "\n",
    "def save_summary(writer, loss_dict, global_step, tag, gating_prob, lr=None, momentum=None, model=None, data=None, flag=False):\n",
    "    for k, v in loss_dict.items():\n",
    "        writer.add_scalar(f'{tag}/{k}', v, global_step)\n",
    "    if lr is not None:\n",
    "        writer.add_scalar('lr', lr, global_step)\n",
    "    if momentum is not None:\n",
    "        writer.add_scalar('momentum', momentum, global_step)\n",
    "    if model is not None and global_step % 1000 == 0:\n",
    "        for tag, value in model.named_parameters():\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "            if value.grad is not None:\n",
    "                writer.add_histogram(tag + \"/grad\", value.grad.cpu(), global_step)\n",
    "    \n",
    "    if global_step % 100 == 0:\n",
    "        image = plot_scalars(gating_prob.detach().cpu().numpy(), global_step)\n",
    "        writer.add_image(\"Scalars Plot\", image, global_step=global_step)\n",
    "    # for i in data:\n",
    "    #     del i['gt_names']\n",
    "    #     del i['image_info']\n",
    "    #     del i['calib_info']\n",
    "    #     del i['difficulty']\n",
    "\n",
    "    # writer.add_graph(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_root = \"dataset/KITTI\"\n",
    "        self.saved_path = \"logs/pillar_sequence_mem_lin_gat_4\"\n",
    "        self.saved_path_exact = self.saved_path + \"/results_exact\"\n",
    "        self.saved_path_estimate = self.saved_path + \"/results_estimate\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 4\n",
    "        self.nclasses = 3\n",
    "        self.init_lr = 0.00025\n",
    "        self.max_epoch = 200\n",
    "        self.log_freq = 1        \n",
    "        self.ckpt_freq_epoch = 2\n",
    "        self.val_freq_epoch = 5\n",
    "        self.no_cuda = not torch.cuda.is_available()\n",
    " \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_cuda(d):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            # Check if the NumPy array contains a supported type\n",
    "            if value.dtype in [np.float64, np.float32, np.float16, np.int64, np.int32, np.int16, np.int8, np.uint8, np.bool_]:\n",
    "                value = torch.tensor(value)\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            d[key] = value.cuda()\n",
    "    return d\n",
    "\n",
    "# Send all tensors in the list of dictionaries to CUDA\n",
    "# list_of_dicts_cuda = [send_to_cuda(d) for d in data]\n",
    "\n",
    "# # Verify that the tensors are on CUDA\n",
    "# for d in list_of_dicts_cuda:\n",
    "#     for key, value in d.items():\n",
    "#         if isinstance(value, torch.Tensor):\n",
    "#             print(f'{key}: {value.device}')  # Should print 'cuda:0' or another cuda device\n",
    "#         else:\n",
    "#             print(f'{key}: {value}')  # Print non-tensor values as is\n",
    "\n",
    "setup_seed()\n",
    "train_dataset = Kitti(data_root=args.data_root,\n",
    "                        split='train',\n",
    "                        timelength=10)\n",
    "val_dataset = Kitti(data_root=args.data_root,\n",
    "                    split='val',\n",
    "                    timelength=1)\n",
    "\n",
    "# data = train_dataset.__getitem__(9)\n",
    "\n",
    "train_dataset_length = len(train_dataset.sorted_ids)\n",
    "train_dataset_batch_count =  train_dataset_length // train_dataset.timelength\n",
    "val_dataset_length = len(val_dataset.sorted_ids)\n",
    "\n",
    "def get_sequence_from_velodyne_path(file_path):\n",
    "    parts = file_path.split('/')\n",
    "    file_name = parts[-1]\n",
    "    extracted_part = file_name.split('_')[0]\n",
    "    return extracted_part\n",
    "\n",
    "# Print the extracted part\n",
    "\n",
    "CLASSES = Kitti.CLASSES\n",
    "LABEL2CLASSES = {v:k for k, v in CLASSES.items()}\n",
    "\n",
    "\n",
    "pcd_limit_range = np.array([0, -40, -3, 70.4, 40, 0.0], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, Optimizer, Scheduler, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.no_cuda:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses).cuda()\n",
    "else:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses)\n",
    "\n",
    "model_flag = False\n",
    "\n",
    "loss_func = Loss()\n",
    "\n",
    "max_iters = 2* train_dataset_batch_count * args.max_epoch\n",
    "init_lr = args.init_lr\n",
    "optimizer = torch.optim.AdamW(params=pointpillars.parameters(), \n",
    "                                lr=init_lr, \n",
    "                                betas=(0.95, 0.99),\n",
    "                                weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,  \n",
    "                                                max_lr=init_lr*10, \n",
    "                                                total_steps=max_iters, \n",
    "                                                pct_start=0.4, \n",
    "                                                anneal_strategy='cos',\n",
    "                                                cycle_momentum=True, \n",
    "                                                base_momentum=0.95*0.895, \n",
    "                                                max_momentum=0.95,\n",
    "                                                div_factor=10)\n",
    "\n",
    "\n",
    "saved_logs_path = os.path.join(args.saved_path, 'summary')\n",
    "import shutil\n",
    "if os.path.exists(saved_logs_path):\n",
    "    shutil.rmtree(saved_logs_path)\n",
    "os.makedirs(saved_logs_path, exist_ok=True)\n",
    "writer = SummaryWriter(saved_logs_path)\n",
    "saved_ckpt_path = os.path.join(args.saved_path, 'checkpoints')\n",
    "os.makedirs(saved_ckpt_path, exist_ok=True)\n",
    "\n",
    "# Directory for exact results\n",
    "saved_path_exact = args.saved_path_exact\n",
    "os.makedirs(saved_path_exact, exist_ok=True)\n",
    "saved_submit_path_exact = os.path.join(saved_path_exact, 'submit')\n",
    "os.makedirs(saved_submit_path_exact, exist_ok=True)\n",
    "\n",
    "# Directory for estimate results\n",
    "saved_path_estimate = args.saved_path_estimate\n",
    "os.makedirs(saved_path_estimate, exist_ok=True)\n",
    "saved_submit_path_estimate = os.path.join(saved_path_estimate, 'submit')\n",
    "os.makedirs(saved_submit_path_estimate, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_thresholds(tp_scores, total_num_valid_gt, num_sample_pts=41):\n",
    "    score_thresholds = []\n",
    "    tp_scores = sorted(tp_scores)[::-1]\n",
    "    cur_recall, pts_ind = 0, 0\n",
    "    for i, score in enumerate(tp_scores):\n",
    "        lrecall = (i + 1) / total_num_valid_gt\n",
    "        rrecall = (i + 2) / total_num_valid_gt\n",
    "\n",
    "        if i == len(tp_scores) - 1:\n",
    "            score_thresholds.append(score)\n",
    "            break\n",
    "\n",
    "        if (lrecall + rrecall) / 2 < cur_recall:\n",
    "            continue\n",
    "\n",
    "        score_thresholds.append(score)\n",
    "        pts_ind += 1\n",
    "        cur_recall = pts_ind / (num_sample_pts - 1)\n",
    "    return score_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(det_results, gt_results, CLASSES, saved_path):\n",
    "    '''\n",
    "    det_results: list,\n",
    "    gt_results: dict(id -> det_results)\n",
    "    CLASSES: dict\n",
    "    '''\n",
    "    assert len(det_results) == len(gt_results)\n",
    "    f = open(os.path.join(saved_path, 'eval_results.txt'), 'w')\n",
    "\n",
    "    # 1. calculate iou\n",
    "    ious = {\n",
    "        'bbox_2d': [],\n",
    "        'bbox_bev': [],\n",
    "        'bbox_3d': []\n",
    "    }\n",
    "    ids = list(sorted(gt_results.keys()))\n",
    "    for id in ids:\n",
    "        gt_result = gt_results[id]['annos']\n",
    "        det_result = det_results[id]\n",
    "\n",
    "        # 1.1, 2d bboxes iou\n",
    "        gt_bboxes2d = gt_result['bbox'].astype(np.float32)\n",
    "        det_bboxes2d = det_result['bbox'].astype(np.float32)\n",
    "        iou2d_v = iou2d(torch.from_numpy(gt_bboxes2d).cuda(), torch.from_numpy(det_bboxes2d).cuda())\n",
    "        ious['bbox_2d'].append(iou2d_v.cpu().numpy())\n",
    "\n",
    "        # 1.2, bev iou\n",
    "        gt_location = gt_result['location'].astype(np.float32)\n",
    "        gt_dimensions = gt_result['dimensions'].astype(np.float32)\n",
    "        gt_rotation_y = gt_result['rotation_y'].astype(np.float32)\n",
    "        det_location = det_result['location'].astype(np.float32)\n",
    "        det_dimensions = det_result['dimensions'].astype(np.float32)\n",
    "        det_rotation_y = det_result['rotation_y'].astype(np.float32)\n",
    "\n",
    "        gt_bev = np.concatenate([gt_location[:, [0, 2]], gt_dimensions[:, [0, 2]], gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bev = np.concatenate([det_location[:, [0, 2]], det_dimensions[:, [0, 2]], det_rotation_y[:, None]], axis=-1)\n",
    "        iou_bev_v = iou_bev(torch.from_numpy(gt_bev).cuda(), torch.from_numpy(det_bev).cuda())\n",
    "        ious['bbox_bev'].append(iou_bev_v.cpu().numpy())\n",
    "\n",
    "        # 1.3, 3dbboxes iou\n",
    "        gt_bboxes3d = np.concatenate([gt_location, gt_dimensions, gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bboxes3d = np.concatenate([det_location, det_dimensions, det_rotation_y[:, None]], axis=-1)\n",
    "        iou3d_v = iou3d_camera(torch.from_numpy(gt_bboxes3d).cuda(), torch.from_numpy(det_bboxes3d).cuda())\n",
    "        ious['bbox_3d'].append(iou3d_v.cpu().numpy())\n",
    "\n",
    "    MIN_IOUS = {\n",
    "        'Pedestrian': [0.5, 0.5, 0.5],\n",
    "        'Cyclist': [0.5, 0.5, 0.5],\n",
    "        'Car': [0.7, 0.7, 0.7]\n",
    "    }\n",
    "    MIN_HEIGHT = [40, 25, 25]\n",
    "\n",
    "    overall_results = {}\n",
    "    for e_ind, eval_type in enumerate(['bbox_2d', 'bbox_bev', 'bbox_3d']):\n",
    "        eval_ious = ious[eval_type]\n",
    "        eval_ap_results, eval_aos_results = {}, {}\n",
    "        for cls in CLASSES:\n",
    "            eval_ap_results[cls] = []\n",
    "            eval_aos_results[cls] = []\n",
    "            CLS_MIN_IOU = MIN_IOUS[cls][e_ind]\n",
    "            for difficulty in [0, 1, 2]:\n",
    "                # 1. bbox property\n",
    "                total_gt_ignores, total_det_ignores, total_dc_bboxes, total_scores = [], [], [], []\n",
    "                total_gt_alpha, total_det_alpha = [], []\n",
    "                for id in ids:\n",
    "                    gt_result = gt_results[id]['annos']\n",
    "                    det_result = det_results[id]\n",
    "\n",
    "                    # 1.1 gt bbox property\n",
    "                    cur_gt_names = gt_result['name']\n",
    "                    cur_difficulty = gt_result['difficulty']\n",
    "                    gt_ignores, dc_bboxes = [], []\n",
    "                    for j, cur_gt_name in enumerate(cur_gt_names):\n",
    "                        ignore = cur_difficulty[j] < 0 or cur_difficulty[j] > difficulty\n",
    "                        if cur_gt_name == cls:\n",
    "                            valid_class = 1\n",
    "                        elif cls == 'Pedestrian' and cur_gt_name == 'Person_sitting':\n",
    "                            valid_class = 0\n",
    "                        elif cls == 'Car' and cur_gt_name == 'Van':\n",
    "                            valid_class = 0\n",
    "                        else:\n",
    "                            valid_class = -1\n",
    "                        \n",
    "                        if valid_class == 1 and not ignore:\n",
    "                            gt_ignores.append(0)\n",
    "                        elif valid_class == 0 or (valid_class == 1 and ignore):\n",
    "                            gt_ignores.append(1)\n",
    "                        else:\n",
    "                            gt_ignores.append(-1)\n",
    "                        \n",
    "                        if cur_gt_name == 'DontCare':\n",
    "                            dc_bboxes.append(gt_result['bbox'][j])\n",
    "                    total_gt_ignores.append(gt_ignores)\n",
    "                    total_dc_bboxes.append(np.array(dc_bboxes))\n",
    "                    total_gt_alpha.append(gt_result['alpha'])\n",
    "\n",
    "                    # 1.2 det bbox property\n",
    "                    cur_det_names = det_result['name']\n",
    "                    cur_det_heights = det_result['bbox'][:, 3] - det_result['bbox'][:, 1]\n",
    "                    det_ignores = []\n",
    "                    for j, cur_det_name in enumerate(cur_det_names):\n",
    "                        if cur_det_heights[j] < MIN_HEIGHT[difficulty]:\n",
    "                            det_ignores.append(1)\n",
    "                        elif cur_det_name == cls:\n",
    "                            det_ignores.append(0)\n",
    "                        else:\n",
    "                            det_ignores.append(-1)\n",
    "                    total_det_ignores.append(det_ignores)\n",
    "                    total_scores.append(det_result['score'])\n",
    "                    total_det_alpha.append(det_result['alpha'])\n",
    "\n",
    "                # 2. calculate scores thresholds for PR curve\n",
    "                tp_scores = []\n",
    "                for i, id in enumerate(ids):\n",
    "                    cur_eval_ious = eval_ious[i]\n",
    "                    gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                    scores = total_scores[i]\n",
    "\n",
    "                    nn, mm = cur_eval_ious.shape\n",
    "                    assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                    for j in range(nn):\n",
    "                        if gt_ignores[j] == -1:\n",
    "                            continue\n",
    "                        match_id, match_score = -1, -1\n",
    "                        for k in range(mm):\n",
    "                            if not assigned[k] and det_ignores[k] >= 0 and cur_eval_ious[j, k] > CLS_MIN_IOU and scores[k] > match_score:\n",
    "                                match_id = k\n",
    "                                match_score = scores[k]\n",
    "                        if match_id != -1:\n",
    "                            assigned[match_id] = True\n",
    "                            if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                tp_scores.append(match_score)\n",
    "                total_num_valid_gt = np.sum([np.sum(np.array(gt_ignores) == 0) for gt_ignores in total_gt_ignores])\n",
    "                score_thresholds = get_score_thresholds(tp_scores, total_num_valid_gt)    \n",
    "            \n",
    "                # 3. draw PR curve and calculate mAP\n",
    "                tps, fns, fps, total_aos = [], [], [], []\n",
    "\n",
    "                for score_threshold in score_thresholds:\n",
    "                    tp, fn, fp = 0, 0, 0\n",
    "                    aos = 0\n",
    "                    for i, id in enumerate(ids):\n",
    "                        cur_eval_ious = eval_ious[i]\n",
    "                        gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                        gt_alpha, det_alpha = total_gt_alpha[i], total_det_alpha[i]\n",
    "                        scores = total_scores[i]\n",
    "\n",
    "                        nn, mm = cur_eval_ious.shape\n",
    "                        assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                        for j in range(nn):\n",
    "                            if gt_ignores[j] == -1:\n",
    "                                continue\n",
    "                            match_id, match_iou = -1, -1\n",
    "                            for k in range(mm):\n",
    "                                if not assigned[k] and det_ignores[k] >= 0 and scores[k] >= score_threshold and cur_eval_ious[j, k] > CLS_MIN_IOU:\n",
    "    \n",
    "                                    if det_ignores[k] == 0 and cur_eval_ious[j, k] > match_iou:\n",
    "                                        match_iou = cur_eval_ious[j, k]\n",
    "                                        match_id = k\n",
    "                                    elif det_ignores[k] == 1 and match_iou == -1:\n",
    "                                        match_id = k\n",
    "\n",
    "                            if match_id != -1:\n",
    "                                assigned[match_id] = True\n",
    "                                if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                    tp += 1\n",
    "                                    if eval_type == 'bbox_2d':\n",
    "                                        aos += (1 + np.cos(gt_alpha[j] - det_alpha[match_id])) / 2\n",
    "                            else:\n",
    "                                if gt_ignores[j] == 0:\n",
    "                                    fn += 1\n",
    "                            \n",
    "                        for k in range(mm):\n",
    "                            if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                fp += 1\n",
    "                        \n",
    "                        # In case 2d bbox evaluation, we should consider dontcare bboxes\n",
    "                        if eval_type == 'bbox_2d':\n",
    "                            dc_bboxes = total_dc_bboxes[i]\n",
    "                            det_bboxes = det_results[id]['bbox']\n",
    "                            if len(dc_bboxes) > 0:\n",
    "                                ious_dc_det = iou2d(torch.from_numpy(det_bboxes), torch.from_numpy(dc_bboxes), metric=1).numpy().T\n",
    "                                for j in range(len(dc_bboxes)):\n",
    "                                    for k in range(len(det_bboxes)):\n",
    "                                        if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                            if ious_dc_det[j, k] > CLS_MIN_IOU:\n",
    "                                                fp -= 1\n",
    "                                                assigned[k] = True\n",
    "                            \n",
    "                    tps.append(tp)\n",
    "                    fns.append(fn)\n",
    "                    fps.append(fp)\n",
    "                    if eval_type == 'bbox_2d':\n",
    "                        total_aos.append(aos)\n",
    "\n",
    "                tps, fns, fps = np.array(tps), np.array(fns), np.array(fps)\n",
    "\n",
    "                precisions = tps / (tps + fns) # actually this is recalls\n",
    "                # precisions = tps / (tps + fps)\n",
    "                for i in range(len(score_thresholds)):\n",
    "                    precisions[i] = np.max(precisions[i:])\n",
    "                \n",
    "                sums_AP = 0\n",
    "                for i in range(0, len(score_thresholds), 4):\n",
    "                    sums_AP += precisions[i]\n",
    "                mAP = sums_AP / 11 * 100\n",
    "                eval_ap_results[cls].append(mAP)\n",
    "\n",
    "                if eval_type == 'bbox_2d':\n",
    "                    total_aos = np.array(total_aos)\n",
    "                    similarity = total_aos / (tps + fps)\n",
    "                    for i in range(len(score_thresholds)):\n",
    "                        similarity[i] = np.max(similarity[i:])\n",
    "                    sums_similarity = 0\n",
    "                    for i in range(0, len(score_thresholds), 4):\n",
    "                        sums_similarity += similarity[i]\n",
    "                    mSimilarity = sums_similarity / 11 * 100\n",
    "                    eval_aos_results[cls].append(mSimilarity)\n",
    "\n",
    "        print(f'=========={eval_type.upper()}==========')\n",
    "        print(f'=========={eval_type.upper()}==========', file=f)\n",
    "        for k, v in eval_ap_results.items():\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            print(f'==========AOS==========')\n",
    "            print(f'==========AOS==========', file=f)\n",
    "            for k, v in eval_aos_results.items():\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        \n",
    "        overall_results[eval_type] = np.mean(list(eval_ap_results.values()), 0)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            overall_results['AOS'] = np.mean(list(eval_aos_results.values()), 0)\n",
    "    \n",
    "    print(f'\\n==========Overall==========')\n",
    "    print(f'\\n==========Overall==========', file=f)\n",
    "    for k, v in overall_results.items():\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_writer(results, data_cuda):\n",
    "    # batched_gt_bboxes = [data_cuda[0]['gt_bboxes_3d']]\n",
    "    # batched_labels = [data_cuda[0]['gt_labels']]\n",
    "    # batched_difficulty = [data_cuda[0]['difficulty']]\n",
    "    # batched_gt_bboxes = [data_cuda[0]['gt_bboxes_3d']]\n",
    "\n",
    "    format_result = {\n",
    "        'name': [],\n",
    "        'truncated': [],\n",
    "        'occluded': [],\n",
    "        'alpha': [],\n",
    "        'bbox': [],\n",
    "        'dimensions': [],\n",
    "        'location': [],\n",
    "        'rotation_y': [],\n",
    "        'score': []\n",
    "    }\n",
    "\n",
    "    calib_info = data_cuda[0]['calib_info']\n",
    "    tr_velo_to_cam = calib_info['Tr_velo_to_cam'].astype(np.float32)\n",
    "    r0_rect = calib_info['R0_rect'].astype(np.float32)\n",
    "    P2 = calib_info['P2'].astype(np.float32)\n",
    "\n",
    "    image_shape = data_cuda[0]['image_info']['image_shape']\n",
    "    idx = data_cuda[0]['image_info']['image_idx']\n",
    "    result_filter = keep_bbox_from_image_range(results[0], tr_velo_to_cam, r0_rect, P2, image_shape)\n",
    "    result_filter = keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n",
    "\n",
    "    lidar_bboxes = result_filter['lidar_bboxes']\n",
    "    labels, scores = result_filter['labels'], result_filter['scores']\n",
    "    bboxes2d, camera_bboxes = result_filter['bboxes2d'], result_filter['camera_bboxes']\n",
    "\n",
    "    for lidar_bbox, label, score, bbox2d, camera_bbox in \\\n",
    "        zip(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes):\n",
    "        format_result['name'].append(LABEL2CLASSES[label])\n",
    "        format_result['truncated'].append(0.0)\n",
    "        format_result['occluded'].append(0)\n",
    "        alpha = camera_bbox[6] - np.arctan2(camera_bbox[0], camera_bbox[2])\n",
    "        format_result['alpha'].append(alpha)\n",
    "        format_result['bbox'].append(bbox2d)\n",
    "        format_result['dimensions'].append(camera_bbox[3:6])\n",
    "        format_result['location'].append(camera_bbox[:3])\n",
    "        format_result['rotation_y'].append(camera_bbox[6])\n",
    "        format_result['score'].append(score)\n",
    "\n",
    "    return format_result, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_losses(results, mode=None):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for result in results:\n",
    "\n",
    "        bbox_cls_pred0 = torch.cat([result['bbox_cls_pred']],dim=0)\n",
    "        bbox_pred0 = torch.cat([result['bbox_pred']],dim=0)\n",
    "        bbox_dir_cls_pred0 = torch.cat([result['bbox_dir_cls_pred']],dim=0)\n",
    "        anchor_target_dict_list = [result['anchor_target_dict']]\n",
    "        y = torch.cat([result['y']],dim=0)\n",
    "        ye = torch.cat([result['ye']],dim=0)\n",
    "        g = torch.cat([result['q']],dim=0)\n",
    "\n",
    "        # g = g.unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(y.shape)\n",
    "        # print(g.shape)\n",
    "        \n",
    "        ################# Full features #################\n",
    "        bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "        bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "        bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "        batched_bbox_labels = torch.cat([anchor_target_dict['batched_labels'].reshape(-1) for anchor_target_dict in anchor_target_dict_list],dim=0)\n",
    "        batched_label_weights = torch.cat([anchor_target_dict['batched_label_weights'].reshape(-1) for anchor_target_dict in anchor_target_dict_list],dim=0)\n",
    "        batched_bbox_reg = torch.cat([anchor_target_dict['batched_bbox_reg'].reshape(-1, 7) for anchor_target_dict in anchor_target_dict_list],dim=0)\n",
    "        batched_dir_labels = torch.cat([anchor_target_dict['batched_dir_labels'].reshape(-1) for anchor_target_dict in anchor_target_dict_list],dim=0)\n",
    "\n",
    "        pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "        bbox_pred0 = bbox_pred0[pos_idx]\n",
    "        batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "        batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "        # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "        bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "        batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "        bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "        batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "        num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "        bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "        batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "        batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "        loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                                bbox_pred=bbox_pred0,\n",
    "                                bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                                batched_labels=batched_bbox_labels, \n",
    "                                num_cls_pos=num_cls_pos, \n",
    "                                batched_bbox_reg=batched_bbox_reg0, \n",
    "                                batched_dir_labels=batched_dir_labels)\n",
    "\n",
    "        # gLoss = torch.sum(p*torch.norm(y-ye, dim=(1,2,3)))\n",
    "        # l2 norm\n",
    "        # gLoss = (((1-g)*(y-ye)).pow(2).sum(dim=(1,2,3)).pow(0.5)).sum()\n",
    "        loss_dictionary = {}\n",
    "        gLoss = (((y-ye)).pow(2).sum(dim=(1,2,3)).pow(0.5)).sum()\n",
    "\n",
    "        loss_dictionary['detection_loss'] = loss_dict0\n",
    "        loss_dictionary['probability'] = g\n",
    "        loss_dictionary['distance'] = gLoss\n",
    "        \n",
    "        losses.append(loss_dictionary)\n",
    "\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pointpillars.load_state_dict(torch.load(\"logs/pillar_sequence_memory_gating_binary/checkpoints/epoch_60.pth\"))\n",
    "# checkpoint = torch.load(\"logs/pillar_sequence_mem_lin_gat/checkpoints/epoch_196.pth.tar\")\n",
    "# pointpillars.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch0 = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# training_loss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/train_loss.npy\").tolist()\n",
    "# training_gLoss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/train_gloss.npy\").tolist()\n",
    "# val_loss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/val_loss.npy\").tolist()\n",
    "# val_gLoss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/val_gloss.npy\").tolist()\n",
    "\n",
    "training_loss0 = []\n",
    "training_gLoss0 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/526 [00:00<?, ?it/s]/home/sayeed/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 526/526 [10:42<00:00,  1.22s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 0.5952 0.5102 0.5102\n",
      "Cyclist AP@0.5: 4.5455 18.5407 18.5407\n",
      "Car AP@0.7: 29.5612 19.3665 17.9709\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 0.1133 0.1133 0.1133\n",
      "Cyclist AOS@0.5: 0.0189 0.1708 0.1708\n",
      "Car AOS@0.7: 28.1554 20.8878 19.0988\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 0.1082 0.1855 0.1855\n",
      "Cyclist AP@0.5: 4.5455 6.1005 6.1005\n",
      "Car AP@0.7: 20.5510 17.6410 12.9647\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 0.0000 0.0000 0.0000\n",
      "Cyclist AP@0.5: 1.1364 0.4785 0.4785\n",
      "Car AP@0.7: 5.0296 2.3796 2.1638\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 11.5673 12.8058 12.3406\n",
      "AOS AP: 9.4292 7.0573 6.4610\n",
      "bbox_bev AP: 8.4016 7.9757 6.4169\n",
      "bbox_3d AP: 2.0553 0.9527 0.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 0.5952 0.5102 0.5102\n",
      "Cyclist AP@0.5: 4.5455 18.5407 18.5407\n",
      "Car AP@0.7: 29.5612 19.3665 17.9709\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 0.1133 0.1133 0.1133\n",
      "Cyclist AOS@0.5: 0.0189 0.1708 0.1708\n",
      "Car AOS@0.7: 28.1554 20.8878 19.0988\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 0.1082 0.1855 0.1855\n",
      "Cyclist AP@0.5: 4.5455 6.1005 6.1005\n",
      "Car AP@0.7: 20.5510 17.6410 12.9647\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 0.0000 0.0000 0.0000\n",
      "Cyclist AP@0.5: 1.1364 0.4785 0.4785\n",
      "Car AP@0.7: 5.0296 2.3796 2.1638\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 11.5673 12.8058 12.3406\n",
      "AOS AP: 9.4292 7.0573 6.4610\n",
      "bbox_bev AP: 8.4016 7.9757 6.4169\n",
      "bbox_3d AP: 2.0553 0.9527 0.8808\n",
      "==================== 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:32<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 3 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:37<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 4 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:36<00:00,  1.21s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 11.0390 10.3896 10.3896\n",
      "Cyclist AP@0.5: 18.1818 71.0526 71.0526\n",
      "Car AP@0.7: 89.6170 83.7089 81.0437\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 1.8811 1.8739 1.8739\n",
      "Cyclist AOS@0.5: 0.1781 14.9152 14.9152\n",
      "Car AOS@0.7: 90.2775 86.8353 83.8393\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 2.4892 2.7829 2.7829\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.4790 71.1888 68.9563\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 0.2165 0.1855 0.1855\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 54.6556 42.2797 39.9697\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 39.6126 55.0504 54.1620\n",
      "AOS AP: 30.7789 34.5415 33.5428\n",
      "bbox_bev AP: 35.7167 46.5472 45.8030\n",
      "bbox_3d AP: 24.3513 26.7149 25.9449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 11.0390 10.3896 10.3896\n",
      "Cyclist AP@0.5: 18.1818 71.0526 71.0526\n",
      "Car AP@0.7: 89.6170 83.7089 81.0437\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 1.8811 1.8739 1.8739\n",
      "Cyclist AOS@0.5: 0.1781 14.9152 14.9152\n",
      "Car AOS@0.7: 90.2775 86.8353 83.8393\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 2.4892 2.7829 2.7829\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.4790 71.1888 68.9563\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 0.2165 0.1855 0.1855\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 54.6556 42.2797 39.9697\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 39.6126 55.0504 54.1620\n",
      "AOS AP: 30.7789 34.5415 33.5428\n",
      "bbox_bev AP: 35.7167 46.5472 45.8030\n",
      "bbox_3d AP: 24.3513 26.7149 25.9449\n",
      "==================== 6 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 7 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:36<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 8 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 9 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:33<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 10 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:33<00:00,  1.20s/it]\n",
      "100%|██████████| 866/866 [00:52<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 5.1948 5.0093 5.0093\n",
      "Cyclist AP@0.5: 18.1818 83.7321 83.7321\n",
      "Car AP@0.7: 87.7462 71.3179 69.2912\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 0.6033 0.6210 0.6210\n",
      "Cyclist AOS@0.5: 0.1170 17.9160 17.9160\n",
      "Car AOS@0.7: 89.9073 79.2976 78.3968\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 6.9805 6.9573 6.9573\n",
      "Cyclist AP@0.5: 18.1818 67.8230 67.8230\n",
      "Car AP@0.7: 85.2160 71.0673 68.0300\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 1.5152 1.9481 1.9481\n",
      "Cyclist AP@0.5: 18.1818 41.0287 41.0287\n",
      "Car AP@0.7: 53.1552 40.1743 37.9056\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 37.0410 53.3531 52.6775\n",
      "AOS AP: 30.2092 32.6115 32.3113\n",
      "bbox_bev AP: 36.7928 48.6159 47.6034\n",
      "bbox_3d AP: 24.2841 27.7170 26.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 5.1948 5.0093 5.0093\n",
      "Cyclist AP@0.5: 18.1818 83.7321 83.7321\n",
      "Car AP@0.7: 87.7462 71.3179 69.2912\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 0.6033 0.6210 0.6210\n",
      "Cyclist AOS@0.5: 0.1170 17.9160 17.9160\n",
      "Car AOS@0.7: 89.9073 79.2976 78.3968\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 6.9805 6.9573 6.9573\n",
      "Cyclist AP@0.5: 18.1818 67.8230 67.8230\n",
      "Car AP@0.7: 85.2160 71.0673 68.0300\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 1.5152 1.9481 1.9481\n",
      "Cyclist AP@0.5: 18.1818 41.0287 41.0287\n",
      "Car AP@0.7: 53.1552 40.1743 37.9056\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 37.0410 53.3531 52.6775\n",
      "AOS AP: 30.2092 32.6115 32.3113\n",
      "bbox_bev AP: 36.7928 48.6159 47.6034\n",
      "bbox_3d AP: 24.2841 27.7170 26.9608\n",
      "==================== 11 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:33<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 12 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 13 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:33<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 14 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 15 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 12.7706 12.4304 12.4304\n",
      "Cyclist AP@0.5: 18.1818 53.5885 53.5885\n",
      "Car AP@0.7: 89.8060 79.9711 69.7107\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 7.5584 7.2415 7.2415\n",
      "Cyclist AOS@0.5: 0.0014 4.8810 4.8810\n",
      "Car AOS@0.7: 90.2698 84.3833 78.9258\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 13.2035 12.8015 12.8015\n",
      "Cyclist AP@0.5: 18.1818 40.1914 40.1914\n",
      "Car AP@0.7: 86.9565 80.3659 70.0589\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 7.3052 7.3748 7.3748\n",
      "Cyclist AP@0.5: 18.1818 28.7081 28.7081\n",
      "Car AP@0.7: 67.2254 50.2577 41.8677\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 40.2528 48.6634 45.2432\n",
      "AOS AP: 32.6098 32.1686 30.3494\n",
      "bbox_bev AP: 39.4473 44.4529 41.0173\n",
      "bbox_3d AP: 30.9041 28.7802 25.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 12.7706 12.4304 12.4304\n",
      "Cyclist AP@0.5: 18.1818 53.5885 53.5885\n",
      "Car AP@0.7: 89.8060 79.9711 69.7107\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 7.5584 7.2415 7.2415\n",
      "Cyclist AOS@0.5: 0.0014 4.8810 4.8810\n",
      "Car AOS@0.7: 90.2698 84.3833 78.9258\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 13.2035 12.8015 12.8015\n",
      "Cyclist AP@0.5: 18.1818 40.1914 40.1914\n",
      "Car AP@0.7: 86.9565 80.3659 70.0589\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 7.3052 7.3748 7.3748\n",
      "Cyclist AP@0.5: 18.1818 28.7081 28.7081\n",
      "Car AP@0.7: 67.2254 50.2577 41.8677\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 40.2528 48.6634 45.2432\n",
      "AOS AP: 32.6098 32.1686 30.3494\n",
      "bbox_bev AP: 39.4473 44.4529 41.0173\n",
      "bbox_3d AP: 30.9041 28.7802 25.9835\n",
      "==================== 16 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 17 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 18 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:33<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 19 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 20 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:29<00:00,  1.20s/it]\n",
      "100%|██████████| 866/866 [00:52<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 7.4675 7.0965 7.0965\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 89.3729 84.8969 82.8877\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 10.9902 10.6621 10.6621\n",
      "Cyclist AOS@0.5: 0.1146 18.0542 18.0542\n",
      "Car AOS@0.7: 89.7150 87.0493 86.1799\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 19.2100 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 86.7522 81.8389 71.6946\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 10.6061 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 30.1435 30.1435\n",
      "Car AP@0.7: 64.2683 51.0084 43.0576\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 38.3407 52.5544 51.8847\n",
      "AOS AP: 33.6066 38.5885 38.2987\n",
      "bbox_bev AP: 41.3813 47.3420 43.9605\n",
      "bbox_3d AP: 31.0187 30.4520 27.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 7.4675 7.0965 7.0965\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 89.3729 84.8969 82.8877\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 10.9902 10.6621 10.6621\n",
      "Cyclist AOS@0.5: 0.1146 18.0542 18.0542\n",
      "Car AOS@0.7: 89.7150 87.0493 86.1799\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 19.2100 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 86.7522 81.8389 71.6946\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 10.6061 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 30.1435 30.1435\n",
      "Car AP@0.7: 64.2683 51.0084 43.0576\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 38.3407 52.5544 51.8847\n",
      "AOS AP: 33.6066 38.5885 38.2987\n",
      "bbox_bev AP: 41.3813 47.3420 43.9605\n",
      "bbox_3d AP: 31.0187 30.4520 27.8017\n",
      "==================== 21 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 22 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:28<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 23 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 24 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 25 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:27<00:00,  1.19s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 5.6818 5.5659 5.5659\n",
      "Cyclist AP@0.5: 18.1818 69.9761 69.9761\n",
      "Car AP@0.7: 99.3056 84.9071 82.8999\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 9.8324 9.8086 9.8086\n",
      "Cyclist AOS@0.5: 0.9624 18.4672 18.4672\n",
      "Car AOS@0.7: 98.6808 88.5129 87.4935\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 20.5628 19.4805 19.4805\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 87.0310 81.5493 71.2597\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 11.6883 11.1317 11.1317\n",
      "Cyclist AP@0.5: 18.1818 20.9330 20.9330\n",
      "Car AP@0.7: 68.1142 54.0909 51.9901\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 41.0564 53.4830 52.8139\n",
      "AOS AP: 36.4919 38.9296 38.5898\n",
      "bbox_bev AP: 41.9252 46.2364 42.8065\n",
      "bbox_3d AP: 32.6615 28.7185 28.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 5.6818 5.5659 5.5659\n",
      "Cyclist AP@0.5: 18.1818 69.9761 69.9761\n",
      "Car AP@0.7: 99.3056 84.9071 82.8999\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 9.8324 9.8086 9.8086\n",
      "Cyclist AOS@0.5: 0.9624 18.4672 18.4672\n",
      "Car AOS@0.7: 98.6808 88.5129 87.4935\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 20.5628 19.4805 19.4805\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 87.0310 81.5493 71.2597\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 11.6883 11.1317 11.1317\n",
      "Cyclist AP@0.5: 18.1818 20.9330 20.9330\n",
      "Car AP@0.7: 68.1142 54.0909 51.9901\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 41.0564 53.4830 52.8139\n",
      "AOS AP: 36.4919 38.9296 38.5898\n",
      "bbox_bev AP: 41.9252 46.2364 42.8065\n",
      "bbox_3d AP: 32.6615 28.7185 28.0183\n",
      "==================== 26 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 27 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 28 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 29 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 30 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:28<00:00,  1.20s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.4304 12.4304\n",
      "Cyclist AP@0.5: 18.1818 68.8995 68.8995\n",
      "Car AP@0.7: 98.7129 84.9623 83.1800\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.7496 4.1778 4.1778\n",
      "Cyclist AOS@0.5: 0.0153 23.9019 23.9019\n",
      "Car AOS@0.7: 98.4897 87.9698 87.1085\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 21.1039 19.7124 19.7124\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 87.5788 82.7857 80.3222\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.6160 12.6160\n",
      "Cyclist AP@0.5: 18.1818 27.9904 27.9904\n",
      "Car AP@0.7: 68.4518 53.6276 51.2505\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 43.3661 55.4307 54.8366\n",
      "AOS AP: 34.0849 38.6832 38.3961\n",
      "bbox_bev AP: 42.2882 48.1214 47.3002\n",
      "bbox_3d AP: 33.2790 31.4113 30.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.4304 12.4304\n",
      "Cyclist AP@0.5: 18.1818 68.8995 68.8995\n",
      "Car AP@0.7: 98.7129 84.9623 83.1800\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.7496 4.1778 4.1778\n",
      "Cyclist AOS@0.5: 0.0153 23.9019 23.9019\n",
      "Car AOS@0.7: 98.4897 87.9698 87.1085\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 21.1039 19.7124 19.7124\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 87.5788 82.7857 80.3222\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.6160 12.6160\n",
      "Cyclist AP@0.5: 18.1818 27.9904 27.9904\n",
      "Car AP@0.7: 68.4518 53.6276 51.2505\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 43.3661 55.4307 54.8366\n",
      "AOS AP: 34.0849 38.6832 38.3961\n",
      "bbox_bev AP: 42.2882 48.1214 47.3002\n",
      "bbox_3d AP: 33.2790 31.4113 30.6190\n",
      "==================== 31 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:30<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 32 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 33 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 34 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 35 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:29<00:00,  1.20s/it]\n",
      "100%|██████████| 866/866 [00:51<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 20.5628 19.4805 19.4805\n",
      "Cyclist AP@0.5: 18.1818 86.1244 86.1244\n",
      "Car AP@0.7: 99.4048 85.1593 83.2143\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 7.1328 6.3265 6.3265\n",
      "Cyclist AOS@0.5: 0.6609 10.0522 10.0522\n",
      "Car AOS@0.7: 98.1722 88.3649 87.5347\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 27.9221 21.5677 21.5677\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.0390 71.7425 69.9106\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 19.4805 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 69.3994 53.6885 51.6698\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 46.0498 63.5881 62.9397\n",
      "AOS AP: 35.3220 34.9145 34.6378\n",
      "bbox_bev AP: 44.0476 52.9934 52.3827\n",
      "bbox_3d AP: 35.6872 36.5630 35.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:07<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 20.5628 19.4805 19.4805\n",
      "Cyclist AP@0.5: 18.1818 86.1244 86.1244\n",
      "Car AP@0.7: 99.4048 85.1593 83.2143\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 7.1328 6.3265 6.3265\n",
      "Cyclist AOS@0.5: 0.6609 10.0522 10.0522\n",
      "Car AOS@0.7: 98.1722 88.3649 87.5347\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 27.9221 21.5677 21.5677\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.0390 71.7425 69.9106\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 19.4805 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 37.6794 37.6794\n",
      "Car AP@0.7: 69.3994 53.6885 51.6698\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 46.0498 63.5881 62.9397\n",
      "AOS AP: 35.3220 34.9145 34.6378\n",
      "bbox_bev AP: 44.0476 52.9934 52.3827\n",
      "bbox_3d AP: 35.6872 36.5630 35.8901\n",
      "==================== 36 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 37 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:31<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 38 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 39 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:34<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 40 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:17<00:00,  1.17s/it]\n",
      "100%|██████████| 866/866 [00:49<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 19.4805 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 83.7321 83.7321\n",
      "Car AP@0.7: 90.1774 85.7438 83.6014\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.4822 3.5668 3.5668\n",
      "Cyclist AOS@0.5: 2.1817 28.1818 28.1818\n",
      "Car AOS@0.7: 90.4709 88.4996 87.6568\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 30.5195 28.3859 28.3859\n",
      "Cyclist AP@0.5: 18.1818 42.7033 42.7033\n",
      "Car AP@0.7: 87.5353 80.5210 70.2718\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 26.6234 20.8720 20.8720\n",
      "Cyclist AP@0.5: 18.1818 27.9904 27.9904\n",
      "Car AP@0.7: 68.7404 53.2866 50.7622\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 42.6133 62.5989 61.8848\n",
      "AOS AP: 32.0449 40.0827 39.8018\n",
      "bbox_bev AP: 45.4122 50.5367 47.1203\n",
      "bbox_3d AP: 37.8485 34.0497 33.2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:05<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 19.4805 18.3210 18.3210\n",
      "Cyclist AP@0.5: 18.1818 83.7321 83.7321\n",
      "Car AP@0.7: 90.1774 85.7438 83.6014\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.4822 3.5668 3.5668\n",
      "Cyclist AOS@0.5: 2.1817 28.1818 28.1818\n",
      "Car AOS@0.7: 90.4709 88.4996 87.6568\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 30.5195 28.3859 28.3859\n",
      "Cyclist AP@0.5: 18.1818 42.7033 42.7033\n",
      "Car AP@0.7: 87.5353 80.5210 70.2718\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 26.6234 20.8720 20.8720\n",
      "Cyclist AP@0.5: 18.1818 27.9904 27.9904\n",
      "Car AP@0.7: 68.7404 53.2866 50.7622\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 42.6133 62.5989 61.8848\n",
      "AOS AP: 32.0449 40.0827 39.8018\n",
      "bbox_bev AP: 45.4122 50.5367 47.1203\n",
      "bbox_3d AP: 37.8485 34.0497 33.2082\n",
      "==================== 41 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 42 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 43 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:07<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 44 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 45 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.16s/it]\n",
      "100%|██████████| 866/866 [00:49<00:00, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 10.1732 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 67.8230 67.8230\n",
      "Car AP@0.7: 90.2637 84.0236 82.1590\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 6.9713 6.0728 6.0728\n",
      "Cyclist AOS@0.5: 1.7472 29.2633 29.2633\n",
      "Car AOS@0.7: 90.5396 88.3539 84.9255\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 27.2727 21.3358 21.3358\n",
      "Cyclist AP@0.5: 18.1818 51.6746 51.6746\n",
      "Car AP@0.7: 87.5966 81.6834 71.1138\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.0594 12.0594\n",
      "Cyclist AP@0.5: 18.1818 30.8612 30.8612\n",
      "Car AP@0.7: 71.1284 54.8550 52.3416\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 39.5396 54.0169 53.3953\n",
      "AOS AP: 33.0860 41.2300 40.0872\n",
      "bbox_bev AP: 44.3504 51.5646 48.0414\n",
      "bbox_3d AP: 34.1712 32.5919 31.7541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:04<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 10.1732 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 67.8230 67.8230\n",
      "Car AP@0.7: 90.2637 84.0236 82.1590\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 6.9713 6.0728 6.0728\n",
      "Cyclist AOS@0.5: 1.7472 29.2633 29.2633\n",
      "Car AOS@0.7: 90.5396 88.3539 84.9255\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 27.2727 21.3358 21.3358\n",
      "Cyclist AP@0.5: 18.1818 51.6746 51.6746\n",
      "Car AP@0.7: 87.5966 81.6834 71.1138\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 13.2035 12.0594 12.0594\n",
      "Cyclist AP@0.5: 18.1818 30.8612 30.8612\n",
      "Car AP@0.7: 71.1284 54.8550 52.3416\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 39.5396 54.0169 53.3953\n",
      "AOS AP: 33.0860 41.2300 40.0872\n",
      "bbox_bev AP: 44.3504 51.5646 48.0414\n",
      "bbox_3d AP: 34.1712 32.5919 31.7541\n",
      "==================== 46 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:09<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 47 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:13<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 48 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:10<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 49 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 50 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:13<00:00,  1.17s/it]\n",
      "100%|██████████| 866/866 [00:50<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 12.1212 11.1317 11.1317\n",
      "Cyclist AP@0.5: 18.1818 82.5359 82.5359\n",
      "Car AP@0.7: 90.5433 85.1042 83.4870\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 4.4665 4.1354 4.1354\n",
      "Cyclist AOS@0.5: 3.3756 13.0193 13.0193\n",
      "Car AOS@0.7: 90.8007 88.4798 87.4228\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 7.3052 7.3748 7.3748\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.8100 80.8484 70.6932\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 5.1948 5.5659 5.5659\n",
      "Cyclist AP@0.5: 15.9091 41.0287 41.0287\n",
      "Car AP@0.7: 69.9558 54.2051 52.2418\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 40.2821 59.5906 59.0515\n",
      "AOS AP: 32.8809 35.2115 34.8591\n",
      "bbox_bev AP: 37.4323 51.2977 47.9126\n",
      "bbox_3d AP: 30.3532 33.5999 32.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:05<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 12.1212 11.1317 11.1317\n",
      "Cyclist AP@0.5: 18.1818 82.5359 82.5359\n",
      "Car AP@0.7: 90.5433 85.1042 83.4870\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 4.4665 4.1354 4.1354\n",
      "Cyclist AOS@0.5: 3.3756 13.0193 13.0193\n",
      "Car AOS@0.7: 90.8007 88.4798 87.4228\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 7.3052 7.3748 7.3748\n",
      "Cyclist AP@0.5: 18.1818 65.6699 65.6699\n",
      "Car AP@0.7: 86.8100 80.8484 70.6932\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 5.1948 5.5659 5.5659\n",
      "Cyclist AP@0.5: 15.9091 41.0287 41.0287\n",
      "Car AP@0.7: 69.9558 54.2051 52.2418\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 40.2821 59.5906 59.0515\n",
      "AOS AP: 32.8809 35.2115 34.8591\n",
      "bbox_bev AP: 37.4323 51.2977 47.9126\n",
      "bbox_3d AP: 30.3532 33.5999 32.9454\n",
      "==================== 51 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 52 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 53 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 54 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:08<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 55 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:13<00:00,  1.17s/it]\n",
      "100%|██████████| 866/866 [00:49<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 18.3983 17.1614 17.1614\n",
      "Cyclist AP@0.5: 18.1818 84.9282 84.9282\n",
      "Car AP@0.7: 99.3000 85.4763 83.4522\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.1364 3.2235 3.2235\n",
      "Cyclist AOS@0.5: 6.4198 34.5292 34.5292\n",
      "Car AOS@0.7: 99.0929 88.7415 87.9059\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 40.1515 31.4471 31.4471\n",
      "Cyclist AP@0.5: 18.1818 52.6316 52.6316\n",
      "Car AP@0.7: 83.6725 71.0956 69.0458\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 18.9394 17.8571 17.8571\n",
      "Cyclist AP@0.5: 18.1818 38.5167 38.5167\n",
      "Car AP@0.7: 56.6697 50.5785 42.5958\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 45.2934 62.5220 61.8473\n",
      "AOS AP: 36.2164 42.1647 41.8862\n",
      "bbox_bev AP: 47.3353 51.7248 51.0415\n",
      "bbox_3d AP: 31.2636 35.6508 32.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:05<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 18.3983 17.1614 17.1614\n",
      "Cyclist AP@0.5: 18.1818 84.9282 84.9282\n",
      "Car AP@0.7: 99.3000 85.4763 83.4522\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.1364 3.2235 3.2235\n",
      "Cyclist AOS@0.5: 6.4198 34.5292 34.5292\n",
      "Car AOS@0.7: 99.0929 88.7415 87.9059\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 40.1515 31.4471 31.4471\n",
      "Cyclist AP@0.5: 18.1818 52.6316 52.6316\n",
      "Car AP@0.7: 83.6725 71.0956 69.0458\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 18.9394 17.8571 17.8571\n",
      "Cyclist AP@0.5: 18.1818 38.5167 38.5167\n",
      "Car AP@0.7: 56.6697 50.5785 42.5958\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 45.2934 62.5220 61.8473\n",
      "AOS AP: 36.2164 42.1647 41.8862\n",
      "bbox_bev AP: 47.3353 51.7248 51.0415\n",
      "bbox_3d AP: 31.2636 35.6508 32.9899\n",
      "==================== 56 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 57 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 58 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:13<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 59 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 60 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:14<00:00,  1.17s/it]\n",
      "100%|██████████| 866/866 [00:49<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Exact results.. Please wait several seconds.\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 13.2825 12.6160 12.6160\n",
      "Cyclist AP@0.5: 18.1818 57.4163 57.4163\n",
      "Car AP@0.7: 99.1009 85.1108 83.3103\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.3497 2.5019 2.5019\n",
      "Cyclist AOS@0.5: 0.9663 14.0132 14.0132\n",
      "Car AOS@0.7: 95.6522 85.9311 84.8079\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 26.6234 19.7124 19.7124\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 87.2800 81.7509 71.2543\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 11.2554 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 20.9330 20.9330\n",
      "Car AP@0.7: 52.0620 42.4122 40.5690\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 43.5217 51.7143 51.1142\n",
      "AOS AP: 33.3227 34.1487 33.7743\n",
      "bbox_bev AP: 44.0284 47.7765 44.2776\n",
      "bbox_3d AP: 27.1664 24.5164 23.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [01:05<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Estimated results.. Please wait several seconds.\n",
      "Gating count = 0 out of 866\n",
      "==========BBOX_2D==========\n",
      "Pedestrian AP@0.5: 13.2825 12.6160 12.6160\n",
      "Cyclist AP@0.5: 18.1818 57.4163 57.4163\n",
      "Car AP@0.7: 99.1009 85.1108 83.3103\n",
      "==========AOS==========\n",
      "Pedestrian AOS@0.5: 3.3497 2.5019 2.5019\n",
      "Cyclist AOS@0.5: 0.9663 14.0132 14.0132\n",
      "Car AOS@0.7: 95.6522 85.9311 84.8079\n",
      "==========BBOX_BEV==========\n",
      "Pedestrian AP@0.5: 26.6234 19.7124 19.7124\n",
      "Cyclist AP@0.5: 18.1818 41.8660 41.8660\n",
      "Car AP@0.7: 87.2800 81.7509 71.2543\n",
      "==========BBOX_3D==========\n",
      "Pedestrian AP@0.5: 11.2554 10.2041 10.2041\n",
      "Cyclist AP@0.5: 18.1818 20.9330 20.9330\n",
      "Car AP@0.7: 52.0620 42.4122 40.5690\n",
      "\n",
      "==========Overall==========\n",
      "bbox_2d AP: 43.5217 51.7143 51.1142\n",
      "AOS AP: 33.3227 34.1487 33.7743\n",
      "bbox_bev AP: 44.0284 47.7765 44.2776\n",
      "bbox_3d AP: 27.1664 24.5164 23.9020\n",
      "==================== 61 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 62 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 63 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:15<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 64 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:12<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 65 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [10:10<00:00,  1.16s/it]\n",
      "  3%|▎         | 23/866 [00:01<00:49, 16.93it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[1;32m     96\u001b[0m results \u001b[38;5;241m=\u001b[39m pointpillars(data_cuda, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_exact\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m format_result, idx \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m write_label(format_result, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(saved_submit_path_exact, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    102\u001b[0m format_results[idx] \u001b[38;5;241m=\u001b[39m {k:np\u001b[38;5;241m.\u001b[39marray(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m format_result\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mvalidation_writer\u001b[0;34m(results, data_cuda)\u001b[0m\n\u001b[1;32m     24\u001b[0m image_shape \u001b[38;5;241m=\u001b[39m data_cuda[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_info\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_shape\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m idx \u001b[38;5;241m=\u001b[39m data_cuda[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_info\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m result_filter \u001b[38;5;241m=\u001b[39m \u001b[43mkeep_bbox_from_image_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_velo_to_cam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr0_rect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m result_filter \u001b[38;5;241m=\u001b[39m keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n\u001b[1;32m     29\u001b[0m lidar_bboxes \u001b[38;5;241m=\u001b[39m result_filter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlidar_bboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/uncertainty_estimation/point_cloud/PointPillars/utils/process.py:561\u001b[0m, in \u001b[0;36mkeep_bbox_from_image_range\u001b[0;34m(result, tr_velo_to_cam, r0_rect, P2, image_shape)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03mresult: dict(lidar_bboxes, labels, scores)\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03mtr_velo_to_cam: shape=(4, 4)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03mreturn: dict(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    559\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image_shape\n\u001b[0;32m--> 561\u001b[0m lidar_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlidar_bboxes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    562\u001b[0m labels \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    563\u001b[0m scores \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "epoch0 = 0\n",
    "global_step=0\n",
    "\n",
    "for epoch in range(epoch0, args.max_epoch):\n",
    "# for epoch in range(1):\n",
    "    epoch_loss0 = 0\n",
    "    epoch_gLoss0 = 0\n",
    "\n",
    "    train_indices = np.random.randint(train_dataset_length, size=train_dataset_batch_count)\n",
    "\n",
    "    print('=' * 20, epoch, '=' * 20)\n",
    "\n",
    "    train_step, val_step = 0, 0\n",
    "\n",
    "    pointpillars.train()\n",
    "\n",
    "    for step in tqdm(range(train_dataset_batch_count)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load data\n",
    "        data = train_dataset.__getitem__(train_indices[step])\n",
    "        data_cuda = [send_to_cuda(d) for d in data]\n",
    "\n",
    "        # Forward Pass\n",
    "        results, results_est = pointpillars(data_cuda, mode='train')\n",
    "\n",
    "        # Matching etc and loss compute\n",
    "        losses_0 = measure_losses(results)\n",
    "        losses_1 = measure_losses(results_est)\n",
    "\n",
    "        # Backpropagation\n",
    "        lambda_g = 0.00001\n",
    "\n",
    "        loss_total = None\n",
    "        for i in range(len(losses_0)):\n",
    "            loss_0 = losses_0[i]['detection_loss']['total_loss'] * (1-losses_0[i]['probability'])\n",
    "            loss_1 = losses_1[i]['detection_loss']['total_loss'] * losses_1[i]['probability']\n",
    "            loss_2 = losses_0[i]['distance']\n",
    "\n",
    "            if loss_total is None:\n",
    "                loss_total = loss_0 + loss_1\n",
    "            else:\n",
    "                loss_total += loss_0 + loss_1 + lambda_g*loss_2\n",
    "\n",
    "\n",
    "        loss_total.backward()\n",
    "\n",
    "        epoch_loss0 = epoch_loss0 + loss_total\n",
    "        # epoch_gLoss0 = epoch_gLoss0 + lambda_g*gLoss\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(pointpillars.parameters(), max_norm=35)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_step += 1\n",
    "\n",
    "        global_step = 2*train_dataset_batch_count*epoch + train_step\n",
    "\n",
    "        if global_step==0:\n",
    "            model_flag=True\n",
    "        # if global_step % args.log_freq == 0:\n",
    "        #     save_summary(writer, loss_dict, global_step, 'train', p,\n",
    "        #                     lr=optimizer.param_groups[0]['lr'], \n",
    "        #                     momentum=optimizer.param_groups[0]['betas'][0],\n",
    "        #                     model=pointpillars, data=data_cuda, flag=model_flag)\n",
    "\n",
    "    training_loss0.append(epoch_loss0)\n",
    "    # training_gLoss0.append(epoch_gLoss0.detach().cpu())\n",
    "\n",
    "    if epoch % args.ckpt_freq_epoch == 0:\n",
    "\n",
    "        checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': pointpillars.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }   \n",
    "        torch.save(checkpoint, os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth.tar'))\n",
    "    ###################################### Validation ######################################\n",
    "\n",
    "    if epoch % args.val_freq_epoch == 0:\n",
    "\n",
    "        pointpillars.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ######################## validation with exact ##############################\n",
    "            format_results = {}\n",
    "\n",
    "            for step in tqdm(range(val_dataset_length)):\n",
    "\n",
    "                # Load data\n",
    "                data = val_dataset.__getitem__(step)\n",
    "                data_cuda = [send_to_cuda(d) for d in data]\n",
    "\n",
    "                # Forward Pass\n",
    "                results = pointpillars(data_cuda, mode='val_exact')\n",
    "\n",
    "                format_result, idx = validation_writer(results, data_cuda)\n",
    "\n",
    "                write_label(format_result, os.path.join(saved_submit_path_exact, f'{idx:06d}.txt'))\n",
    "\n",
    "                format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "\n",
    "            write_pickle(format_results, os.path.join(saved_path_exact, 'results.pkl'))\n",
    "\n",
    "            print('Evaluating Exact results.. Please wait several seconds.')\n",
    "            try:\n",
    "                do_eval(format_results, val_dataset.data_infos, CLASSES, saved_path_exact)\n",
    "            except:\n",
    "                print(\"Validation failed\")\n",
    "\n",
    "            ######################## validation with estimation ##############################\n",
    "            \n",
    "            format_results = {}\n",
    "\n",
    "            first_velodyne_path = val_dataset.data_infos[val_dataset.sorted_ids[0]]['velodyne_path']\n",
    "            validation_sequence = get_sequence_from_velodyne_path(first_velodyne_path)\n",
    "            pointpillars.gating_count = 0\n",
    "\n",
    "            for step in tqdm(range(val_dataset_length)):\n",
    "\n",
    "                # Load data\n",
    "                data = val_dataset.__getitem__(step)\n",
    "                data_cuda = [send_to_cuda(d) for d in data]\n",
    "\n",
    "                current_velodyne_path = val_dataset.data_infos[val_dataset.sorted_ids[step]]['velodyne_path']\n",
    "                current_validation_sequence = get_sequence_from_velodyne_path(current_velodyne_path)\n",
    "                \n",
    "                if current_validation_sequence != validation_sequence or step==0:\n",
    "                    retain_memory = False\n",
    "                    validation_sequence = current_validation_sequence\n",
    "                else:\n",
    "                    retain_memory = True\n",
    "\n",
    "\n",
    "                # Forward Pass\n",
    "                results = pointpillars(data_cuda, mode='val_estimate', memory=retain_memory)\n",
    "\n",
    "                format_result, idx = validation_writer(results, data_cuda)\n",
    "\n",
    "                write_label(format_result, os.path.join(saved_submit_path_estimate, f'{idx:06d}.txt'))\n",
    "\n",
    "                format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "\n",
    "            write_pickle(format_results, os.path.join(saved_path_estimate, 'results.pkl'))\n",
    "\n",
    "            print('Evaluating Estimated results.. Please wait several seconds.')\n",
    "            print(\"Gating count = \" + str(pointpillars.gating_count) + \" out of \" + str(val_dataset_length))\n",
    "\n",
    "            try:\n",
    "                do_eval(format_results, val_dataset.data_infos, CLASSES, saved_path_estimate)\n",
    "                \n",
    "            except:\n",
    "                print(\"Validation failed\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    #     global_step = epoch * len(train_dataloader) + train_step + 1\n",
    "\n",
    "    #     if global_step % args.log_freq == 0:\n",
    "    #         save_summary(writer, loss_dict0, global_step, 'train',\n",
    "    #                         lr=optimizer.param_groups[0]['lr'], \n",
    "    #                         momentum=optimizer.param_groups[0]['betas'][0])\n",
    "    #     train_step += 1\n",
    "\n",
    "    # training_loss0.append(epoch_loss0)\n",
    "    # training_gLoss0.append(epoch_gLoss0.detach().cpu())\n",
    "\n",
    "    # if epoch % 2 == 0:\n",
    "    #     continue\n",
    "\n",
    "    # pointpillars.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "    #         try:\n",
    "    #             if not args.no_cuda:\n",
    "    #                 # move the tensors to the cuda\n",
    "    #                 for key in data_dict:\n",
    "    #                     for j, item in enumerate(data_dict[key]):\n",
    "    #                         if torch.is_tensor(item):\n",
    "    #                             data_dict[key][j] = data_dict[key][j].cuda()\n",
    "                \n",
    "    #             batched_pts = data_dict['batched_pts']\n",
    "    #             batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "    #             batched_labels = data_dict['batched_labels']\n",
    "    #             batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "    #             bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict, x, xe = pointpillars(batched_pts=batched_pts, \n",
    "    #                                         batched_pts0=batched_pts0, \n",
    "    #                                         mode='train',\n",
    "    #                                         batched_gt_bboxes=batched_gt_bboxes, \n",
    "    #                                         batched_gt_labels=batched_labels)\\\n",
    "    #                 # bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "    #                 #     bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2,  = \\\n",
    "\n",
    "                \n",
    "    #             ################# Full features #################\n",
    "    #             bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    #             bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    #             bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "    #             batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    #             batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    #             batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    #             batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "\n",
    "                \n",
    "    #             pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    #             bbox_pred0 = bbox_pred0[pos_idx]\n",
    "\n",
    "\n",
    "    #             batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    #             batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    #             # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    #             bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    #             batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    #             bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "    #             batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "    #             num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "    #             bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    #             batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    #             batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    #             loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "    #                                     bbox_pred=bbox_pred0,\n",
    "    #                                     bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "    #                                     batched_labels=batched_bbox_labels, \n",
    "    #                                     num_cls_pos=num_cls_pos, \n",
    "    #                                     batched_bbox_reg=batched_bbox_reg0, \n",
    "    #                                     batched_dir_labels=batched_dir_labels)\n",
    "                \n",
    "                \n",
    "    #             loss0 = loss_dict0['total_loss'] \n",
    "                \n",
    "    #             gLoss = torch.norm(x-xe)\n",
    "\n",
    "\n",
    "    #             if not np.isnan(loss0.item()):\n",
    "    #                 val_epoch_loss0 = val_epoch_loss0 + loss0.item()\n",
    "    #                 val_epoch_gLoss0 = val_epoch_gLoss0 + lambda_g*gLoss\n",
    "\n",
    "\n",
    "    #             else:\n",
    "    #                 continue\n",
    "\n",
    "    #             global_step = epoch * len(val_dataloader) + val_step + 1\n",
    "    #             if global_step % args.log_freq == 0:\n",
    "    #                 save_summary(writer, loss_dict0, global_step, 'val')\n",
    "    #             val_step += 1\n",
    "            \n",
    "    #         except:\n",
    "    #             None\n",
    "\n",
    "    # val_loss0.append(val_epoch_loss0)\n",
    "    # val_gLoss0.append(val_epoch_gLoss0.detach().cpu())\n",
    "            \n",
    "    # pointpillars.train()\n",
    "\n",
    "    # if (epoch + 1) % args.ckpt_freq_epoch == 0:\n",
    "    #     torch.save(pointpillars.state_dict(), os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth'))\n",
    "    #     checkpoint = {\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': pointpillars.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'loss': loss\n",
    "    #     }   \n",
    "    #     torch.save(checkpoint, os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth.tar'))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'train_loss.npy'), np.array(training_loss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'train_gloss.npy'), np.array(training_gLoss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'val_loss.npy'), np.array(val_loss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'val_gloss.npy'), np.array(val_gLoss0))\n",
    "\n",
    "    # if (epoch + 1) % args.plot_freq_epoch == 0:\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(training_loss0))\n",
    "plt.plot(np.array(training_gLoss0)*0.01)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend([\"loss\", \"prediction_loss\"])\n",
    "plt.show()\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(np.array(val_loss0))\n",
    "    #     plt.plot(np.array(val_gLoss0))\n",
    "    #     plt.title(\"Validation Loss\")\n",
    "    #     plt.legend([\"loss\",\"prediction_loss\"])\n",
    "    #     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
