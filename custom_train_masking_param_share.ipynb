{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n",
    "from tensorboardX import SummaryWriter\n",
    "import cv2\n",
    "import io\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed, keep_bbox_from_image_range, \\\n",
    "    keep_bbox_from_lidar_range, write_pickle, write_label, \\\n",
    "    iou2d, iou3d_camera, iou_bev\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the scalars and convert the plot to a tensor image\n",
    "def plot_scalars(scalars, step):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(scalars)\n",
    "    ax.set_title(f'Step {step}')\n",
    "    ax.set_xlabel('Scalar Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Convert the plot to a PNG image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert PNG buffer to a tensor image\n",
    "    image = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)  # Decode the image\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1)#.unsqueeze(0)  # Convert to PyTorch tensor and add batch dimension\n",
    "    return image\n",
    "\n",
    "def save_summary(writer, loss_dict, global_step, tag, gating_prob, lr=None, momentum=None, model=None, data=None, flag=False):\n",
    "    for k, v in loss_dict.items():\n",
    "        writer.add_scalar(f'{tag}/{k}', v, global_step)\n",
    "    if lr is not None:\n",
    "        writer.add_scalar('lr', lr, global_step)\n",
    "    if momentum is not None:\n",
    "        writer.add_scalar('momentum', momentum, global_step)\n",
    "    if model is not None and global_step % 1000 == 0:\n",
    "        for tag, value in model.named_parameters():\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "            if value.grad is not None:\n",
    "                writer.add_histogram(tag + \"/grad\", value.grad.cpu(), global_step)\n",
    "    \n",
    "    if global_step % 100 == 0:\n",
    "        image = plot_scalars(gating_prob.detach().cpu().numpy(), global_step)\n",
    "        writer.add_image(\"Scalars Plot\", image, global_step=global_step)\n",
    "    # for i in data:\n",
    "    #     del i['gt_names']\n",
    "    #     del i['image_info']\n",
    "    #     del i['calib_info']\n",
    "    #     del i['difficulty']\n",
    "\n",
    "    # writer.add_graph(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_root = \"dataset/KITTI\"\n",
    "        self.saved_path = \"logs/pillar_sequence_mem_lin_gat_9_param_share_test\"\n",
    "        self.saved_path_exact = self.saved_path + \"/results_exact\"\n",
    "        self.saved_path_estimate = self.saved_path + \"/results_estimate\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 4\n",
    "        self.window_length = 1\n",
    "        self.nclasses = 3\n",
    "        self.init_lr = 0.00025\n",
    "        self.max_epoch = 200\n",
    "        self.log_freq = 1        \n",
    "        self.ckpt_freq_epoch = 2\n",
    "        self.val_freq_epoch = 2\n",
    "        self.no_cuda = not torch.cuda.is_available()\n",
    " \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Kitti(data_root=args.data_root,\n",
    "                        split='train')\n",
    "val_dataset = Kitti(data_root=args.data_root,\n",
    "                    split='val')\n",
    "train_dataloader = get_dataloader(dataset=train_dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    num_workers=args.num_workers,\n",
    "                                    shuffle=True)\n",
    "val_dataloader = get_dataloader(dataset=val_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                num_workers=args.num_workers,\n",
    "                                shuffle=False)\n",
    "\n",
    "# data = train_dataset.__getitem__(9)\n",
    "\n",
    "train_dataset_length = len(train_dataset.sorted_ids)\n",
    "train_dataset_batch_count =  train_dataset_length \n",
    "val_dataset_length = len(val_dataset.sorted_ids)\n",
    "\n",
    "def get_sequence_from_velodyne_path(file_path):\n",
    "    parts = file_path.split('/')\n",
    "    file_name = parts[-1]\n",
    "    extracted_part = file_name.split('_')[0]\n",
    "    return extracted_part\n",
    "\n",
    "# Print the extracted part\n",
    "\n",
    "CLASSES = Kitti.CLASSES\n",
    "LABEL2CLASSES = {v:k for k, v in CLASSES.items()}\n",
    "\n",
    "\n",
    "pcd_limit_range = np.array([0, -40, -3, 70.4, 40, 0.0], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, Optimizer, Scheduler, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.no_cuda:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses).cuda()\n",
    "else:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses)\n",
    "\n",
    "model_flag = False\n",
    "\n",
    "loss_func = Loss()\n",
    "\n",
    "max_iters = 2* train_dataset_batch_count * args.max_epoch\n",
    "init_lr = args.init_lr\n",
    "optimizer = torch.optim.AdamW(params=pointpillars.parameters(), \n",
    "                                lr=init_lr, \n",
    "                                betas=(0.95, 0.99),\n",
    "                                weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,  \n",
    "                                                max_lr=init_lr*10, \n",
    "                                                total_steps=max_iters, \n",
    "                                                pct_start=0.4, \n",
    "                                                anneal_strategy='cos',\n",
    "                                                cycle_momentum=True, \n",
    "                                                base_momentum=0.95*0.895, \n",
    "                                                max_momentum=0.95,\n",
    "                                                div_factor=10)\n",
    "\n",
    "\n",
    "saved_logs_path = os.path.join(args.saved_path, 'summary')\n",
    "import shutil\n",
    "if os.path.exists(saved_logs_path):\n",
    "    shutil.rmtree(saved_logs_path)\n",
    "os.makedirs(saved_logs_path, exist_ok=True)\n",
    "writer = SummaryWriter(saved_logs_path)\n",
    "saved_ckpt_path = os.path.join(args.saved_path, 'checkpoints')\n",
    "os.makedirs(saved_ckpt_path, exist_ok=True)\n",
    "\n",
    "# Directory for exact results\n",
    "saved_path_exact = args.saved_path_exact\n",
    "os.makedirs(saved_path_exact, exist_ok=True)\n",
    "saved_submit_path_exact = os.path.join(saved_path_exact, 'submit')\n",
    "os.makedirs(saved_submit_path_exact, exist_ok=True)\n",
    "\n",
    "# Directory for estimate results\n",
    "saved_path_estimate = args.saved_path_estimate\n",
    "os.makedirs(saved_path_estimate, exist_ok=True)\n",
    "saved_submit_path_estimate = os.path.join(saved_path_estimate, 'submit')\n",
    "os.makedirs(saved_submit_path_estimate, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_thresholds(tp_scores, total_num_valid_gt, num_sample_pts=41):\n",
    "    score_thresholds = []\n",
    "    tp_scores = sorted(tp_scores)[::-1]\n",
    "    cur_recall, pts_ind = 0, 0\n",
    "    for i, score in enumerate(tp_scores):\n",
    "        lrecall = (i + 1) / total_num_valid_gt\n",
    "        rrecall = (i + 2) / total_num_valid_gt\n",
    "\n",
    "        if i == len(tp_scores) - 1:\n",
    "            score_thresholds.append(score)\n",
    "            break\n",
    "\n",
    "        if (lrecall + rrecall) / 2 < cur_recall:\n",
    "            continue\n",
    "\n",
    "        score_thresholds.append(score)\n",
    "        pts_ind += 1\n",
    "        cur_recall = pts_ind / (num_sample_pts - 1)\n",
    "    return score_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(det_results, gt_results, CLASSES, saved_path):\n",
    "    '''\n",
    "    det_results: list,\n",
    "    gt_results: dict(id -> det_results)\n",
    "    CLASSES: dict\n",
    "    '''\n",
    "    assert len(det_results) == len(gt_results)\n",
    "    f = open(os.path.join(saved_path, 'eval_results.txt'), 'w')\n",
    "\n",
    "    # 1. calculate iou\n",
    "    ious = {\n",
    "        'bbox_2d': [],\n",
    "        'bbox_bev': [],\n",
    "        'bbox_3d': []\n",
    "    }\n",
    "    ids = list(sorted(gt_results.keys()))\n",
    "    for id in ids:\n",
    "        gt_result = gt_results[id]['annos']\n",
    "        det_result = det_results[id]\n",
    "\n",
    "        # 1.1, 2d bboxes iou\n",
    "        gt_bboxes2d = gt_result['bbox'].astype(np.float32)\n",
    "        det_bboxes2d = det_result['bbox'].astype(np.float32)\n",
    "        iou2d_v = iou2d(torch.from_numpy(gt_bboxes2d).cuda(), torch.from_numpy(det_bboxes2d).cuda())\n",
    "        ious['bbox_2d'].append(iou2d_v.cpu().numpy())\n",
    "\n",
    "        # 1.2, bev iou\n",
    "        gt_location = gt_result['location'].astype(np.float32)\n",
    "        gt_dimensions = gt_result['dimensions'].astype(np.float32)\n",
    "        gt_rotation_y = gt_result['rotation_y'].astype(np.float32)\n",
    "        det_location = det_result['location'].astype(np.float32)\n",
    "        det_dimensions = det_result['dimensions'].astype(np.float32)\n",
    "        det_rotation_y = det_result['rotation_y'].astype(np.float32)\n",
    "\n",
    "        gt_bev = np.concatenate([gt_location[:, [0, 2]], gt_dimensions[:, [0, 2]], gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bev = np.concatenate([det_location[:, [0, 2]], det_dimensions[:, [0, 2]], det_rotation_y[:, None]], axis=-1)\n",
    "        iou_bev_v = iou_bev(torch.from_numpy(gt_bev).cuda(), torch.from_numpy(det_bev).cuda())\n",
    "        ious['bbox_bev'].append(iou_bev_v.cpu().numpy())\n",
    "\n",
    "        # 1.3, 3dbboxes iou\n",
    "        gt_bboxes3d = np.concatenate([gt_location, gt_dimensions, gt_rotation_y[:, None]], axis=-1)\n",
    "        det_bboxes3d = np.concatenate([det_location, det_dimensions, det_rotation_y[:, None]], axis=-1)\n",
    "        iou3d_v = iou3d_camera(torch.from_numpy(gt_bboxes3d).cuda(), torch.from_numpy(det_bboxes3d).cuda())\n",
    "        ious['bbox_3d'].append(iou3d_v.cpu().numpy())\n",
    "\n",
    "    MIN_IOUS = {\n",
    "        'Pedestrian': [0.5, 0.5, 0.5],\n",
    "        'Cyclist': [0.5, 0.5, 0.5],\n",
    "        'Car': [0.7, 0.7, 0.7]\n",
    "    }\n",
    "    MIN_HEIGHT = [40, 25, 25]\n",
    "\n",
    "    overall_results = {}\n",
    "    for e_ind, eval_type in enumerate(['bbox_2d', 'bbox_bev', 'bbox_3d']):\n",
    "        eval_ious = ious[eval_type]\n",
    "        eval_ap_results, eval_aos_results = {}, {}\n",
    "        for cls in CLASSES:\n",
    "            eval_ap_results[cls] = []\n",
    "            eval_aos_results[cls] = []\n",
    "            CLS_MIN_IOU = MIN_IOUS[cls][e_ind]\n",
    "            for difficulty in [0, 1, 2]:\n",
    "                # 1. bbox property\n",
    "                total_gt_ignores, total_det_ignores, total_dc_bboxes, total_scores = [], [], [], []\n",
    "                total_gt_alpha, total_det_alpha = [], []\n",
    "                for id in ids:\n",
    "                    gt_result = gt_results[id]['annos']\n",
    "                    det_result = det_results[id]\n",
    "\n",
    "                    # 1.1 gt bbox property\n",
    "                    cur_gt_names = gt_result['name']\n",
    "                    cur_difficulty = gt_result['difficulty']\n",
    "                    gt_ignores, dc_bboxes = [], []\n",
    "                    for j, cur_gt_name in enumerate(cur_gt_names):\n",
    "                        ignore = cur_difficulty[j] < 0 or cur_difficulty[j] > difficulty\n",
    "                        if cur_gt_name == cls:\n",
    "                            valid_class = 1\n",
    "                        elif cls == 'Pedestrian' and cur_gt_name == 'Person_sitting':\n",
    "                            valid_class = 0\n",
    "                        elif cls == 'Car' and cur_gt_name == 'Van':\n",
    "                            valid_class = 0\n",
    "                        else:\n",
    "                            valid_class = -1\n",
    "                        \n",
    "                        if valid_class == 1 and not ignore:\n",
    "                            gt_ignores.append(0)\n",
    "                        elif valid_class == 0 or (valid_class == 1 and ignore):\n",
    "                            gt_ignores.append(1)\n",
    "                        else:\n",
    "                            gt_ignores.append(-1)\n",
    "                        \n",
    "                        if cur_gt_name == 'DontCare':\n",
    "                            dc_bboxes.append(gt_result['bbox'][j])\n",
    "                    total_gt_ignores.append(gt_ignores)\n",
    "                    total_dc_bboxes.append(np.array(dc_bboxes))\n",
    "                    total_gt_alpha.append(gt_result['alpha'])\n",
    "\n",
    "                    # 1.2 det bbox property\n",
    "                    cur_det_names = det_result['name']\n",
    "                    cur_det_heights = det_result['bbox'][:, 3] - det_result['bbox'][:, 1]\n",
    "                    det_ignores = []\n",
    "                    for j, cur_det_name in enumerate(cur_det_names):\n",
    "                        if cur_det_heights[j] < MIN_HEIGHT[difficulty]:\n",
    "                            det_ignores.append(1)\n",
    "                        elif cur_det_name == cls:\n",
    "                            det_ignores.append(0)\n",
    "                        else:\n",
    "                            det_ignores.append(-1)\n",
    "                    total_det_ignores.append(det_ignores)\n",
    "                    total_scores.append(det_result['score'])\n",
    "                    total_det_alpha.append(det_result['alpha'])\n",
    "\n",
    "                # 2. calculate scores thresholds for PR curve\n",
    "                tp_scores = []\n",
    "                for i, id in enumerate(ids):\n",
    "                    cur_eval_ious = eval_ious[i]\n",
    "                    gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                    scores = total_scores[i]\n",
    "\n",
    "                    nn, mm = cur_eval_ious.shape\n",
    "                    assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                    for j in range(nn):\n",
    "                        if gt_ignores[j] == -1:\n",
    "                            continue\n",
    "                        match_id, match_score = -1, -1\n",
    "                        for k in range(mm):\n",
    "                            if not assigned[k] and det_ignores[k] >= 0 and cur_eval_ious[j, k] > CLS_MIN_IOU and scores[k] > match_score:\n",
    "                                match_id = k\n",
    "                                match_score = scores[k]\n",
    "                        if match_id != -1:\n",
    "                            assigned[match_id] = True\n",
    "                            if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                tp_scores.append(match_score)\n",
    "                total_num_valid_gt = np.sum([np.sum(np.array(gt_ignores) == 0) for gt_ignores in total_gt_ignores])\n",
    "                score_thresholds = get_score_thresholds(tp_scores, total_num_valid_gt)    \n",
    "            \n",
    "                # 3. draw PR curve and calculate mAP\n",
    "                tps, fns, fps, total_aos = [], [], [], []\n",
    "\n",
    "                for score_threshold in score_thresholds:\n",
    "                    tp, fn, fp = 0, 0, 0\n",
    "                    aos = 0\n",
    "                    for i, id in enumerate(ids):\n",
    "                        cur_eval_ious = eval_ious[i]\n",
    "                        gt_ignores, det_ignores = total_gt_ignores[i], total_det_ignores[i]\n",
    "                        gt_alpha, det_alpha = total_gt_alpha[i], total_det_alpha[i]\n",
    "                        scores = total_scores[i]\n",
    "\n",
    "                        nn, mm = cur_eval_ious.shape\n",
    "                        assigned = np.zeros((mm, ), dtype=np.bool_)\n",
    "                        for j in range(nn):\n",
    "                            if gt_ignores[j] == -1:\n",
    "                                continue\n",
    "                            match_id, match_iou = -1, -1\n",
    "                            for k in range(mm):\n",
    "                                if not assigned[k] and det_ignores[k] >= 0 and scores[k] >= score_threshold and cur_eval_ious[j, k] > CLS_MIN_IOU:\n",
    "    \n",
    "                                    if det_ignores[k] == 0 and cur_eval_ious[j, k] > match_iou:\n",
    "                                        match_iou = cur_eval_ious[j, k]\n",
    "                                        match_id = k\n",
    "                                    elif det_ignores[k] == 1 and match_iou == -1:\n",
    "                                        match_id = k\n",
    "\n",
    "                            if match_id != -1:\n",
    "                                assigned[match_id] = True\n",
    "                                if det_ignores[match_id] == 0 and gt_ignores[j] == 0:\n",
    "                                    tp += 1\n",
    "                                    if eval_type == 'bbox_2d':\n",
    "                                        aos += (1 + np.cos(gt_alpha[j] - det_alpha[match_id])) / 2\n",
    "                            else:\n",
    "                                if gt_ignores[j] == 0:\n",
    "                                    fn += 1\n",
    "                            \n",
    "                        for k in range(mm):\n",
    "                            if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                fp += 1\n",
    "                        \n",
    "                        # In case 2d bbox evaluation, we should consider dontcare bboxes\n",
    "                        if eval_type == 'bbox_2d':\n",
    "                            dc_bboxes = total_dc_bboxes[i]\n",
    "                            det_bboxes = det_results[id]['bbox']\n",
    "                            if len(dc_bboxes) > 0:\n",
    "                                ious_dc_det = iou2d(torch.from_numpy(det_bboxes), torch.from_numpy(dc_bboxes), metric=1).numpy().T\n",
    "                                for j in range(len(dc_bboxes)):\n",
    "                                    for k in range(len(det_bboxes)):\n",
    "                                        if det_ignores[k] == 0 and scores[k] >= score_threshold and not assigned[k]:\n",
    "                                            if ious_dc_det[j, k] > CLS_MIN_IOU:\n",
    "                                                fp -= 1\n",
    "                                                assigned[k] = True\n",
    "                            \n",
    "                    tps.append(tp)\n",
    "                    fns.append(fn)\n",
    "                    fps.append(fp)\n",
    "                    if eval_type == 'bbox_2d':\n",
    "                        total_aos.append(aos)\n",
    "\n",
    "                tps, fns, fps = np.array(tps), np.array(fns), np.array(fps)\n",
    "\n",
    "                precisions = tps / (tps + fns) # actually this is recalls\n",
    "                # precisions = tps / (tps + fps)\n",
    "                for i in range(len(score_thresholds)):\n",
    "                    precisions[i] = np.max(precisions[i:])\n",
    "                \n",
    "                sums_AP = 0\n",
    "                for i in range(0, len(score_thresholds), 4):\n",
    "                    sums_AP += precisions[i]\n",
    "                mAP = sums_AP / 11 * 100\n",
    "                eval_ap_results[cls].append(mAP)\n",
    "\n",
    "                if eval_type == 'bbox_2d':\n",
    "                    total_aos = np.array(total_aos)\n",
    "                    similarity = total_aos / (tps + fps)\n",
    "                    for i in range(len(score_thresholds)):\n",
    "                        similarity[i] = np.max(similarity[i:])\n",
    "                    sums_similarity = 0\n",
    "                    for i in range(0, len(score_thresholds), 4):\n",
    "                        sums_similarity += similarity[i]\n",
    "                    mSimilarity = sums_similarity / 11 * 100\n",
    "                    eval_aos_results[cls].append(mSimilarity)\n",
    "\n",
    "        print(f'=========={eval_type.upper()}==========')\n",
    "        print(f'=========={eval_type.upper()}==========', file=f)\n",
    "        for k, v in eval_ap_results.items():\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "            print(f'{k} AP@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            print(f'==========AOS==========')\n",
    "            print(f'==========AOS==========', file=f)\n",
    "            for k, v in eval_aos_results.items():\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "                print(f'{k} AOS@{MIN_IOUS[k][e_ind]}: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "        \n",
    "        overall_results[eval_type] = np.mean(list(eval_ap_results.values()), 0)\n",
    "        if eval_type == 'bbox_2d':\n",
    "            overall_results['AOS'] = np.mean(list(eval_aos_results.values()), 0)\n",
    "    \n",
    "    print(f'\\n==========Overall==========')\n",
    "    print(f'\\n==========Overall==========', file=f)\n",
    "    for k, v in overall_results.items():\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}')\n",
    "        print(f'{k} AP: {v[0]:.4f} {v[1]:.4f} {v[2]:.4f}', file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_losses(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict):\n",
    "    \n",
    "    ################# Full features #################\n",
    "    bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "    batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "    pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    bbox_pred0 = bbox_pred0[pos_idx]\n",
    "    batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "    batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "    num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "    bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                            bbox_pred=bbox_pred0,\n",
    "                            bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                            batched_labels=batched_bbox_labels, \n",
    "                            num_cls_pos=num_cls_pos, \n",
    "                            batched_bbox_reg=batched_bbox_reg0, \n",
    "                            batched_dir_labels=batched_dir_labels)\n",
    "\n",
    "    # gLoss = torch.sum(p*torch.norm(y-ye, dim=(1,2,3)))\n",
    "    # l2 norm\n",
    "    # gLoss = (((1-g)*(y-ye)).pow(2).sum(dim=(1,2,3)).pow(0.5)).sum()\n",
    "\n",
    "    return loss_dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(model, lambd = 1.01):\n",
    "\n",
    "    A = model.pillar_encoder.global_masks[0]\n",
    "    # B = torch.tensor([lambd ** i for i in range(len(A))], dtype=torch.float32).unsqueeze(0).to(A.device)\n",
    "    _, indices = torch.sort(A, dim=1, descending=True)\n",
    "    B = torch.pow(lambd, indices)\n",
    "    regularized_loss = torch.sum(A * B)\n",
    "\n",
    "\n",
    "    A = model.neck.global_masks[0]\n",
    "    _, indices = torch.sort(A, dim=1, descending=True)\n",
    "    B = torch.pow(lambd, indices)\n",
    "    regularized_loss += torch.sum(A * B)\n",
    "\n",
    "\n",
    "    for A in model.backbone.global_masks:\n",
    "        _, indices = torch.sort(A, dim=1, descending=True)\n",
    "        B = torch.pow(lambd, indices)\n",
    "        regularized_loss += torch.sum(A * B)\n",
    "\n",
    "\n",
    "    return regularized_loss\n",
    "\n",
    "# def regularize2(model, lambd = 1.1):\n",
    "\n",
    "#     A = model.pillar_encoder.global_masks[0]\n",
    "#     B = torch.tensor([lambd ** i for i in range(A.shape[1])], dtype=torch.float32).unsqueeze(0).to(A.device)\n",
    "#     A_sorted, indices = torch.sort(A, descending=True)\n",
    "#     regularized_loss = torch.sum(A_sorted * B)\n",
    "\n",
    "#     A = model.neck.global_masks[0]\n",
    "#     B = torch.tensor([lambd ** i for i in range(A.shape[1])], dtype=torch.float32).unsqueeze(0).to(A.device)\n",
    "#     A_sorted, indices = torch.sort(A, descending=True)\n",
    "#     regularized_loss += torch.sum(A_sorted * B)\n",
    "\n",
    "\n",
    "#     for A in model.backbone.global_masks:\n",
    "#         B = torch.tensor([lambd ** i for i in range(A.shape[1])], dtype=torch.float32).unsqueeze(0).to(A.device)\n",
    "#         A_sorted, indices = torch.sort(A, descending=True)\n",
    "#         regularized_loss += torch.sum(A_sorted * B)\n",
    "\n",
    "\n",
    "#     return regularized_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict):\n",
    "    bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "    batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "    pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    bbox_pred0 = bbox_pred0[pos_idx]\n",
    "\n",
    "    batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "\n",
    "\n",
    "    batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "\n",
    "    num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "\n",
    "    bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                            bbox_pred=bbox_pred0,\n",
    "                            bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                            batched_labels=batched_bbox_labels, \n",
    "                            num_cls_pos=num_cls_pos, \n",
    "                            batched_bbox_reg=batched_bbox_reg0, \n",
    "                            batched_dir_labels=batched_dir_labels)\n",
    "    return loss_dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pointpillars.load_state_dict(torch.load(\"logs/pillar_sequence_memory_gating_binary/checkpoints/epoch_60.pth\"))\n",
    "checkpoint = torch.load(\"logs/pillar_sequence_mem_lin_gat_8_masking_pruning/checkpoints/epoch_3.pth.tar\")\n",
    "pointpillars.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch0 = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# training_loss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/train_loss.npy\").tolist()\n",
    "# training_gLoss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/train_gloss.npy\").tolist()\n",
    "# val_loss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/val_loss.npy\").tolist()\n",
    "# val_gLoss0 = np.load(\"logs/pillar_sequence_memory_linearization/checkpoints/val_gloss.npy\").tolist()\n",
    "\n",
    "training_loss0 = []\n",
    "training_gLoss0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and Formatting the results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 101/866 [00:08<01:03, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating.. Please wait several seconds.\n",
      "> \u001b[0;32m/home/sayeed/uncertainty_estimation/point_cloud/PointPillars/utils/process.py\u001b[0m(440)\u001b[0;36miou2d\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    438 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    439 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 440 \u001b[0;31m    \u001b[0mbboxes_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_x2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbboxes_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    441 \u001b[0;31m    \u001b[0mbboxes_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_y2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbboxes_y1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    442 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([5, 4])\n",
      "torch.Size([0])\n",
      "torch.Size([0])\n",
      "*** NameError: name 'format_results' is not defined\n",
      "*** NameError: name 'format_results' is not defined\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "epoch0 = 2\n",
    "global_step=0\n",
    "\n",
    "for epoch in range(epoch0, args.max_epoch):\n",
    "# for epoch in range(1):\n",
    "    # epoch_loss0 = 0\n",
    "    # epoch_gLoss0 = 0\n",
    "\n",
    "    # train_indices = np.random.randint(train_dataset_length, size=train_dataset_batch_count)\n",
    "\n",
    "    # print('=' * 20, epoch, '=' * 20)\n",
    "\n",
    "    # train_step, val_step = 0, 0\n",
    "\n",
    "    # pointpillars.train()\n",
    "\n",
    "    # for i, data_dict in enumerate(tqdm(train_dataloader)):\n",
    "    #     if not args.no_cuda:\n",
    "    #         # move the tensors to the cuda\n",
    "    #         for key in data_dict:\n",
    "    #             for j, item in enumerate(data_dict[key]):\n",
    "    #                 if torch.is_tensor(item):\n",
    "    #                     data_dict[key][j] = data_dict[key][j].cuda()\n",
    "        \n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    #     batched_pts = data_dict['batched_pts']\n",
    "    #     batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "    #     batched_labels = data_dict['batched_labels']\n",
    "    #     batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "    #     ################# Full features #################\n",
    "\n",
    "    #     bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0, \\\n",
    "    #             bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, anchor_target_dict1, \\\n",
    "    #                 bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict2 = pointpillars(batched_pts=batched_pts, \n",
    "    #                                 mode='train',\n",
    "    #                                 batched_gt_bboxes=batched_gt_bboxes, \n",
    "    #                                 batched_gt_labels=batched_labels)\n",
    "\n",
    "    #     loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict0)\n",
    "    #     loss_dict1 = bugicugi(bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, anchor_target_dict1)\n",
    "    #     loss_dict2 = bugicugi(bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict2)\n",
    "\n",
    "    #     loss = loss_dict0['total_loss'] + loss_dict1['total_loss'] + loss_dict2['total_loss'] \n",
    "    #     loss.backward()\n",
    "\n",
    "    #     epoch_loss0 += epoch_loss0 + loss.item()\n",
    "\n",
    "    #     # ################# Half features #################\n",
    "\n",
    "    #     # bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict = pointpillars(batched_pts=batched_pts, \n",
    "    #     #                             mode='train',\n",
    "    #     #                             batched_gt_bboxes=batched_gt_bboxes, \n",
    "    #     #                             batched_gt_labels=batched_labels, level=1)\n",
    "\n",
    "    #     # loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict)\n",
    "    #     # loss = loss_dict0['total_loss'] \n",
    "    #     # loss.backward()\n",
    "        \n",
    "    #     # epoch_loss0 += epoch_loss0 + loss.item()\n",
    "\n",
    "    #     # ################# Quart features #################\n",
    "\n",
    "    #     # bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict = pointpillars(batched_pts=batched_pts, \n",
    "    #     #                             mode='train',\n",
    "    #     #                             batched_gt_bboxes=batched_gt_bboxes, \n",
    "    #     #                             batched_gt_labels=batched_labels, level=2)\n",
    "\n",
    "    #     # loss_dict0 = bugicugi(bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict)\n",
    "    #     # loss = loss_dict0['total_loss'] \n",
    "    #     # loss.backward()\n",
    "        \n",
    "    #     # epoch_loss0 += epoch_loss0 + loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    #     # torch.nn.utils.clip_grad_norm_(pointpillars.parameters(), max_norm=35)\n",
    "    #     optimizer.step()\n",
    "    #     scheduler.step()\n",
    "\n",
    "    \n",
    "    #     train_step += 1\n",
    "\n",
    "    #     global_step = 2*train_dataset_batch_count*epoch + train_step\n",
    "\n",
    "    #     # if global_step % args.log_freq == 0:\n",
    "    #     #     save_summary(writer, loss_dict, global_step, 'train', p,\n",
    "    #     #                     lr=optimizer.param_groups[0]['lr'], \n",
    "    #     #                     momentum=optimizer.param_groups[0]['betas'][0],\n",
    "    #     #                     model=pointpillars, data=data_cuda, flag=model_flag)\n",
    "\n",
    "    # training_loss.append(loss.item())\n",
    "\n",
    "    if epoch % args.ckpt_freq_epoch == 0:\n",
    "\n",
    "        checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': pointpillars.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }   \n",
    "        torch.save(checkpoint, os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth.tar'))\n",
    "    ###################################### Validation ######################################\n",
    "\n",
    "    if (epoch % args.val_freq_epoch) == 0 and epoch >0:\n",
    "    # if (epoch % args.val_freq_epoch) == 0:\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(training_loss0))\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.legend([\"loss\", \"g_loss\"])\n",
    "        plt.show()\n",
    "\n",
    "        ################################### Validation ###################################\n",
    "        pointpillars.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            format_results = {}\n",
    "            print('Predicting and Formatting the results.')\n",
    "            for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "                if i > 100:\n",
    "                    break\n",
    "                if not args.no_cuda:\n",
    "                    # move the tensors to the cuda\n",
    "                    for key in data_dict:\n",
    "                        for j, item in enumerate(data_dict[key]):\n",
    "                            if torch.is_tensor(item):\n",
    "                                data_dict[key][j] = data_dict[key][j].cuda()\n",
    "                \n",
    "                batched_pts = data_dict['batched_pts']\n",
    "                batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "                batched_labels = data_dict['batched_labels']\n",
    "                batched_difficulty = data_dict['batched_difficulty']\n",
    "                batch_results0  = pointpillars(batched_pts=batched_pts,\n",
    "                                        mode='val0',\n",
    "                                        batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                        batched_gt_labels=batched_labels)\n",
    "\n",
    "                for j, result in enumerate(batch_results0):\n",
    "                    format_result = {\n",
    "                        'name': [],\n",
    "                        'truncated': [],\n",
    "                        'occluded': [],\n",
    "                        'alpha': [],\n",
    "                        'bbox': [],\n",
    "                        'dimensions': [],\n",
    "                        'location': [],\n",
    "                        'rotation_y': [],\n",
    "                        'score': []\n",
    "                    }\n",
    "                    \n",
    "                    calib_info = data_dict['batched_calib_info'][j]\n",
    "                    tr_velo_to_cam = calib_info['Tr_velo_to_cam'].astype(np.float32)\n",
    "                    r0_rect = calib_info['R0_rect'].astype(np.float32)\n",
    "                    P2 = calib_info['P2'].astype(np.float32)\n",
    "                    image_shape = data_dict['batched_img_info'][j]['image_shape']\n",
    "                    idx = data_dict['batched_img_info'][j]['image_idx']\n",
    "                    result_filter = keep_bbox_from_image_range(result, tr_velo_to_cam, r0_rect, P2, image_shape)\n",
    "                    result_filter = keep_bbox_from_lidar_range(result_filter, pcd_limit_range)\n",
    "\n",
    "                    lidar_bboxes = result_filter['lidar_bboxes']\n",
    "                    labels, scores = result_filter['labels'], result_filter['scores']\n",
    "                    bboxes2d, camera_bboxes = result_filter['bboxes2d'], result_filter['camera_bboxes']\n",
    "                    \n",
    "                    for lidar_bbox, label, score, bbox2d, camera_bbox in \\\n",
    "                        zip(lidar_bboxes, labels, scores, bboxes2d, camera_bboxes):\n",
    "                        format_result['name'].append(LABEL2CLASSES[label])\n",
    "                        format_result['truncated'].append(0.0)\n",
    "                        format_result['occluded'].append(0)\n",
    "                        alpha = camera_bbox[6] - np.arctan2(camera_bbox[0], camera_bbox[2])\n",
    "                        format_result['alpha'].append(alpha)\n",
    "                        format_result['bbox'].append(bbox2d)\n",
    "                        format_result['dimensions'].append(camera_bbox[3:6])\n",
    "                        format_result['location'].append(camera_bbox[:3])\n",
    "                        format_result['rotation_y'].append(camera_bbox[6])\n",
    "                        format_result['score'].append(score)\n",
    "                    \n",
    "                    write_label(format_result, os.path.join(args.saved_path_exact, f'{idx:06d}.txt'))\n",
    "\n",
    "                    format_results[idx] = {k:np.array(v) for k, v in format_result.items()}\n",
    "            \n",
    "            write_pickle(format_results, os.path.join(args.saved_path, 'results.pkl'))\n",
    "\n",
    "        print('Evaluating.. Please wait several seconds.')\n",
    "        # do_eval(format_results, val_dataset.data_infos, CLASSES, args.saved_path)\n",
    "        do_eval(format_results, dict(islice(val_dataset.data_infos.items(), i)), CLASSES, args.saved_path)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2000000,\n",
       " 2000001,\n",
       " 2000002,\n",
       " 2000003,\n",
       " 2000004,\n",
       " 2000005,\n",
       " 2000006,\n",
       " 2000007,\n",
       " 2000008,\n",
       " 2000009,\n",
       " 2000010,\n",
       " 2000011,\n",
       " 2000012,\n",
       " 2000013,\n",
       " 2000014,\n",
       " 2000015,\n",
       " 2000016,\n",
       " 2000017,\n",
       " 2000018,\n",
       " 2000019,\n",
       " 2000020,\n",
       " 2000021,\n",
       " 2000022,\n",
       " 2000023,\n",
       " 2000024,\n",
       " 2000025,\n",
       " 2000026,\n",
       " 2000027,\n",
       " 2000028,\n",
       " 2000029,\n",
       " 2000030,\n",
       " 2000031,\n",
       " 2000032,\n",
       " 2000033,\n",
       " 2000034,\n",
       " 2000035,\n",
       " 2000036,\n",
       " 2000037,\n",
       " 2000038,\n",
       " 2000039,\n",
       " 2000040,\n",
       " 2000041,\n",
       " 2000042,\n",
       " 2000043,\n",
       " 2000044,\n",
       " 2000045,\n",
       " 2000046,\n",
       " 2000047,\n",
       " 2000048,\n",
       " 2000049,\n",
       " 2000050,\n",
       " 2000051,\n",
       " 2000052,\n",
       " 2000053,\n",
       " 2000054,\n",
       " 2000055,\n",
       " 2000056,\n",
       " 2000057,\n",
       " 2000058,\n",
       " 2000059,\n",
       " 2000060,\n",
       " 2000061,\n",
       " 2000062,\n",
       " 2000063,\n",
       " 2000064,\n",
       " 2000065,\n",
       " 2000066,\n",
       " 2000067,\n",
       " 2000068,\n",
       " 2000069,\n",
       " 2000070,\n",
       " 2000071,\n",
       " 2000072,\n",
       " 2000073,\n",
       " 2000074,\n",
       " 2000075,\n",
       " 2000076,\n",
       " 2000077,\n",
       " 2000078,\n",
       " 2000079,\n",
       " 2000080,\n",
       " 2000081,\n",
       " 2000082,\n",
       " 2000083,\n",
       " 2000084,\n",
       " 2000085,\n",
       " 2000086,\n",
       " 2000087,\n",
       " 2000088,\n",
       " 2000089,\n",
       " 2000090,\n",
       " 2000091,\n",
       " 2000092,\n",
       " 2000093,\n",
       " 2000094,\n",
       " 2000095,\n",
       " 2000096,\n",
       " 2000097,\n",
       " 2000098,\n",
       " 2000099,\n",
       " 2000100,\n",
       " 2000101,\n",
       " 2000102,\n",
       " 2000103,\n",
       " 2000104,\n",
       " 2000105,\n",
       " 2000106,\n",
       " 2000107,\n",
       " 2000108,\n",
       " 2000109,\n",
       " 2000110,\n",
       " 2000111,\n",
       " 2000112,\n",
       " 2000113,\n",
       " 2000114,\n",
       " 2000115,\n",
       " 2000116,\n",
       " 2000117,\n",
       " 2000118,\n",
       " 2000119,\n",
       " 2000120,\n",
       " 2000121,\n",
       " 2000122,\n",
       " 2000123,\n",
       " 2000124,\n",
       " 2000125,\n",
       " 2000126,\n",
       " 2000127,\n",
       " 2000128,\n",
       " 2000129,\n",
       " 2000130,\n",
       " 2000131,\n",
       " 2000132,\n",
       " 2000133,\n",
       " 2000134,\n",
       " 2000135,\n",
       " 2000136,\n",
       " 2000137,\n",
       " 2000138,\n",
       " 2000139,\n",
       " 2000140,\n",
       " 2000141,\n",
       " 2000142,\n",
       " 2000143,\n",
       " 2000144,\n",
       " 2000145,\n",
       " 2000146,\n",
       " 2000147,\n",
       " 2000148,\n",
       " 2000149,\n",
       " 2000150,\n",
       " 2000151,\n",
       " 2000152,\n",
       " 2000153,\n",
       " 2000154,\n",
       " 2000155,\n",
       " 2000156,\n",
       " 2000157,\n",
       " 2000158,\n",
       " 2000159,\n",
       " 2000160,\n",
       " 2000161,\n",
       " 2000162,\n",
       " 2000163,\n",
       " 2000164,\n",
       " 2000165,\n",
       " 2000166,\n",
       " 2000167,\n",
       " 2000168,\n",
       " 2000169,\n",
       " 2000170,\n",
       " 2000171,\n",
       " 2000172,\n",
       " 2000173,\n",
       " 2000174,\n",
       " 2000175,\n",
       " 2000176,\n",
       " 2000177,\n",
       " 2000178,\n",
       " 2000179,\n",
       " 2000180,\n",
       " 2000181,\n",
       " 2000182,\n",
       " 2000183,\n",
       " 2000184,\n",
       " 2000185,\n",
       " 2000186,\n",
       " 2000187,\n",
       " 2000188,\n",
       " 2000189,\n",
       " 2000190,\n",
       " 2000191,\n",
       " 2000192,\n",
       " 2000193,\n",
       " 2000194,\n",
       " 2000195,\n",
       " 2000196,\n",
       " 2000197,\n",
       " 2000198,\n",
       " 2000199,\n",
       " 2000200,\n",
       " 2000201,\n",
       " 2000202,\n",
       " 2000203,\n",
       " 2000204,\n",
       " 2000205,\n",
       " 2000206,\n",
       " 2000207,\n",
       " 2000208,\n",
       " 2000209,\n",
       " 2000210,\n",
       " 2000211,\n",
       " 2000212,\n",
       " 2000213,\n",
       " 2000214,\n",
       " 2000215,\n",
       " 2000216,\n",
       " 2000217,\n",
       " 2000218,\n",
       " 2000219,\n",
       " 2000220,\n",
       " 2000221,\n",
       " 2000222,\n",
       " 2000223,\n",
       " 2000224,\n",
       " 2000225,\n",
       " 2000226,\n",
       " 2000227,\n",
       " 2000228,\n",
       " 2000229,\n",
       " 2000230,\n",
       " 2000231,\n",
       " 2000232,\n",
       " 10000000,\n",
       " 10000001,\n",
       " 10000002,\n",
       " 10000003,\n",
       " 10000004,\n",
       " 10000005,\n",
       " 10000006,\n",
       " 10000007,\n",
       " 10000008,\n",
       " 10000009,\n",
       " 10000010,\n",
       " 10000011,\n",
       " 10000012,\n",
       " 10000013,\n",
       " 10000014,\n",
       " 10000015,\n",
       " 10000016,\n",
       " 10000017,\n",
       " 10000018,\n",
       " 10000019,\n",
       " 10000020,\n",
       " 10000021,\n",
       " 10000022,\n",
       " 10000023,\n",
       " 10000024,\n",
       " 10000025,\n",
       " 10000026,\n",
       " 10000027,\n",
       " 10000028,\n",
       " 10000029,\n",
       " 10000030,\n",
       " 10000031,\n",
       " 10000032,\n",
       " 10000033,\n",
       " 10000034,\n",
       " 10000035,\n",
       " 10000036,\n",
       " 10000037,\n",
       " 10000038,\n",
       " 10000039,\n",
       " 10000040,\n",
       " 10000041,\n",
       " 10000042,\n",
       " 10000043,\n",
       " 10000044,\n",
       " 10000045,\n",
       " 10000046,\n",
       " 10000047,\n",
       " 10000048,\n",
       " 10000049,\n",
       " 10000050,\n",
       " 10000051,\n",
       " 10000052,\n",
       " 10000053,\n",
       " 10000054,\n",
       " 10000055,\n",
       " 10000056,\n",
       " 10000057,\n",
       " 10000058,\n",
       " 10000059,\n",
       " 10000060,\n",
       " 10000061,\n",
       " 10000062,\n",
       " 10000063,\n",
       " 10000064,\n",
       " 10000065,\n",
       " 10000066,\n",
       " 10000067,\n",
       " 10000068,\n",
       " 10000069,\n",
       " 10000070,\n",
       " 10000071,\n",
       " 10000072,\n",
       " 10000073,\n",
       " 10000074,\n",
       " 10000075,\n",
       " 10000076,\n",
       " 10000077,\n",
       " 10000078,\n",
       " 10000079,\n",
       " 10000080,\n",
       " 10000081,\n",
       " 10000082,\n",
       " 10000083,\n",
       " 10000084,\n",
       " 10000085,\n",
       " 10000086,\n",
       " 10000087,\n",
       " 10000088,\n",
       " 10000089,\n",
       " 10000090,\n",
       " 10000091,\n",
       " 10000092,\n",
       " 10000093,\n",
       " 10000094,\n",
       " 10000095,\n",
       " 10000096,\n",
       " 10000097,\n",
       " 10000098,\n",
       " 10000099,\n",
       " 10000100,\n",
       " 10000101,\n",
       " 10000102,\n",
       " 10000103,\n",
       " 10000104,\n",
       " 10000105,\n",
       " 10000106,\n",
       " 10000107,\n",
       " 10000108,\n",
       " 10000109,\n",
       " 10000110,\n",
       " 10000111,\n",
       " 10000112,\n",
       " 10000113,\n",
       " 10000114,\n",
       " 10000115,\n",
       " 10000116,\n",
       " 10000117,\n",
       " 10000118,\n",
       " 10000119,\n",
       " 10000120,\n",
       " 10000121,\n",
       " 10000122,\n",
       " 10000123,\n",
       " 10000124,\n",
       " 10000125,\n",
       " 10000126,\n",
       " 10000127,\n",
       " 10000128,\n",
       " 10000129,\n",
       " 10000130,\n",
       " 10000131,\n",
       " 10000132,\n",
       " 10000133,\n",
       " 10000134,\n",
       " 10000135,\n",
       " 10000136,\n",
       " 10000137,\n",
       " 10000138,\n",
       " 10000139,\n",
       " 10000140,\n",
       " 10000141,\n",
       " 10000142,\n",
       " 10000143,\n",
       " 10000144,\n",
       " 10000145,\n",
       " 10000146,\n",
       " 10000147,\n",
       " 10000148,\n",
       " 10000149,\n",
       " 10000150,\n",
       " 10000151,\n",
       " 10000152,\n",
       " 10000153,\n",
       " 10000154,\n",
       " 10000155,\n",
       " 10000156,\n",
       " 10000157,\n",
       " 10000158,\n",
       " 10000159,\n",
       " 10000160,\n",
       " 10000161,\n",
       " 10000162,\n",
       " 10000163,\n",
       " 10000164,\n",
       " 10000165,\n",
       " 10000166,\n",
       " 10000167,\n",
       " 10000168,\n",
       " 10000169,\n",
       " 10000170,\n",
       " 10000171,\n",
       " 10000172,\n",
       " 10000173,\n",
       " 10000174,\n",
       " 10000175,\n",
       " 10000176,\n",
       " 10000177,\n",
       " 10000178,\n",
       " 10000179,\n",
       " 10000180,\n",
       " 10000181,\n",
       " 10000182,\n",
       " 10000183,\n",
       " 10000184,\n",
       " 10000185,\n",
       " 10000186,\n",
       " 10000187,\n",
       " 10000188,\n",
       " 10000189,\n",
       " 10000190,\n",
       " 10000191,\n",
       " 10000192,\n",
       " 10000193,\n",
       " 10000194,\n",
       " 10000195,\n",
       " 10000196,\n",
       " 10000197,\n",
       " 10000198,\n",
       " 10000199,\n",
       " 10000200,\n",
       " 10000201,\n",
       " 10000202,\n",
       " 10000203,\n",
       " 10000204,\n",
       " 10000205,\n",
       " 10000206,\n",
       " 10000207,\n",
       " 10000208,\n",
       " 10000209,\n",
       " 10000210,\n",
       " 10000211,\n",
       " 10000212,\n",
       " 10000213,\n",
       " 10000214,\n",
       " 10000215,\n",
       " 10000216,\n",
       " 10000217,\n",
       " 10000218,\n",
       " 10000219,\n",
       " 10000220,\n",
       " 10000221,\n",
       " 10000222,\n",
       " 10000223,\n",
       " 10000224,\n",
       " 10000225,\n",
       " 10000226,\n",
       " 10000227,\n",
       " 10000228,\n",
       " 10000229,\n",
       " 10000230,\n",
       " 10000231,\n",
       " 10000232,\n",
       " 10000233,\n",
       " 10000234,\n",
       " 10000235,\n",
       " 10000236,\n",
       " 10000237,\n",
       " 10000238,\n",
       " 10000239,\n",
       " 10000240,\n",
       " 10000241,\n",
       " 10000242,\n",
       " 10000243,\n",
       " 10000244,\n",
       " 10000245,\n",
       " 10000246,\n",
       " 10000247,\n",
       " 10000248,\n",
       " 10000249,\n",
       " 10000250,\n",
       " 10000251,\n",
       " 10000252,\n",
       " 10000253,\n",
       " 10000254,\n",
       " 10000255,\n",
       " 10000256,\n",
       " 10000257,\n",
       " 10000258,\n",
       " 10000259,\n",
       " 10000260,\n",
       " 10000261,\n",
       " 10000262,\n",
       " 10000263,\n",
       " 10000264,\n",
       " 10000265,\n",
       " 10000266,\n",
       " 10000267,\n",
       " 10000268,\n",
       " 10000269,\n",
       " 10000270,\n",
       " 10000271,\n",
       " 10000272,\n",
       " 10000273,\n",
       " 10000274,\n",
       " 10000275,\n",
       " 10000276,\n",
       " 10000277,\n",
       " 10000278,\n",
       " 10000279,\n",
       " 10000280,\n",
       " 10000281,\n",
       " 10000282,\n",
       " 10000283,\n",
       " 10000284,\n",
       " 10000285,\n",
       " 10000286,\n",
       " 10000287,\n",
       " 10000288,\n",
       " 10000289,\n",
       " 10000290,\n",
       " 10000291,\n",
       " 10000292,\n",
       " 10000293,\n",
       " 18000000,\n",
       " 18000001,\n",
       " 18000002,\n",
       " 18000003,\n",
       " 18000004,\n",
       " 18000005,\n",
       " 18000006,\n",
       " 18000007,\n",
       " 18000008,\n",
       " 18000009,\n",
       " 18000010,\n",
       " 18000011,\n",
       " 18000012,\n",
       " 18000013,\n",
       " 18000014,\n",
       " 18000015,\n",
       " 18000016,\n",
       " 18000017,\n",
       " 18000018,\n",
       " 18000019,\n",
       " 18000020,\n",
       " 18000021,\n",
       " 18000022,\n",
       " 18000023,\n",
       " 18000024,\n",
       " 18000025,\n",
       " 18000026,\n",
       " 18000027,\n",
       " 18000028,\n",
       " 18000029,\n",
       " 18000030,\n",
       " 18000031,\n",
       " 18000032,\n",
       " 18000033,\n",
       " 18000034,\n",
       " 18000035,\n",
       " 18000036,\n",
       " 18000037,\n",
       " 18000038,\n",
       " 18000039,\n",
       " 18000040,\n",
       " 18000041,\n",
       " 18000042,\n",
       " 18000043,\n",
       " 18000044,\n",
       " 18000045,\n",
       " 18000046,\n",
       " 18000047,\n",
       " 18000048,\n",
       " 18000049,\n",
       " 18000050,\n",
       " 18000051,\n",
       " 18000052,\n",
       " 18000053,\n",
       " 18000054,\n",
       " 18000055,\n",
       " 18000056,\n",
       " 18000057,\n",
       " 18000058,\n",
       " 18000059,\n",
       " 18000060,\n",
       " 18000061,\n",
       " 18000062,\n",
       " 18000063,\n",
       " 18000064,\n",
       " 18000065,\n",
       " 18000066,\n",
       " 18000067,\n",
       " 18000068,\n",
       " 18000069,\n",
       " 18000070,\n",
       " 18000071,\n",
       " 18000072,\n",
       " 18000073,\n",
       " 18000074,\n",
       " 18000075,\n",
       " 18000076,\n",
       " 18000077,\n",
       " 18000078,\n",
       " 18000079,\n",
       " 18000080,\n",
       " 18000081,\n",
       " 18000082,\n",
       " 18000083,\n",
       " 18000084,\n",
       " 18000085,\n",
       " 18000086,\n",
       " 18000087,\n",
       " 18000088,\n",
       " 18000089,\n",
       " 18000090,\n",
       " 18000091,\n",
       " 18000092,\n",
       " 18000093,\n",
       " 18000094,\n",
       " 18000095,\n",
       " 18000096,\n",
       " 18000097,\n",
       " 18000098,\n",
       " 18000099,\n",
       " 18000100,\n",
       " 18000101,\n",
       " 18000102,\n",
       " 18000103,\n",
       " 18000104,\n",
       " 18000105,\n",
       " 18000106,\n",
       " 18000107,\n",
       " 18000108,\n",
       " 18000109,\n",
       " 18000110,\n",
       " 18000111,\n",
       " 18000112,\n",
       " 18000113,\n",
       " 18000114,\n",
       " 18000115,\n",
       " 18000116,\n",
       " 18000117,\n",
       " 18000118,\n",
       " 18000119,\n",
       " 18000120,\n",
       " 18000121,\n",
       " 18000122,\n",
       " 18000123,\n",
       " 18000124,\n",
       " 18000125,\n",
       " 18000126,\n",
       " 18000127,\n",
       " 18000128,\n",
       " 18000129,\n",
       " 18000130,\n",
       " 18000131,\n",
       " 18000132,\n",
       " 18000133,\n",
       " 18000134,\n",
       " 18000135,\n",
       " 18000136,\n",
       " 18000137,\n",
       " 18000138,\n",
       " 18000139,\n",
       " 18000140,\n",
       " 18000141,\n",
       " 18000142,\n",
       " 18000143,\n",
       " 18000144,\n",
       " 18000145,\n",
       " 18000146,\n",
       " 18000147,\n",
       " 18000148,\n",
       " 18000149,\n",
       " 18000150,\n",
       " 18000151,\n",
       " 18000152,\n",
       " 18000153,\n",
       " 18000154,\n",
       " 18000155,\n",
       " 18000156,\n",
       " 18000157,\n",
       " 18000158,\n",
       " 18000159,\n",
       " 18000160,\n",
       " 18000161,\n",
       " 18000162,\n",
       " 18000163,\n",
       " 18000164,\n",
       " 18000165,\n",
       " 18000166,\n",
       " 18000167,\n",
       " 18000168,\n",
       " 18000169,\n",
       " 18000170,\n",
       " 18000171,\n",
       " 18000172,\n",
       " 18000173,\n",
       " 18000174,\n",
       " 18000175,\n",
       " 18000176,\n",
       " 18000177,\n",
       " 18000178,\n",
       " 18000179,\n",
       " 18000180,\n",
       " 18000181,\n",
       " 18000182,\n",
       " 18000183,\n",
       " 18000184,\n",
       " 18000185,\n",
       " 18000186,\n",
       " 18000187,\n",
       " 18000188,\n",
       " 18000189,\n",
       " 18000190,\n",
       " 18000191,\n",
       " 18000192,\n",
       " 18000193,\n",
       " 18000194,\n",
       " 18000195,\n",
       " 18000196,\n",
       " 18000197,\n",
       " 18000198,\n",
       " 18000199,\n",
       " 18000200,\n",
       " 18000201,\n",
       " 18000202,\n",
       " 18000203,\n",
       " 18000204,\n",
       " 18000205,\n",
       " 18000206,\n",
       " 18000207,\n",
       " 18000208,\n",
       " 18000209,\n",
       " 18000210,\n",
       " 18000211,\n",
       " 18000212,\n",
       " 18000213,\n",
       " 18000214,\n",
       " 18000215,\n",
       " 18000216,\n",
       " 18000217,\n",
       " 18000218,\n",
       " 18000219,\n",
       " 18000220,\n",
       " 18000221,\n",
       " 18000222,\n",
       " 18000223,\n",
       " 18000224,\n",
       " 18000225,\n",
       " 18000226,\n",
       " 18000227,\n",
       " 18000228,\n",
       " 18000229,\n",
       " 18000230,\n",
       " 18000231,\n",
       " 18000232,\n",
       " 18000233,\n",
       " 18000234,\n",
       " 18000235,\n",
       " 18000236,\n",
       " 18000237,\n",
       " 18000238,\n",
       " 18000239,\n",
       " 18000240,\n",
       " 18000241,\n",
       " 18000242,\n",
       " 18000243,\n",
       " 18000244,\n",
       " 18000245,\n",
       " 18000246,\n",
       " 18000247,\n",
       " 18000248,\n",
       " 18000249,\n",
       " 18000250,\n",
       " 18000251,\n",
       " 18000252,\n",
       " 18000253,\n",
       " 18000254,\n",
       " 18000255,\n",
       " 18000256,\n",
       " 18000257,\n",
       " 18000258,\n",
       " 18000259,\n",
       " 18000260,\n",
       " 18000261,\n",
       " 18000262,\n",
       " 18000263,\n",
       " 18000264,\n",
       " 18000265,\n",
       " 18000266,\n",
       " 18000267,\n",
       " 18000268,\n",
       " 18000269,\n",
       " 18000270,\n",
       " 18000271,\n",
       " 18000272,\n",
       " 18000273,\n",
       " 18000274,\n",
       " 18000275,\n",
       " 18000276,\n",
       " 18000277,\n",
       " 18000278,\n",
       " 18000279,\n",
       " 18000280,\n",
       " 18000281,\n",
       " 18000282,\n",
       " 18000283,\n",
       " 18000284,\n",
       " 18000285,\n",
       " 18000286,\n",
       " 18000287,\n",
       " 18000288,\n",
       " 18000289,\n",
       " 18000290,\n",
       " 18000291,\n",
       " 18000292,\n",
       " 18000293,\n",
       " 18000294,\n",
       " 18000295,\n",
       " 18000296,\n",
       " 18000297,\n",
       " 18000298,\n",
       " 18000299,\n",
       " 18000300,\n",
       " 18000301,\n",
       " 18000302,\n",
       " 18000303,\n",
       " 18000304,\n",
       " 18000305,\n",
       " 18000306,\n",
       " 18000307,\n",
       " 18000308,\n",
       " 18000309,\n",
       " 18000310,\n",
       " 18000311,\n",
       " 18000312,\n",
       " 18000313,\n",
       " 18000314,\n",
       " 18000315,\n",
       " 18000316,\n",
       " 18000317,\n",
       " 18000318,\n",
       " 18000319,\n",
       " 18000320,\n",
       " 18000321,\n",
       " 18000322,\n",
       " 18000323,\n",
       " 18000324,\n",
       " 18000325,\n",
       " 18000326,\n",
       " 18000327,\n",
       " 18000328,\n",
       " 18000329,\n",
       " 18000330,\n",
       " 18000331,\n",
       " 18000332,\n",
       " 18000333,\n",
       " 18000334,\n",
       " 18000335,\n",
       " 18000336,\n",
       " 18000337,\n",
       " 18000338]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(val_dataset.data_infos.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': array([], dtype=float64),\n",
       " 'truncated': array([], dtype=float64),\n",
       " 'occluded': array([], dtype=float64),\n",
       " 'alpha': array([], dtype=float64),\n",
       " 'bbox': array([], dtype=float64),\n",
       " 'dimensions': array([], dtype=float64),\n",
       " 'location': array([], dtype=float64),\n",
       " 'rotation_y': array([], dtype=float64),\n",
       " 'score': array([], dtype=float64)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_results[2000009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==========BBOX_2D==========\\nPedestrian AP@0.5: 12.5541 12.0594 12.0594\\nCyclist AP@0.5: 18.1818 67.8230 67.8230\\nCar AP@0.7: 90.3473 85.0690 82.6321\\n==========AOS==========\\nPedestrian AOS@0.5: 2.2431 1.8547 1.8547\\nCyclist AOS@0.5: 0.0884 9.7744 9.7744\\nCar AOS@0.7: 90.1198 88.3073 86.8500\\n==========BBOX_BEV==========\\nPedestrian AP@0.5: 17.3160 12.9870 12.9870\\nCyclist AP@0.5: 18.1818 53.5885 53.5885\\nCar AP@0.7: 86.9970 81.8544 71.0886\\n==========BBOX_3D==========\\nPedestrian AP@0.5: 11.4719 10.9462 10.9462\\nCyclist AP@0.5: 18.1818 28.7081 28.7081\\nCar AP@0.7: 71.3308 56.4050 53.9020\\n\\n==========Overall==========\\nbbox_2d AP: 40.3611 54.9838 54.1715\\nAOS AP: 30.8171 33.3121 32.8263\\nbbox_bev AP: 40.8316 49.4766 45.8880\\nbbox_3d AP: 33.6615 32.0198 31.1854\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "==========BBOX_2D==========\n",
    "Pedestrian AP@0.5: 12.5541 12.0594 12.0594\n",
    "Cyclist AP@0.5: 18.1818 67.8230 67.8230\n",
    "Car AP@0.7: 90.3473 85.0690 82.6321\n",
    "==========AOS==========\n",
    "Pedestrian AOS@0.5: 2.2431 1.8547 1.8547\n",
    "Cyclist AOS@0.5: 0.0884 9.7744 9.7744\n",
    "Car AOS@0.7: 90.1198 88.3073 86.8500\n",
    "==========BBOX_BEV==========\n",
    "Pedestrian AP@0.5: 17.3160 12.9870 12.9870\n",
    "Cyclist AP@0.5: 18.1818 53.5885 53.5885\n",
    "Car AP@0.7: 86.9970 81.8544 71.0886\n",
    "==========BBOX_3D==========\n",
    "Pedestrian AP@0.5: 11.4719 10.9462 10.9462\n",
    "Cyclist AP@0.5: 18.1818 28.7081 28.7081\n",
    "Car AP@0.7: 71.3308 56.4050 53.9020\n",
    "\n",
    "==========Overall==========\n",
    "bbox_2d AP: 40.3611 54.9838 54.1715\n",
    "AOS AP: 30.8171 33.3121 32.8263\n",
    "bbox_bev AP: 40.8316 49.4766 45.8880\n",
    "bbox_3d AP: 33.6615 32.0198 31.1854\n",
    "\n",
    "\n",
    "==========BBOX_2D==========\n",
    "Pedestrian AP@0.5: 12.3377 11.1317 11.1317\n",
    "Cyclist AP@0.5: 3.4091 39.3541 39.3541\n",
    "Car AP@0.7: 71.9899 55.0626 53.1738\n",
    "==========AOS==========\n",
    "Pedestrian AOS@0.5: 2.9628 2.4612 2.4612\n",
    "Cyclist AOS@0.5: 0.0064 2.0339 2.0339\n",
    "Car AOS@0.7: 74.2191 63.3802 62.0901\n",
    "==========BBOX_BEV==========\n",
    "Pedestrian AP@0.5: 10.1732 6.8182 6.8182\n",
    "Cyclist AP@0.5: 3.4091 11.9617 11.9617\n",
    "Car AP@0.7: 57.2220 39.6281 37.3518\n",
    "==========BBOX_3D==========\n",
    "Pedestrian AP@0.5: 5.1948 3.1540 3.1540\n",
    "Cyclist AP@0.5: 3.4091 5.7416 5.7416\n",
    "Car AP@0.7: 39.7968 26.3120 20.4752\n",
    "\n",
    "==========Overall==========\n",
    "bbox_2d AP: 29.2456 35.1828 34.5532\n",
    "AOS AP: 25.7295 22.6251 22.1951\n",
    "bbox_bev AP: 23.6014 19.4693 18.7106\n",
    "bbox_3d AP: 16.1336 11.7359 9.7903\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    #     global_step = epoch * len(train_dataloader) + train_step + 1\n",
    "\n",
    "    #     if global_step % args.log_freq == 0:\n",
    "    #         save_summary(writer, loss_dict0, global_step, 'train',\n",
    "    #                         lr=optimizer.param_groups[0]['lr'], \n",
    "    #                         momentum=optimizer.param_groups[0]['betas'][0])\n",
    "    #     train_step += 1\n",
    "\n",
    "    # training_loss0.append(epoch_loss0)\n",
    "    # training_gLoss0.append(epoch_gLoss0.detach().cpu())\n",
    "\n",
    "    # if epoch % 2 == 0:\n",
    "    #     continue\n",
    "\n",
    "    # pointpillars.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "    #         try:\n",
    "    #             if not args.no_cuda:\n",
    "    #                 # move the tensors to the cuda\n",
    "    #                 for key in data_dict:\n",
    "    #                     for j, item in enumerate(data_dict[key]):\n",
    "    #                         if torch.is_tensor(item):\n",
    "    #                             data_dict[key][j] = data_dict[key][j].cuda()\n",
    "                \n",
    "    #             batched_pts = data_dict['batched_pts']\n",
    "    #             batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "    #             batched_labels = data_dict['batched_labels']\n",
    "    #             batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "    #             bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict, x, xe = pointpillars(batched_pts=batched_pts, \n",
    "    #                                         batched_pts0=batched_pts0, \n",
    "    #                                         mode='train',\n",
    "    #                                         batched_gt_bboxes=batched_gt_bboxes, \n",
    "    #                                         batched_gt_labels=batched_labels)\\\n",
    "    #                 # bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "    #                 #     bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2,  = \\\n",
    "\n",
    "                \n",
    "    #             ################# Full features #################\n",
    "    #             bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "    #             bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "    #             bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "    #             batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "    #             batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "    #             batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "    #             batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "\n",
    "                \n",
    "    #             pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "    #             bbox_pred0 = bbox_pred0[pos_idx]\n",
    "\n",
    "\n",
    "    #             batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "    #             batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "\n",
    "    #             # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "    #             bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "    #             batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "    #             bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "    #             batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "    #             num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "    #             bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "\n",
    "\n",
    "    #             batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "    #             batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "    #             loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "    #                                     bbox_pred=bbox_pred0,\n",
    "    #                                     bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "    #                                     batched_labels=batched_bbox_labels, \n",
    "    #                                     num_cls_pos=num_cls_pos, \n",
    "    #                                     batched_bbox_reg=batched_bbox_reg0, \n",
    "    #                                     batched_dir_labels=batched_dir_labels)\n",
    "                \n",
    "                \n",
    "    #             loss0 = loss_dict0['total_loss'] \n",
    "                \n",
    "    #             gLoss = torch.norm(x-xe)\n",
    "\n",
    "\n",
    "    #             if not np.isnan(loss0.item()):\n",
    "    #                 val_epoch_loss0 = val_epoch_loss0 + loss0.item()\n",
    "    #                 val_epoch_gLoss0 = val_epoch_gLoss0 + lambda_g*gLoss\n",
    "\n",
    "\n",
    "    #             else:\n",
    "    #                 continue\n",
    "\n",
    "    #             global_step = epoch * len(val_dataloader) + val_step + 1\n",
    "    #             if global_step % args.log_freq == 0:\n",
    "    #                 save_summary(writer, loss_dict0, global_step, 'val')\n",
    "    #             val_step += 1\n",
    "            \n",
    "    #         except:\n",
    "    #             None\n",
    "\n",
    "    # val_loss0.append(val_epoch_loss0)\n",
    "    # val_gLoss0.append(val_epoch_gLoss0.detach().cpu())\n",
    "            \n",
    "    # pointpillars.train()\n",
    "\n",
    "    # if (epoch + 1) % args.ckpt_freq_epoch == 0:\n",
    "    #     torch.save(pointpillars.state_dict(), os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth'))\n",
    "    #     checkpoint = {\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': pointpillars.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'loss': loss\n",
    "    #     }   \n",
    "    #     torch.save(checkpoint, os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth.tar'))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'train_loss.npy'), np.array(training_loss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'train_gloss.npy'), np.array(training_gLoss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'val_loss.npy'), np.array(val_loss0))\n",
    "    #     np.save(os.path.join(saved_ckpt_path, f'val_gloss.npy'), np.array(val_gLoss0))\n",
    "\n",
    "    # if (epoch + 1) % args.plot_freq_epoch == 0:\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(training_loss0))\n",
    "plt.plot(np.array(training_gLoss0)*0.01)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend([\"loss\", \"g_loss\"])\n",
    "plt.show()\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(np.array(val_loss0))\n",
    "    #     plt.plot(np.array(val_gLoss0))\n",
    "    #     plt.title(\"Validation Loss\")\n",
    "    #     plt.legend([\"loss\",\"prediction_loss\"])\n",
    "    #     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
