{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI Sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(69)\n",
    "\n",
    "window_len = 20\n",
    "\n",
    "sequences = [\"0000\", \"0001\", \"0002\", \"0003\",\n",
    "            \"0004\", \"0005\",\n",
    "            \"0008\", \"0010\", \"0011\",\n",
    "            \"0012\", \"0013\", \"0014\", \"0015\",\n",
    "            \"0016\", \"0017\", \"0018\", \"0019\", \"0020\"]\n",
    "\n",
    "all_files = []\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    bin_files = glob.glob(os.path.join(\"dataset/KITTI/training/velodyne/\" + sequence, '*.bin'))\n",
    "    filenames = [(sequence + \"_\" + os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "    filenames.sort()\n",
    "    \n",
    "    filenames_actual = [(os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "    filenames_actual.sort()\n",
    "\n",
    "    total_files = len(filenames)\n",
    "    for idx in range(0, total_files, window_len):\n",
    "        if np.random.rand() < 0.5:\n",
    "            train_list.extend(filenames[idx:min(idx+window_len, total_files)])\n",
    "        else:\n",
    "            val_list.extend(filenames[idx:min(idx+window_len, total_files)])\n",
    "\n",
    "    all_files.extend(filenames)\n",
    "\n",
    "    for filename in filenames_actual:\n",
    "        shutil.move(\"dataset/KITTI/training/velodyne/\" + sequence + \"/\" + filename + \".bin\", \"dataset/KITTI/training/velodyne/\" + sequence + \"_\" + filename + \".bin\")\n",
    "        shutil.copy(\"dataset/KITTI/training/calib/\" + sequence + \".txt\", \"dataset/KITTI/training/calib/\" + sequence + \"_\" + filename + \".txt\")\n",
    "        shutil.move(\"dataset/KITTI/training/image_2/\" + sequence + \"/\" + filename + \".png\", \"dataset/KITTI/training/image_2/\" + sequence + \"_\" + filename + \".png\")\n",
    "        shutil.move(\"dataset/KITTI/training/label_2/\" + sequence + \"/\" + filename + \".txt\", \"dataset/KITTI/training/label_2/\" + sequence + \"_\" + filename + \".txt\")\n",
    "\n",
    "# print(all_files)\n",
    "# print(train_list)\n",
    "# print(val_list)\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open('dataset/ImageSets/train.txt', 'w') as f:\n",
    "    for item in train_list:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "with open('dataset/ImageSets/val.txt', 'w') as f:\n",
    "    for item in val_list:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('dataset/ImageSets/trainval.txt', 'w') as f:\n",
    "    for item in all_files:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)      \n",
    "\n",
    "# for filename in tqdm(filenames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(69)\n",
    "\n",
    "sequences = [\"0000\", \"0001\", \"0002\", \"0003\",\n",
    "            \"0004\", \"0005\", \"0006\", \"0007\",\n",
    "            \"0008\", \"0009\", \"0010\", \"0011\",\n",
    "            \"0012\", \"0013\", \"0014\", \"0015\",\n",
    "            \"0016\", \"0017\", \"0018\", \"0019\", \n",
    "            \"0020\", \"0021\", \"0022\", \"0023\",\n",
    "            \"0024\", \"0025\", \"0026\", \"0027\", \"0028\"]\n",
    "\n",
    "all_files = []\n",
    "\n",
    "\n",
    "for sequence in sequences:\n",
    "    bin_files = glob.glob(os.path.join(\"dataset/KITTI/testing/velodyne/\" + sequence, '*.bin'))\n",
    "    filenames = [(sequence + \"_\" + os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "    filenames.sort()\n",
    "    \n",
    "    filenames_actual = [(os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "    filenames_actual.sort()\n",
    "\n",
    "    all_files.extend(filenames)\n",
    "\n",
    "    for filename in filenames_actual:\n",
    "        shutil.move(\"dataset/KITTI/testing/velodyne/\" + sequence + \"/\" + filename + \".bin\", \"dataset/KITTI/testing/velodyne/\" + sequence + \"_\" + filename + \".bin\")\n",
    "        shutil.copy(\"dataset/KITTI/testing/calib/\" + sequence + \".txt\", \"dataset/KITTI/testing/calib/\" + sequence + \"_\" + filename + \".txt\")\n",
    "        shutil.move(\"dataset/KITTI/testing/image_2/\" + sequence + \"/\" + filename + \".png\", \"dataset/KITTI/testing/image_2/\" + sequence + \"_\" + filename + \".png\")\n",
    "\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open('dataset/ImageSets/test.txt', 'w') as f:\n",
    "    for item in all_files:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    \n",
    "\n",
    "# for filename in tqdm(filenames):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sequences = [\"0000\", \"0001\", \"0002\", \"0003\",\n",
    "            \"0004\", \"0005\",\n",
    "            \"0008\", \"0010\", \"0011\",\n",
    "            \"0012\", \"0013\", \"0014\", \"0015\",\n",
    "            \"0016\", \"0017\", \"0018\", \"0019\", \"0020\"]\n",
    "\n",
    "all_files = []\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    bin_files = glob.glob(os.path.join(\"dataset/KITTI/training/velodyne/\" + sequence, '*.bin'))\n",
    "    filenames = [(os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "    filenames.sort()\n",
    "    \n",
    "    df = pd.read_csv(\"dataset/KITTI/training/label_2_tracking/\" + sequence +  \".txt\", sep=\"\\s+\", header=None)\n",
    "    frame_no = df[0].tolist()\n",
    "    df.drop(df.columns[[0, 1]], axis=1, inplace=True)\n",
    "\n",
    "    label_destination = \"dataset/KITTI/training/label_2/\" + sequence\n",
    "\n",
    "    if not os.path.exists(label_destination):\n",
    "        os.makedirs(label_destination)\n",
    "\n",
    "    total_files = len(filenames)\n",
    "\n",
    "    for idx in range(total_files):\n",
    "        indices = [i for i, frame in enumerate(frame_no) if frame == idx]\n",
    "        frame_labels_df = df.iloc[indices]\n",
    "        frame_labels_df.to_csv(label_destination + \"/\" + filenames[idx] + \".txt\", sep = ' ', index = False, header = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '000258.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([prefix, number])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecrement_string_number\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/data/kitti/0001_000258.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Output: '0000_000039'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m, in \u001b[0;36mdecrement_string_number\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      3\u001b[0m prefix, number \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the number part to an integer, decrement it, and convert it back to a string\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;28mlen\u001b[39m(number))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Join everything back together\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([prefix, number])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '000258.bin'"
     ]
    }
   ],
   "source": [
    "def decrement_string_number(s):\n",
    "    # Split the string into parts before and after the underscore\n",
    "    prefix, number = s.split('_')\n",
    "    # Convert the number part to an integer, decrement it, and convert it back to a string\n",
    "    number = str(int(number) - 1).zfill(len(number))\n",
    "    # Join everything back together\n",
    "    return '_'.join([prefix, number])\n",
    "\n",
    "# Usage\n",
    "print(decrement_string_number('dataset/data/kitti/0001_000258.bin'))  # Output: '0000_000039'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/data/kitti/0001_000257\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def decrease_numeric_part(input_string):\n",
    "    numeric_parts = re.findall(r'\\d+', input_string)\n",
    "    if not numeric_parts:\n",
    "        return input_string\n",
    "    last_numeric_part = numeric_parts[-1]\n",
    "    decreased_part = str(int(last_numeric_part) - 1).zfill(len(last_numeric_part))\n",
    "    output_string = input_string[::-1].replace(last_numeric_part[::-1], decreased_part[::-1], 1)[::-1]\n",
    "    return output_string\n",
    "\n",
    "# Test the function\n",
    "input_string = 'dataset/data/kitti/0001_000258.bin'\n",
    "output_string = decrease_numeric_part(input_string)\n",
    "print(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6401\n",
      "6401\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(69)\n",
    "\n",
    "window_len = 20\n",
    "\n",
    "\n",
    "all_files = []\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "\n",
    "bin_files = glob.glob(os.path.join(\"dataset/KITTI/training/velodyne/*.bin\"))\n",
    "\n",
    "filenames_actual = [(os.path.splitext(os.path.basename(file))[0]) for file in bin_files]\n",
    "filenames_actual.sort()\n",
    "\n",
    "total_files = len(filenames_actual)\n",
    "for idx in range(0, total_files, window_len):\n",
    "    if np.random.rand() < 0.5:\n",
    "        train_list.extend(filenames_actual[idx:min(idx+window_len, total_files)])\n",
    "    else:\n",
    "        val_list.extend(filenames_actual[idx:min(idx+window_len, total_files)])\n",
    "\n",
    "\n",
    "# print(all_files)\n",
    "# print(train_list)\n",
    "# print(val_list)\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open('dataset/ImageSets/train.txt', 'w') as f:\n",
    "    for item in train_list:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "with open('dataset/ImageSets/val.txt', 'w') as f:\n",
    "    for item in val_list:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('dataset/ImageSets/trainval.txt', 'w') as f:\n",
    "    for item in filenames_actual:\n",
    "        # Write each item on a new line\n",
    "        f.write(\"%s\\n\" % item)      \n",
    "\n",
    "# for filename in tqdm(filenames):\n",
    "\n",
    "print(len(train_list) + len(val_list))\n",
    "print(total_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
