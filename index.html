<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Latent Pruning for Compute Optimization in 3D Object Detection Frameworks</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Pruning for Compute Optimization in 3D Object Detection Frameworks</h1>


          
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Mir Sayeed Mohammad</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Mizanur Rahaman Nayan</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Sujoy Mondal</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Uday Kamal</a>
                  </span>
                  </div>  
     
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Institute of Technology<br>CS 7641</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Our work aims to reduce the computation in a deep neural network model deployed in 3D object detection task by removing channels in different model layers and optimizing inference. Structured pruning methods, such as CoFi, combine coarse and fine-grained units with layer-wise distillation to enhance model compression and inference speed without significant accuracy loss (Yu et al.; Wang et al.). Layer and head pruning in Transformer models also effectively reduce model size while maintaining performance (Yu et al.). Regularization-based pruning methods like L1 regularization and growing regularization gradually drive unimportant weights to zero before pruning, balancing parameter reduction and accuracy (Yeh et al.; Wang et al., "Neural Pruning via Growing Regularization"). Importance-based pruning uses criteria like neuron importance score propagation (NISP) and Taylor expansion-based methods to iteratively prune less important neurons, retaining essential features and performance (Yu et al.; Molchanov et al.; Sunil and Salem; Liu et al.). These techniques collectively improve the efficiency and performance of neural networks by systematically reducing their complexity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            LIDAR scanners are vital for autonomous systems, providing essential point cloud data. Processing this data incurs significant computational costs. Techniques like PointPillars (Lang, Alex H., et al.) convert point clouds into voxels, often leading to redundancy. Optimizing voxelized data processing enhances efficiency without sacrificing accuracy. Methods like Matryoshka Representation Learning prune feature channels, reducing computational load while maintaining performance. This project integrates pruning within PointPillars, using adaptive latent regularization to cut computation and memory needs, evaluating detection accuracy across various scenes and object classes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>Pointcloud data is a stream of points reflecting LASER in the Cartesian coordinate system. For object detection, the space is converted to a grid structure called voxels. In the PointPillars architecture, points are filtered within a field of view, grouped into pillars in the x-y plane, and augmented to ensure each pillar contains the same number of points. The pillars then pass through an invariant dense layer, creating a fixed number of features, forming 2D pseudo images. Noise is added during preprocessing and pillar encoding to compensate for sensor uncertainty, with additional augmentation methods like rotation and flipping.
          </p> <p>
            Object detection frameworks commonly use algorithms such as anchor generation, bounding box encoding, and non-max suppression. Models like PointNet, PointNet++, PV-RCNN, VoxelNet, and F-ConvNet are utilized for 3D object detection. These pipelines include modules like the encoder, backbone, and model head, with the dense backbone handling most feature processing and computation. Our goal is to develop a model-agnostic learning approach, ensuring the pruning method applies to all convolution/dense layer-based architectures.
            <figure><img src="fig/pointpillars.png" alt="PointPillars architecture">
              <figcaption>PointPillars architecture</figcaption>
            </figure>
          </p><p>
            Our work optimizes model architecture by pruning parallel channels in each layer. Not all feature channels are necessary, and the latent space often contains redundant features. The pruning method regularizes channels, imposing higher penalties on higher channel outputs, encouraging the model to minimize unnecessary channels to zero output. During inference, a distilled smaller model retains only the non-zero parts of the parent model, improving efficiency while reducing computational requirements.
            <figure>
            <img src="fig/pruning.png" alt="Proposed pruning method">
            <figcaption>Proposed pruning method</figcaption>
          </figure>
          </p>
          <p>
            <h4>Midterm Checkpoint</h4>
            We jointly trained several model heads simultaneously utilizing different number of channels from the pillar feature extractor. The detection losses for each head are jointly optimized. The modified training scheme is provided below:
            <figure>
              <img src="fig/matryoshka.png" alt="matryoshka">
              <figcaption>Implemented Matryoshka representation learning (Kusupati, Aditya, et al.)</figcaption>
            </figure>
            The key motivation behind this implementation is that the point cloud feature extractor is common to all the model heads, and based on the desired level of computation, a different model head can be selected that uses more or less number of features. The computational advantage of the different model heads are the key motivation behind this scheme.
            <figure>
              <img src="fig/computation.png" alt="Computation">
              <figcaption>Reducing feature volume from pillar feature net to backbone by 75% can reduce host computation by ~93.2% and memory usage by ~93.6%.</figcaption>              
            </figure> 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results and Discussion</h2>
        <div class="content has-text-justified">
          <p>
            The goal is to reduce frame-by-frame computation in object detection while maintaining performance. We will evaluate performance vs. computation, aiming to minimize the curve's slope, meaning reduced computation with minimal performance loss. Key metrics: 2D AP, BEV AP, 3D AP, and MACs/FLOPs during inference.
          </p>  
          <p>
            <h4>Midterm Checkpoint</h4>
            The joint training scheme converges after 160 training epochs and are evaluated for performance.
            <figure>
              <img src="fig/matryoshka_training.png" alt="trainig">
              <figcaption>From train and validation loss plots, we can see that lowering the number of channels causes the loss to saturate at a higher global minima, which is also reflected in the performance metrics. </figcaption>
            </figure>
            One interesting key observation in the 2D bounding box average precision is that the detection of vehicle class objects are consistent for reduced feature channels, meaning that for easy/less cluttered scenes, vehicles can be easily detected at 93% less computation overhead - which is a signifiant achievement. The key problem is knowing when to switch between the different model heads.
            <figure>
              <img src="fig/matryoshka_out_2D.png" alt="2D">
              <figcaption>2D bounding box average precision for easy-medium-hard splits for different feature channels </figcaption>              
            </figure> 
            For the 3D bounding boxes, the performance consistently goes down. As shown later, the different feature channels do not learn any form of sparsity, which might be one of the key reasons why pruning channels result in performance loss.
            <figure>
              <img src="fig/matryoshka_out_3D.png" alt="3D">
              <figcaption>3D bounding box average precision for easy-medium-hard splits for different feature channels</figcaption>              
            </figure> 
            <figure>
              <img src="fig/matryoshka_qualitative_easy.png" alt="qualeasy">
              <figcaption>Visual detection results in an easy scene. We can see that vehicles (large objects) are mostly detected fine in easy scenes for any number of feature channels.</figcaption>              
            </figure> 
            <figure>
              <img src="fig/matryoshka_qualitative_hard.png" alt="qualHard">
              <figcaption>Visual results in a difficult scene. We can see that lowering the number of channels is often causing false detections in smaller objects.</figcaption>              
            </figure>  
            <figure>
              <img src="fig/matryoshka_energy.png" alt="energy">
              <figcaption>Channel-wise energy of 64 channels for 20 random samples. We can see that there is no specific trend in the feature energies, meaning that feature channels are not explicitly trained to be prunable which might cause large levels of performance degradation.</figcaption>              
            </figure>                       
          </p>

            <figure>
              <img src="fig/gantt.jpg" alt="Gantt">
              <figcaption>Gantt Chart for Workflow</figcaption>
            </figure>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Literature Review -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <ol>
          <li>
              Lang, Alex H., et al. "Pointpillars: Fast encoders for object detection from point clouds." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.
          </li>
          <li>
              Yu, Chenglin, et al. "Structured Pruning Learns Compact and Accurate Models." <em>arXiv</em>, 2022.
          </li>
          <li>
              Wang, Jiaxiang, et al. "Structured Pruning for Deep Convolutional Neural Networks: A Survey." <em>arXiv</em>, 2023.
          </li>
          <li>
              Yeh, Chih-Kuan, et al. "Deep Trim: Revisiting L1 Regularization for Connection Pruning of Deep Networks." <em>Papers with Code</em>, 2019.
          </li>
          <li>
              Wang, Jiaxiang, et al. "Neural Pruning via Growing Regularization." <em>arXiv</em>, 2020.
          </li>
          <li>
              Yu, Ritchie, et al. "NISP: Pruning Networks Using Neuron Importance Score Propagation." <em>arXiv</em>, 2017.
          </li>
          <li>
              Molchanov, Pavlo, et al. "Importance Estimation for Neural Network Pruning." <em>arXiv</em>, 2019.
          </li>
          <li>
              Sunil, Vadera, and Salem Ameen. "Methods for Pruning Deep Neural Networks." <em>arXiv</em>, 2020.
          </li>
          <li>
              Liu, Zhuang, et al. "Rethinking the Value of Network Pruning." <em>arXiv</em>, 2019.
          </li>
          <li>
            Kusupati, Aditya, et al. "Matryoshka representation learning." Advances in Neural Information Processing Systems 35 (2022): 30233-30249.
          </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/W3zkv7_Dq3Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->








  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            </a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
