{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import setup_seed\n",
    "from dataset import Kitti, get_dataloader\n",
    "from model import PointPillars\n",
    "from loss import Loss\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary(writer, loss_dict, global_step, tag, lr=None, momentum=None):\n",
    "    for k, v in loss_dict.items():\n",
    "        writer.add_scalar(f'{tag}/{k}', v, global_step)\n",
    "    if lr is not None:\n",
    "        writer.add_scalar('lr', lr, global_step)\n",
    "    if momentum is not None:\n",
    "        writer.add_scalar('momentum', momentum, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_root = \"dataset/KITTI\"\n",
    "        self.saved_path = \"logs/pillar_sequence\"\n",
    "        self.batch_size = 4\n",
    "        self.num_workers = 4\n",
    "        self.nclasses = 3\n",
    "        self.init_lr = 0.00025\n",
    "        self.max_epoch = 160\n",
    "        self.log_freq = 8\n",
    "        self.ckpt_freq_epoch = 20\n",
    "        self.no_cuda = not torch.cuda.is_available()\n",
    " \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "train_dataset = Kitti(data_root=args.data_root,\n",
    "                        split='train')\n",
    "val_dataset = Kitti(data_root=args.data_root,\n",
    "                    split='val')\n",
    "train_dataloader = get_dataloader(dataset=train_dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    num_workers=args.num_workers,\n",
    "                                    shuffle=True)\n",
    "val_dataloader = get_dataloader(dataset=val_dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                num_workers=args.num_workers,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18200, 4)\n"
     ]
    }
   ],
   "source": [
    "data_dict = val_dataset.__getitem__(8)\n",
    "print(data_dict['pts'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, Optimizer, Scheduler, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.no_cuda:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses).cuda()\n",
    "else:\n",
    "    pointpillars = PointPillars(nclasses=args.nclasses)\n",
    "\n",
    "loss_func = Loss()\n",
    "\n",
    "max_iters = 2*len(train_dataloader) * args.max_epoch\n",
    "init_lr = args.init_lr\n",
    "optimizer = torch.optim.AdamW(params=pointpillars.parameters(), \n",
    "                                lr=init_lr, \n",
    "                                betas=(0.95, 0.99),\n",
    "                                weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,  \n",
    "                                                max_lr=init_lr*10, \n",
    "                                                total_steps=max_iters, \n",
    "                                                pct_start=0.4, \n",
    "                                                anneal_strategy='cos',\n",
    "                                                cycle_momentum=True, \n",
    "                                                base_momentum=0.95*0.895, \n",
    "                                                max_momentum=0.95,\n",
    "                                                div_factor=10)\n",
    "\n",
    "saved_logs_path = os.path.join(args.saved_path, 'summary')\n",
    "os.makedirs(saved_logs_path, exist_ok=True)\n",
    "writer = SummaryWriter(saved_logs_path)\n",
    "saved_ckpt_path = os.path.join(args.saved_path, 'checkpoints')\n",
    "os.makedirs(saved_ckpt_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1317 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Voxelization is treated as a zero-op.\n",
      "Warning: module PillarLayer is treated as a zero-op.\n",
      "Warning: module PillarEncoder is treated as a zero-op.\n",
      "Warning: module Backbone is treated as a zero-op.\n",
      "Warning: module Neck is treated as a zero-op.\n",
      "Warning: module Head is treated as a zero-op.\n",
      "Warning: module PointPillars is treated as a zero-op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayeed/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointPillars(\n",
      "  6.36 M, 100.000% Params, 63.29 GMac, 99.922% MACs, \n",
      "  (pillar_layer): PillarLayer(\n",
      "    0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (voxel_layer): Voxelization(voxel_size=[0.16, 0.16, 4], point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1], max_num_points=32, max_voxels=(16000, 40000), deterministic=True)\n",
      "  )\n",
      "  (pillar_encoder): PillarEncoder(\n",
      "    704, 0.011% Params, 22.53 KMac, 0.000% MACs, \n",
      "    (conv): Conv1d(576, 0.009% Params, 18.43 KMac, 0.000% MACs, 9, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (bn): BatchNorm1d(128, 0.002% Params, 4.1 KMac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (backbone0): Backbone(\n",
      "    4.21 M, 66.155% Params, 29.71 GMac, 46.904% MACs, \n",
      "    (multi_blocks): ModuleList(\n",
      "      4.21 M, 66.155% Params, 29.71 GMac, 46.904% MACs, \n",
      "      (0): Sequential(\n",
      "        147.97 k, 2.326% Params, 7.94 GMac, 12.536% MACs, \n",
      "        (0): Conv2d(36.86 k, 0.580% Params, 1.97 GMac, 3.118% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 6.86 MMac, 0.011% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 3.43 MMac, 0.005% MACs, inplace=True)\n",
      "        (3): Conv2d(36.86 k, 0.580% Params, 1.97 GMac, 3.118% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, 0.002% Params, 6.86 MMac, 0.011% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 3.43 MMac, 0.005% MACs, inplace=True)\n",
      "        (6): Conv2d(36.86 k, 0.580% Params, 1.97 GMac, 3.118% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(128, 0.002% Params, 6.86 MMac, 0.011% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 3.43 MMac, 0.005% MACs, inplace=True)\n",
      "        (9): Conv2d(36.86 k, 0.580% Params, 1.97 GMac, 3.118% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(128, 0.002% Params, 6.86 MMac, 0.011% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 3.43 MMac, 0.005% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        812.54 k, 12.775% Params, 10.89 GMac, 17.196% MACs, \n",
      "        (0): Conv2d(73.73 k, 1.159% Params, 987.37 MMac, 1.559% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "        (3): Conv2d(147.46 k, 2.318% Params, 1.97 GMac, 3.118% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "        (6): Conv2d(147.46 k, 2.318% Params, 1.97 GMac, 3.118% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "        (9): Conv2d(147.46 k, 2.318% Params, 1.97 GMac, 3.118% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "        (12): Conv2d(147.46 k, 2.318% Params, 1.97 GMac, 3.118% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "        (15): Conv2d(147.46 k, 2.318% Params, 1.97 GMac, 3.118% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(256, 0.004% Params, 3.43 MMac, 0.005% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 1.71 MMac, 0.003% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        3.25 M, 51.053% Params, 10.88 GMac, 17.172% MACs, \n",
      "        (0): Conv2d(294.91 k, 4.637% Params, 987.37 MMac, 1.559% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "        (3): Conv2d(589.82 k, 9.274% Params, 1.97 GMac, 3.118% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "        (6): Conv2d(589.82 k, 9.274% Params, 1.97 GMac, 3.118% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "        (9): Conv2d(589.82 k, 9.274% Params, 1.97 GMac, 3.118% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "        (12): Conv2d(589.82 k, 9.274% Params, 1.97 GMac, 3.118% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "        (15): Conv2d(589.82 k, 9.274% Params, 1.97 GMac, 3.118% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(512, 0.008% Params, 1.71 MMac, 0.003% MACs, 256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 857.09 KMac, 0.001% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck0): Neck(\n",
      "    598.78 k, 9.414% Params, 32.1 GMac, 50.674% MACs, \n",
      "    (decoder_blocks): ModuleList(\n",
      "      598.78 k, 9.414% Params, 32.1 GMac, 50.674% MACs, \n",
      "      (0): Sequential(\n",
      "        8.45 k, 0.133% Params, 459.4 MMac, 0.725% MACs, \n",
      "        (0): ConvTranspose2d(8.19 k, 0.129% Params, 438.83 MMac, 0.693% MACs, 64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.004% Params, 13.71 MMac, 0.022% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 6.86 MMac, 0.011% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        65.79 k, 1.034% Params, 3.53 GMac, 5.575% MACs, \n",
      "        (0): ConvTranspose2d(65.54 k, 1.030% Params, 3.51 GMac, 5.543% MACs, 128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.004% Params, 13.71 MMac, 0.022% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 6.86 MMac, 0.011% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        524.54 k, 8.247% Params, 28.11 GMac, 44.373% MACs, \n",
      "        (0): ConvTranspose2d(524.29 k, 8.243% Params, 28.09 GMac, 44.341% MACs, 256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.004% Params, 13.71 MMac, 0.022% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 6.86 MMac, 0.011% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head0): Head(\n",
      "    27.72 k, 0.436% Params, 1.48 GMac, 2.344% MACs, \n",
      "    (conv_cls): Conv2d(6.93 k, 0.109% Params, 371.23 MMac, 0.586% MACs, 384, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_reg): Conv2d(16.17 k, 0.254% Params, 866.19 MMac, 1.368% MACs, 384, 42, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_dir_cls): Conv2d(4.62 k, 0.073% Params, 247.48 MMac, 0.391% MACs, 384, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (backbone1): Backbone(\n",
      "    1.05 M, 16.559% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (multi_blocks): ModuleList(\n",
      "      1.05 M, 16.559% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Sequential(\n",
      "        37.12 k, 0.584% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        203.52 k, 3.200% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(18.43 k, 0.290% Params, 0.0 Mac, 0.000% MACs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (12): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (15): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        812.54 k, 12.775% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(73.73 k, 1.159% Params, 0.0 Mac, 0.000% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(147.46 k, 2.318% Params, 0.0 Mac, 0.000% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(147.46 k, 2.318% Params, 0.0 Mac, 0.000% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(147.46 k, 2.318% Params, 0.0 Mac, 0.000% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (12): Conv2d(147.46 k, 2.318% Params, 0.0 Mac, 0.000% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (15): Conv2d(147.46 k, 2.318% Params, 0.0 Mac, 0.000% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(256, 0.004% Params, 0.0 Mac, 0.000% MACs, 128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck1): Neck(\n",
      "    149.89 k, 2.357% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (decoder_blocks): ModuleList(\n",
      "      149.89 k, 2.357% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Sequential(\n",
      "        2.18 k, 0.034% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(2.05 k, 0.032% Params, 0.0 Mac, 0.000% MACs, 32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        16.51 k, 0.260% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(16.38 k, 0.258% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        131.2 k, 2.063% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(131.07 k, 2.061% Params, 0.0 Mac, 0.000% MACs, 128, 64, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head1): Head(\n",
      "    13.9 k, 0.218% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (conv_cls): Conv2d(3.47 k, 0.055% Params, 0.0 Mac, 0.000% MACs, 192, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_reg): Conv2d(8.11 k, 0.127% Params, 0.0 Mac, 0.000% MACs, 192, 42, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_dir_cls): Conv2d(2.32 k, 0.036% Params, 0.0 Mac, 0.000% MACs, 192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (backbone2): Backbone(\n",
      "    263.94 k, 4.150% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (multi_blocks): ModuleList(\n",
      "      263.94 k, 4.150% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Sequential(\n",
      "        9.34 k, 0.147% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(2.3 k, 0.036% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(2.3 k, 0.036% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(2.3 k, 0.036% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(2.3 k, 0.036% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        51.07 k, 0.803% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(4.61 k, 0.072% Params, 0.0 Mac, 0.000% MACs, 16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (12): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (15): Conv2d(9.22 k, 0.145% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        203.52 k, 3.200% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(18.43 k, 0.290% Params, 0.0 Mac, 0.000% MACs, 32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (3): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (6): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (8): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (9): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (11): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (12): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "        (15): Conv2d(36.86 k, 0.580% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (16): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (17): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck2): Neck(\n",
      "    37.57 k, 0.591% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (decoder_blocks): ModuleList(\n",
      "      37.57 k, 0.591% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Sequential(\n",
      "        576, 0.009% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(512, 0.008% Params, 0.0 Mac, 0.000% MACs, 16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        4.16 k, 0.065% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(4.1 k, 0.064% Params, 0.0 Mac, 0.000% MACs, 32, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        32.83 k, 0.516% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): ConvTranspose2d(32.77 k, 0.515% Params, 0.0 Mac, 0.000% MACs, 64, 32, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "        (1): BatchNorm2d(64, 0.001% Params, 0.0 Mac, 0.000% MACs, 32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head2): Head(\n",
      "    6.98 k, 0.110% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (conv_cls): Conv2d(1.75 k, 0.027% Params, 0.0 Mac, 0.000% MACs, 96, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_reg): Conv2d(4.07 k, 0.064% Params, 0.0 Mac, 0.000% MACs, 96, 42, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_dir_cls): Conv2d(1.16 k, 0.018% Params, 0.0 Mac, 0.000% MACs, 96, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "FLOPs: 63.34 GMac\n",
      "Parameters: 6.36 M\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, data_dict in enumerate(tqdm(train_dataloader)):\n",
    "        break\n",
    "\n",
    "    if not args.no_cuda:\n",
    "        # move the tensors to the cuda\n",
    "        for key in data_dict:\n",
    "            for j, item in enumerate(data_dict[key]):\n",
    "                if torch.is_tensor(item):\n",
    "                    data_dict[key][j] = data_dict[key][j].cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batched_pts = data_dict['batched_pts']\n",
    "\n",
    "    input_size = tuple(batched_pts[0].shape)\n",
    "\n",
    "    macs, params = get_model_complexity_info(pointpillars, input_size, as_strings=True,\n",
    "                                                 print_per_layer_stat=True, verbose=True)\n",
    "\n",
    "    print(f\"FLOPs: {macs}\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "\n",
    "# bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, \\\n",
    "#                 bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "#                     bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, \\\n",
    "#                         anchor_target_dict = pointpillars(batched_pts=batched_pts, \n",
    "#                             mode='train',\n",
    "#                             batched_gt_bboxes=batched_gt_bboxes, \n",
    "#                             batched_gt_labels=batched_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fvcore.nn.flop_count.FlopCountAnalysis object at 0x7fd9a2699340>\n"
     ]
    }
   ],
   "source": [
    "print(flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, data_dict in enumerate(tqdm(train_dataloader)):\\n    if not args.no_cuda:\\n        # move the tensors to the cuda\\n        print(\"Here\")\\n        for key in data_dict:\\n            for j, item in enumerate(data_dict[key]):\\n                if torch.is_tensor(item):\\n                    data_dict[key][j] = data_dict[key][j].cuda()\\n    \\n    optimizer.zero_grad()\\n\\n    batched_pts = data_dict[\\'batched_pts\\']\\n    batched_gt_bboxes = data_dict[\\'batched_gt_bboxes\\']\\n    batched_labels = data_dict[\\'batched_labels\\']\\n    batched_difficulty = data_dict[\\'batched_difficulty\\']\\n    bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0,         bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1,             bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict =                 pointpillars(batched_pts=batched_pts, \\n                                mode=\\'train\\',\\n                                batched_gt_bboxes=batched_gt_bboxes, \\n                                batched_gt_labels=batched_labels)\\n    \\n    break\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i, data_dict in enumerate(tqdm(train_dataloader)):\n",
    "    if not args.no_cuda:\n",
    "        # move the tensors to the cuda\n",
    "        print(\"Here\")\n",
    "        for key in data_dict:\n",
    "            for j, item in enumerate(data_dict[key]):\n",
    "                if torch.is_tensor(item):\n",
    "                    data_dict[key][j] = data_dict[key][j].cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batched_pts = data_dict['batched_pts']\n",
    "    batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "    batched_labels = data_dict['batched_labels']\n",
    "    batched_difficulty = data_dict['batched_difficulty']\n",
    "    bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, \\\n",
    "        bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "            bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict = \\\n",
    "                pointpillars(batched_pts=batched_pts, \n",
    "                                mode='train',\n",
    "                                batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                batched_gt_labels=batched_labels)\n",
    "    \n",
    "    break\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss0 = []\n",
    "training_loss1 = []\n",
    "training_loss2 = []\n",
    "\n",
    "val_loss0 = []\n",
    "val_loss1 = []\n",
    "val_loss2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1317 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'batched_gt_bboxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m batched_labels \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatched_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m batched_difficulty \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatched_difficulty\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, \\\n\u001b[1;32m     30\u001b[0m     bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n\u001b[0;32m---> 31\u001b[0m         bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2,  \u001b[38;5;241m=\u001b[39m anchor_target_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpointpillars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_pts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatched_gt_bboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_gt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatched_gt_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m################# Full features #################\u001b[39;00m\n\u001b[1;32m     38\u001b[0m bbox_cls_pred0 \u001b[38;5;241m=\u001b[39m bbox_cls_pred0\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mnclasses)\n",
      "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'batched_gt_bboxes'"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.max_epoch):\n",
    "# for epoch in range(args.max_epoch):\n",
    "    epoch_loss0 = 0\n",
    "    # epoch_loss1 = 0\n",
    "    # epoch_loss2 = 0\n",
    "\n",
    "    val_epoch_loss0 = 0\n",
    "    # val_epoch_loss1 = 0\n",
    "    # val_epoch_loss2 = 0\n",
    "\n",
    "    print('=' * 20, epoch, '=' * 20)\n",
    "    train_step, val_step = 0, 0\n",
    "    for i, data_dict in enumerate(tqdm(train_dataloader)):\n",
    "        if not args.no_cuda:\n",
    "            # move the tensors to the cuda\n",
    "            for key in data_dict:\n",
    "                for j, item in enumerate(data_dict[key]):\n",
    "                    if torch.is_tensor(item):\n",
    "                        data_dict[key][j] = data_dict[key][j].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batched_pts = data_dict['batched_pts']\n",
    "        batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "        batched_labels = data_dict['batched_labels']\n",
    "        batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "\n",
    "        bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, \\\n",
    "            bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "                bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2, anchor_target_dict = pointpillars(batched_pts=batched_pts, \n",
    "                                    mode='train',\n",
    "                                    batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                    batched_gt_labels=batched_labels)\n",
    "\n",
    "        \n",
    "        ################# Full features #################\n",
    "        bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "        bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "        bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "        # ################# Half features #################\n",
    "        # bbox_cls_pred1 = bbox_cls_pred1.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "        # bbox_pred1 = bbox_pred1.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "        # bbox_dir_cls_pred1 = bbox_dir_cls_pred1.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "        # ################# Quar features #################\n",
    "        # bbox_cls_pred2 = bbox_cls_pred2.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "        # bbox_pred2 = bbox_pred2.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "        # bbox_dir_cls_pred2 = bbox_dir_cls_pred2.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "\n",
    "        batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "        batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "        batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "        batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "\n",
    "        \n",
    "        pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "        bbox_pred0 = bbox_pred0[pos_idx]\n",
    "        # bbox_pred1 = bbox_pred1[pos_idx]\n",
    "        # bbox_pred2 = bbox_pred2[pos_idx]\n",
    "\n",
    "        batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "        batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "        # batched_bbox_reg1 = batched_bbox_reg.clone()\n",
    "        # batched_bbox_reg2 = batched_bbox_reg.clone()\n",
    "\n",
    "        # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "        bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "        # bbox_pred1[:, -1] = torch.sin(bbox_pred1[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "        # bbox_pred2[:, -1] = torch.sin(bbox_pred2[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "\n",
    "        batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "        # batched_bbox_reg1[:, -1] = torch.cos(bbox_pred1[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "        # batched_bbox_reg2[:, -1] = torch.cos(bbox_pred2[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "\n",
    "\n",
    "        bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "        # bbox_dir_cls_pred1 = bbox_dir_cls_pred1[pos_idx]\n",
    "        # bbox_dir_cls_pred2 = bbox_dir_cls_pred2[pos_idx]\n",
    "\n",
    "        batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "\n",
    "        num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "\n",
    "        bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "        # bbox_cls_pred1 = bbox_cls_pred1[batched_label_weights > 0]\n",
    "        # bbox_cls_pred2 = bbox_cls_pred2[batched_label_weights > 0]\n",
    "\n",
    "        batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "        batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "        loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                                bbox_pred=bbox_pred0,\n",
    "                                bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                                batched_labels=batched_bbox_labels, \n",
    "                                num_cls_pos=num_cls_pos, \n",
    "                                batched_bbox_reg=batched_bbox_reg0, \n",
    "                                batched_dir_labels=batched_dir_labels)\n",
    "        \n",
    "        # loss_dict1 = loss_func(bbox_cls_pred=bbox_cls_pred1,\n",
    "        #                         bbox_pred=bbox_pred1,\n",
    "        #                         bbox_dir_cls_pred=bbox_dir_cls_pred1,\n",
    "        #                         batched_labels=batched_bbox_labels, \n",
    "        #                         num_cls_pos=num_cls_pos, \n",
    "        #                         batched_bbox_reg=batched_bbox_reg1, \n",
    "        #                         batched_dir_labels=batched_dir_labels)\n",
    "        \n",
    "        # loss_dict2 = loss_func(bbox_cls_pred=bbox_cls_pred2,\n",
    "        #                         bbox_pred=bbox_pred2,\n",
    "        #                         bbox_dir_cls_pred=bbox_dir_cls_pred2,\n",
    "        #                         batched_labels=batched_bbox_labels, \n",
    "        #                         num_cls_pos=num_cls_pos, \n",
    "        #                         batched_bbox_reg=batched_bbox_reg2, \n",
    "        #                         batched_dir_labels=batched_dir_labels)\n",
    "        \n",
    "        loss0 = loss_dict0['total_loss'] \n",
    "        # loss1 = loss_dict1['total_loss']\n",
    "        # loss2 = loss_dict2['total_loss'] \n",
    "        loss = loss0 # + loss1 + loss2\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss0 = epoch_loss0 + loss0.item()\n",
    "        # epoch_loss1 = epoch_loss1 + loss1.item()\n",
    "        # epoch_loss2 = epoch_loss2 + loss2.item()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(pointpillars.parameters(), max_norm=35)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + train_step + 1\n",
    "\n",
    "        if global_step % args.log_freq == 0:\n",
    "            save_summary(writer, loss_dict0, global_step, 'train',\n",
    "                            lr=optimizer.param_groups[0]['lr'], \n",
    "                            momentum=optimizer.param_groups[0]['betas'][0])\n",
    "        train_step += 1\n",
    "\n",
    "    training_loss0.append(epoch_loss0)\n",
    "    # training_loss1.append(epoch_loss1)\n",
    "    # training_loss2.append(epoch_loss2)\n",
    "\n",
    "    if (epoch + 1) % args.ckpt_freq_epoch == 0:\n",
    "        torch.save(pointpillars.state_dict(), os.path.join(saved_ckpt_path, f'epoch_{epoch+1}.pth'))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(training_loss0))\n",
    "        # plt.plot(np.array(training_loss1))\n",
    "        # plt.plot(np.array(training_loss2))\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.legend([\"64 channel\",\"32 channel\",\"16 channel\"])\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(val_loss0))\n",
    "        # plt.plot(np.array(val_loss1))\n",
    "        # plt.plot(np.array(val_loss2))\n",
    "        plt.title(\"Validation Loss\")\n",
    "        plt.legend([\"64 channel\",\"32 channel\",\"16 channel\"])\n",
    "        plt.show()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        continue\n",
    "    pointpillars.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data_dict in enumerate(tqdm(val_dataloader)):\n",
    "            try:\n",
    "                if not args.no_cuda:\n",
    "                    # move the tensors to the cuda\n",
    "                    for key in data_dict:\n",
    "                        for j, item in enumerate(data_dict[key]):\n",
    "                            if torch.is_tensor(item):\n",
    "                                data_dict[key][j] = data_dict[key][j].cuda()\n",
    "                \n",
    "                batched_pts = data_dict['batched_pts']\n",
    "                batched_gt_bboxes = data_dict['batched_gt_bboxes']\n",
    "                batched_labels = data_dict['batched_labels']\n",
    "                batched_difficulty = data_dict['batched_difficulty']\n",
    "\n",
    "                bbox_cls_pred0, bbox_pred0, bbox_dir_cls_pred0, anchor_target_dict = pointpillars(batched_pts=batched_pts, \n",
    "                                            mode='train',\n",
    "                                            batched_gt_bboxes=batched_gt_bboxes, \n",
    "                                            batched_gt_labels=batched_labels)\\\n",
    "                    # bbox_cls_pred1, bbox_pred1, bbox_dir_cls_pred1, \\\n",
    "                    #     bbox_cls_pred2, bbox_pred2, bbox_dir_cls_pred2,  = \\\n",
    "\n",
    "                \n",
    "                ################# Full features #################\n",
    "                bbox_cls_pred0 = bbox_cls_pred0.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "                bbox_pred0 = bbox_pred0.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "                bbox_dir_cls_pred0 = bbox_dir_cls_pred0.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "                ################# Half features #################\n",
    "                # bbox_cls_pred1 = bbox_cls_pred1.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "                # bbox_pred1 = bbox_pred1.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "                # bbox_dir_cls_pred1 = bbox_dir_cls_pred1.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "                # ################# Quar features #################\n",
    "                # bbox_cls_pred2 = bbox_cls_pred2.permute(0, 2, 3, 1).reshape(-1, args.nclasses)\n",
    "                # bbox_pred2 = bbox_pred2.permute(0, 2, 3, 1).reshape(-1, 7)\n",
    "                # bbox_dir_cls_pred2 = bbox_dir_cls_pred2.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "\n",
    "\n",
    "                batched_bbox_labels = anchor_target_dict['batched_labels'].reshape(-1)\n",
    "                batched_label_weights = anchor_target_dict['batched_label_weights'].reshape(-1)\n",
    "                batched_bbox_reg = anchor_target_dict['batched_bbox_reg'].reshape(-1, 7)\n",
    "                batched_dir_labels = anchor_target_dict['batched_dir_labels'].reshape(-1)\n",
    "\n",
    "\n",
    "                \n",
    "                pos_idx = (batched_bbox_labels >= 0) & (batched_bbox_labels < args.nclasses)\n",
    "\n",
    "                bbox_pred0 = bbox_pred0[pos_idx]\n",
    "                # bbox_pred1 = bbox_pred1[pos_idx]\n",
    "                # bbox_pred2 = bbox_pred2[pos_idx]\n",
    "\n",
    "                batched_bbox_reg = batched_bbox_reg[pos_idx]\n",
    "                batched_bbox_reg0 = batched_bbox_reg.clone()\n",
    "                # batched_bbox_reg1 = batched_bbox_reg.clone()\n",
    "                # batched_bbox_reg2 = batched_bbox_reg.clone()\n",
    "\n",
    "                # sin(a - b) = sin(a)*cos(b) - cos(a)*sin(b)\n",
    "                bbox_pred0[:, -1] = torch.sin(bbox_pred0[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "                # bbox_pred1[:, -1] = torch.sin(bbox_pred1[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "                # bbox_pred2[:, -1] = torch.sin(bbox_pred2[:, -1].clone()) * torch.cos(batched_bbox_reg[:, -1].clone())\n",
    "\n",
    "                batched_bbox_reg0[:, -1] = torch.cos(bbox_pred0[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "                # batched_bbox_reg1[:, -1] = torch.cos(bbox_pred1[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "                # batched_bbox_reg2[:, -1] = torch.cos(bbox_pred2[:, -1].clone()) * torch.sin(batched_bbox_reg[:, -1].clone())\n",
    "\n",
    "\n",
    "                bbox_dir_cls_pred0 = bbox_dir_cls_pred0[pos_idx]\n",
    "                # bbox_dir_cls_pred1 = bbox_dir_cls_pred1[pos_idx]\n",
    "                # bbox_dir_cls_pred2 = bbox_dir_cls_pred2[pos_idx]\n",
    "\n",
    "                batched_dir_labels = batched_dir_labels[pos_idx]\n",
    "\n",
    "                num_cls_pos = (batched_bbox_labels < args.nclasses).sum()\n",
    "\n",
    "                bbox_cls_pred0 = bbox_cls_pred0[batched_label_weights > 0]\n",
    "                # bbox_cls_pred1 = bbox_cls_pred1[batched_label_weights > 0]\n",
    "                # bbox_cls_pred2 = bbox_cls_pred2[batched_label_weights > 0]\n",
    "\n",
    "                batched_bbox_labels[batched_bbox_labels < 0] = args.nclasses\n",
    "                batched_bbox_labels = batched_bbox_labels[batched_label_weights > 0]\n",
    "\n",
    "                loss_dict0 = loss_func(bbox_cls_pred=bbox_cls_pred0,\n",
    "                                        bbox_pred=bbox_pred0,\n",
    "                                        bbox_dir_cls_pred=bbox_dir_cls_pred0,\n",
    "                                        batched_labels=batched_bbox_labels, \n",
    "                                        num_cls_pos=num_cls_pos, \n",
    "                                        batched_bbox_reg=batched_bbox_reg0, \n",
    "                                        batched_dir_labels=batched_dir_labels)\n",
    "                \n",
    "                # loss_dict1 = loss_func(bbox_cls_pred=bbox_cls_pred1,\n",
    "                #                         bbox_pred=bbox_pred1,\n",
    "                #                         bbox_dir_cls_pred=bbox_dir_cls_pred1,\n",
    "                #                         batched_labels=batched_bbox_labels, \n",
    "                #                         num_cls_pos=num_cls_pos, \n",
    "                #                         batched_bbox_reg=batched_bbox_reg1, \n",
    "                #                         batched_dir_labels=batched_dir_labels)\n",
    "                \n",
    "                # loss_dict2 = loss_func(bbox_cls_pred=bbox_cls_pred2,\n",
    "                #                         bbox_pred=bbox_pred2,\n",
    "                #                         bbox_dir_cls_pred=bbox_dir_cls_pred2,\n",
    "                #                         batched_labels=batched_bbox_labels, \n",
    "                #                         num_cls_pos=num_cls_pos, \n",
    "                #                         batched_bbox_reg=batched_bbox_reg2, \n",
    "                #                         batched_dir_labels=batched_dir_labels)\n",
    "                \n",
    "                loss0 = loss_dict0['total_loss'] \n",
    "                # loss1 = loss_dict1['total_loss']\n",
    "                # loss2 = loss_dict2['total_loss'] \n",
    "\n",
    "                val_epoch_loss0 = val_epoch_loss0 + loss0.item()\n",
    "                # val_epoch_loss1 = val_epoch_loss1 + loss1.item()\n",
    "                # val_epoch_loss2 = val_epoch_loss2 + loss2.item()\n",
    "\n",
    "                global_step = epoch * len(val_dataloader) + val_step + 1\n",
    "                if global_step % args.log_freq == 0:\n",
    "                    save_summary(writer, loss_dict0, global_step, 'val')\n",
    "                val_step += 1\n",
    "            \n",
    "            except:\n",
    "                None\n",
    "\n",
    "    val_loss0.append(val_epoch_loss0)\n",
    "    # val_loss1.append(val_epoch_loss1)\n",
    "    # val_loss2.append(val_epoch_loss2)\n",
    "            \n",
    "    pointpillars.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.array(training_loss0))\n",
    "# plt.plot(np.array(training_loss1))\n",
    "# plt.plot(np.array(training_loss2))\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend([\"64 channel\",\"32 channel\",\"16 channel\"])\n",
    "plt.savefig(\"Matryoshka_train_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(val_loss0))\n",
    "# plt.plot(np.array(val_loss1))\n",
    "# plt.plot(np.array(val_loss2))\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend([\"64 channel\",\"32 channel\",\"16 channel\"])\n",
    "plt.savefig(\"Matryoshka_val_loss.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
